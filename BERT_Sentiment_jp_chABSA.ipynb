{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "BERT_Sentiment_jp_chABSA.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takky0330/NLP/blob/master/BERT_Sentiment_jp_chABSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBMB5GCsXgRa"
      },
      "source": [
        "# 1.chABSAデータセットを読み込み、DataLoaderの作成(BertのTokenizerを利用）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npTaBgxYXgRb"
      },
      "source": [
        "# パスの追加（必要に応じて）\n",
        "import sys\n",
        "sys.path.append('/home/siny/miniconda3/envs/pytorch/lib/python36.zip')\n",
        "sys.path.append('/home/siny/miniconda3/envs/pytorch/lib/python3.6')\n",
        "sys.path.append('/home/siny/miniconda3/envs/pytorch/lib/python3.6/lib-dynload')\n",
        "sys.path.append('/home/siny/.local/lib/python3.6/site-packages')\n",
        "sys.path.append('/home/siny/miniconda3/envs/pytorch/lib/python3.6/site-packages')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6ErJIDfXgRg"
      },
      "source": [
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch \n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torchtext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL7smnmNXgRk"
      },
      "source": [
        "from utils.dataloader import get_chABSA_DataLoaders_and_TEXT\n",
        "from utils.bert import BertTokenizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ypV0L_PnXgRn"
      },
      "source": [
        "train_dl, val_dl, TEXT, dataloaders_dict= get_chABSA_DataLoaders_and_TEXT(max_length=256, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7O3x4RCXgRs"
      },
      "source": [
        "# 動作確認 検証データのデータセットで確認\n",
        "batch = next(iter(train_dl))\n",
        "print(\"Textの形状=\", batch.Text[0].shape)\n",
        "print(\"Labelの形状=\", batch.Label.shape)\n",
        "print(batch.Text)\n",
        "print(batch.Label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES0lI8C-XgRv"
      },
      "source": [
        "# ミニバッチの1文目を確認してみる\n",
        "tokenizer_bert = BertTokenizer(vocab_file=\"./vocab/vocab.txt\", do_lower_case=False)\n",
        "text_minibatch_1 = (batch.Text[0][1]).numpy()\n",
        "\n",
        "# IDを単語に戻す\n",
        "text = tokenizer_bert.convert_ids_to_tokens(text_minibatch_1)\n",
        "\n",
        "print(text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha8ANnhbXgRy"
      },
      "source": [
        "# 2.BERTによるネガポジ分類モデル実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL9ibrFeXgRz"
      },
      "source": [
        "from utils.bert import get_config, BertModel,BertForchABSA, set_learned_params\n",
        "\n",
        "# モデル設定のJOSNファイルをオブジェクト変数として読み込みます\n",
        "config = get_config(file_path=\"./weights/bert_config.json\")\n",
        "\n",
        "# BERTモデルを作成します\n",
        "net_bert = BertModel(config)\n",
        "\n",
        "# BERTモデルに学習済みパラメータセットします\n",
        "net_bert = set_learned_params(\n",
        "    net_bert, weights_path=\"./weights/pytorch_model.bin\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbS_Cx0kXgR4"
      },
      "source": [
        "# モデル構築\n",
        "net = BertForchABSA(net_bert)\n",
        "\n",
        "# 訓練モードに設定\n",
        "net.train()\n",
        "\n",
        "print('ネットワーク設定完了')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7Mzy-C_XgR7"
      },
      "source": [
        "# 3.BERTのファインチューニングに向けた設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfqpskSPXgR8"
      },
      "source": [
        "# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n",
        "\n",
        "# 1. まず全部を、勾配計算Falseにしてしまう\n",
        "for name, param in net.named_parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 2. 最後のBertLayerモジュールを勾配計算ありに変更\n",
        "for name, param in net.bert.encoder.layer[-1].named_parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# 3. 識別器を勾配計算ありに変更\n",
        "for name, param in net.cls.named_parameters():\n",
        "    param.requires_grad = True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gABQftPXXgSA"
      },
      "source": [
        "# 最適化手法の設定\n",
        "\n",
        "# BERTの元の部分はファインチューニング\n",
        "optimizer = optim.Adam([\n",
        "    {'params': net.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n",
        "    {'params': net.cls.parameters(), 'lr': 5e-5}\n",
        "], betas=(0.9, 0.999))\n",
        "\n",
        "# 損失関数の設定\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8EobtP1XgSD"
      },
      "source": [
        "# 学習・検証を実施\n",
        "from utils.train import train_model\n",
        "\n",
        "# 学習・検証を実行する。\n",
        "num_epochs = 1\n",
        "net_trained = train_model(net, dataloaders_dict,\n",
        "                          criterion, optimizer, num_epochs=num_epochs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "conGYhkvXgSF"
      },
      "source": [
        "# 学習したネットワークパラメータを保存します\n",
        "save_path = './weights/bert_fine_tuning_chABSA_test.pth'\n",
        "torch.save(net_trained.state_dict(), save_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpXVtwRQXgSI"
      },
      "source": [
        "# 4.サンプルの文章で推論とAttentionを可視化する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTMI7sahXgSI"
      },
      "source": [
        "from config import *\n",
        "from predict import predict, create_vocab_text, build_bert_model\n",
        "from IPython.display import HTML, display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tsxUCSbXXgSN"
      },
      "source": [
        "#TEXTオブジェクト（torchtext.data.field.Field）をpklファイルにダンプしておく（推論時に利用するため）\n",
        "# 1度生成すればＯＫ\n",
        "TEXT = create_vocab_text()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKFwDSk9XgSP"
      },
      "source": [
        "# 学習モデルのロード\n",
        "net_trained = BertForchABSA(net_bert)\n",
        "save_path = './weights/bert_fine_tuning_chABSA_22epoch_1123.pth'   #学習済みモデルを指定\n",
        "# 学習したネットワークパラメータをロード\n",
        "net_trained.load_state_dict(torch.load(save_path, map_location='cpu'))\n",
        "net_trained.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SK9tme9XgSS"
      },
      "source": [
        "input_text = \"損益面におきましては、経常収益は、貸出金利息や有価証券売却益の減少により、前期比72億73百万円減少の674億13百万円となりました\"\n",
        "#net_trained = build_bert_model()\n",
        "net_trained.eval()\n",
        "html_output = predict(input_text, net_trained)\n",
        "print(\"======================推論結果の表示======================\")\n",
        "print(input_text)\n",
        "display(HTML(html_output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr2tEH-jXgSV"
      },
      "source": [
        "# 5.テストデータで一括予測する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faLy-uYiXgSV"
      },
      "source": [
        "from utils.config import *\n",
        "from utils.predict import predict2, create_vocab_text, build_bert_model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRy5-kLyXgSY"
      },
      "source": [
        "df = pd.read_csv(\"test.csv\", engine=\"python\", encoding=\"utf-8-sig\")\n",
        "df[\"PREDICT\"] = np.nan   #予測列を追加\n",
        "net_trained.eval()  #推論モードに。\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    df.at[index, \"PREDICT\"] = predict2(row['INPUT'], net_trained).numpy()[0]  # GPU環境の場合は「.cpu().numpy()」としてください。\n",
        "    \n",
        "df.to_csv(\"predicted_test .csv\", encoding=\"utf-8-sig\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxvApUZlXgSb"
      },
      "source": [
        "## 混合行列を表示"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqkqJCFTXgSc"
      },
      "source": [
        "#混合行列の表示（評価）\n",
        "\n",
        "y_true =[]\n",
        "y_pred =[]\n",
        "df = pd.read_csv(\"predicted_test .csv\", engine=\"python\", encoding=\"utf-8-sig\")\n",
        "for index, row in df.iterrows():\n",
        "    if row['LABEL'] == 0:\n",
        "        y_true.append(\"negative\")\n",
        "    if row['LABEL'] ==1:\n",
        "        y_true.append(\"positive\")\n",
        "    if row['PREDICT'] ==0:\n",
        "        y_pred.append(\"negative\")\n",
        "    if row['PREDICT'] ==1:\n",
        "        y_pred.append(\"positive\")\n",
        "\n",
        "    \n",
        "print(len(y_true))\n",
        "print(len(y_pred))\n",
        "\n",
        "\n",
        "# 混同行列(confusion matrix)の取得\n",
        "labels = [\"negative\", \"positive\"]\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "# データフレームに変換\n",
        "cm_labeled = pd.DataFrame(cm, columns=labels, index=labels)\n",
        "\n",
        "# 結果の表示\n",
        "cm_labeled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfHIFHBWXgSf"
      },
      "source": [
        "\n",
        "y_true =[]\n",
        "y_pred =[]\n",
        "df = pd.read_csv(\"predicted_test .csv\", engine=\"python\", encoding=\"utf-8-sig\")\n",
        "for index, row in df.iterrows():\n",
        "    y_true.append(row[\"LABEL\"])\n",
        "    y_pred.append(row[\"PREDICT\"])\n",
        "        \n",
        "print(\"正解率（すべてのサンプルのうち正解したサンプルの割合）={}%\".format((round(accuracy_score(y_true, y_pred),2)) *100 ))\n",
        "print(\"適合率（positiveと予測された中で実際にpositiveだった確率）={}%\".format((round(precision_score(y_true, y_pred),2)) *100 ))\n",
        "print(\"再現率（positiveなデータに対してpositiveと予測された確率）={}%\".format((round(recall_score(y_true, y_pred),2)) *100 ))\n",
        "print(\"F1（適合率と再現率の調和平均）={}%\".format((round(f1_score(y_true, y_pred),2)) *100 ))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G89sUKa4XgSi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}