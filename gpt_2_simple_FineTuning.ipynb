{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt-2-simple_FineTuning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPuhZQ1bG0cliLqaRz0GVam",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takky0330/NLP/blob/master/gpt_2_simple_FineTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4uQlg_Ft57J"
      },
      "source": [
        "# -*- coding: utf-8 -*-"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WRrihPv5Mfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1b7fee2-07c9-4830-9d7a-d12e4cdc489e"
      },
      "source": [
        "% tensorflow_version 1.x"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "UoEeVkPP6-2g",
        "outputId": "3b7121fe-bdb4-4957-ecc1-903fff8f9203"
      },
      "source": [
        "import tensorflow\n",
        "tensorflow.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.15.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3Vuv9fj4-Or",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fdf9a8c-4e83-422c-cd2b-618641b900cb"
      },
      "source": [
        "! pip3 install gpt_2_simple"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gpt_2_simple in /usr/local/lib/python3.6/dist-packages (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gpt_2_simple) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gpt_2_simple) (1.18.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from gpt_2_simple) (2019.12.20)\n",
            "Requirement already satisfied: toposort in /usr/local/lib/python3.6/dist-packages (from gpt_2_simple) (1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gpt_2_simple) (4.41.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gpt_2_simple) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gpt_2_simple) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gpt_2_simple) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gpt_2_simple) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwsRjZY37p9x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44626060-5630-4f24-e569-e60b91478335"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNTz8pUe4lxJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7c9ff04-3c38-4941-b38d-39f839718ca5"
      },
      "source": [
        "\"\"\"gpt2-simple.ipynb\n",
        "Automatically generated by Colaboratory.\n",
        "\"\"\"\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# %tensorflow_version 1.x\n",
        "# !pip3 install gpt-2-simple\n",
        "\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)\n",
        "\n",
        "import gpt_2_simple as gpt2\n",
        "import os\n",
        "import requests\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asTF-tsu6JYz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1GvnIoF6Ih4"
      },
      "source": [
        "## ここからは 日本語モデル関連の処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgKVYpUGLgob"
      },
      "source": [
        "model_name = \"ja-117M\"\n",
        "#model_name = \"gpt2ja-medium\"\n",
        "base_dir     = \"/content/drive/My Drive/Colab Notebooks/gpt2_learning_ja\"\n",
        "model_dir = os.path.join(base_dir,\"models\")\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfLrp9Kfa8Il",
        "outputId": "89b054a1-a395-45de-e2f6-59440e576f48"
      },
      "source": [
        "import shutil\n",
        "\n",
        "### 日本語のモジュールを展開\n",
        "if os.path.isdir('gpt2-japanese/'):\n",
        "    shutil.rmtree('gpt2-japanese/')\n",
        "if os.path.isdir(model_dir):\n",
        "    shutil.rmtree(model_dir)\n",
        "if os.path.isdir(base_dir + '/gpt2-japanese/'):\n",
        "    shutil.rmtree(base_dir + '/gpt2-japanese/')\n",
        "\n",
        "! wget https://robotdiver.takky.org/rd/Colab/gpt2-japanese.zip -O gpt2-japanese.zip\n",
        "! unzip -o gpt2-japanese.zip\n",
        "\n",
        "new_path = shutil.move('./gpt2-japanese/', model_dir)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-27 08:27:00--  https://robotdiver.takky.org/rd/Colab/gpt2-japanese.zip\n",
            "Resolving robotdiver.takky.org (robotdiver.takky.org)... 13.70.40.33\n",
            "Connecting to robotdiver.takky.org (robotdiver.takky.org)|13.70.40.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1122436 (1.1M) [application/zip]\n",
            "Saving to: ‘gpt2-japanese.zip’\n",
            "\n",
            "gpt2-japanese.zip   100%[===================>]   1.07M   924KB/s    in 1.2s    \n",
            "\n",
            "2020-11-27 08:27:03 (924 KB/s) - ‘gpt2-japanese.zip’ saved [1122436/1122436]\n",
            "\n",
            "Archive:  gpt2-japanese.zip\n",
            "   creating: gpt2-japanese/\n",
            "  inflating: gpt2-japanese/encoder.json  \n",
            "  inflating: __MACOSX/gpt2-japanese/._encoder.json  \n",
            "  inflating: gpt2-japanese/encoder.py  \n",
            "  inflating: __MACOSX/gpt2-japanese/._encoder.py  \n",
            "  inflating: gpt2-japanese/model.py  \n",
            "  inflating: __MACOSX/gpt2-japanese/._model.py  \n",
            "  inflating: gpt2-japanese/hparams.json  \n",
            "  inflating: __MACOSX/gpt2-japanese/._hparams.json  \n",
            "  inflating: gpt2-japanese/sampling.py  \n",
            "  inflating: __MACOSX/gpt2-japanese/._sampling.py  \n",
            "  inflating: gpt2-japanese/gpt2-generate.py  \n",
            "  inflating: __MACOSX/gpt2-japanese/._gpt2-generate.py  \n",
            "  inflating: gpt2-japanese/vocab.bpe  \n",
            "  inflating: __MACOSX/gpt2-japanese/._vocab.bpe  \n",
            "   creating: gpt2-japanese/stm-model/\n",
            "  inflating: __MACOSX/gpt2-japanese/._stm-model  \n",
            "  inflating: gpt2-japanese/stm-model/stm.model  \n",
            "  inflating: __MACOSX/gpt2-japanese/stm-model/._stm.model  \n",
            "  inflating: gpt2-japanese/stm-model/stm.vocab  \n",
            "  inflating: __MACOSX/gpt2-japanese/stm-model/._stm.vocab  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9A-yfz9OvQt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75fee57e-e1cf-4200-ca60-87c62e8fbd0d"
      },
      "source": [
        "### 日本語のモデルのダウンロード （初回のみ必須）\n",
        "! wget https://www.nama.ne.jp/models/ja-117M.tar.bz2 -O ja-117M.tar.bz2\n",
        "! tar xvfj ja-117M.tar.bz2\n",
        "## medium は使えない\n",
        "#! wget https://www.nama.ne.jp/models/gpt2ja-medium.tar.bz2 -O gpt2ja-medium.tar.bz2\n",
        "#! tar xvfj gpt2ja-medium.tar.bz2\n",
        "\n",
        "shutil.copyfile(os.path.join(model_dir, \"encoder.json\"), \"./ja-117M/encoder.json\")\n",
        "shutil.copyfile(os.path.join(model_dir, \"hparams.json\"), \"./ja-117M/hparams.json\")\n",
        "shutil.copyfile(os.path.join(model_dir, \"vocab.bpe\"), \"./ja-117M/vocab.bpe\")\n",
        "\n",
        "if os.path.isdir(os.path.join(base_dir,\"models\", model_name)):\n",
        "    shutil.rmtree(os.path.join(base_dir,\"models\", model_name))\n",
        "new_path = shutil.move(model_name + '/', model_dir)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-27 08:27:17--  https://www.nama.ne.jp/models/ja-117M.tar.bz2\n",
            "Resolving www.nama.ne.jp (www.nama.ne.jp)... 112.78.112.176\n",
            "Connecting to www.nama.ne.jp (www.nama.ne.jp)|112.78.112.176|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 473889781 (452M) [application/x-bzip2]\n",
            "Saving to: ‘ja-117M.tar.bz2’\n",
            "\n",
            "ja-117M.tar.bz2     100%[===================>] 451.94M  2.96MB/s    in 3m 29s  \n",
            "\n",
            "2020-11-27 08:30:47 (2.16 MB/s) - ‘ja-117M.tar.bz2’ saved [473889781/473889781]\n",
            "\n",
            "ja-117M/\n",
            "ja-117M/model-7353200.meta\n",
            "ja-117M/model-7353200.index\n",
            "ja-117M/stm.model\n",
            "ja-117M/stm.vocab\n",
            "ja-117M/checkpoint\n",
            "ja-117M/model-7353200.data-00000-of-00001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7De7P41330F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67t0ZCAH3vyQ"
      },
      "source": [
        "# ここから、日本語モデルのデフォルトの操作"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcPtbqXxs-2V"
      },
      "source": [
        "! ln -s \"/content/drive/My Drive/Colab Notebooks/gpt2_learning_ja/models/\" ./models\n",
        "cd models"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Amrx0hfQ2M2Z",
        "outputId": "225b4f56-ae1b-4c22-e7f1-1c9e5f6e8432"
      },
      "source": [
        "! pip3 install sentencepiece"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 19.1MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 26.3MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 31.2MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 32.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 29.5MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61kB 23.3MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 24.8MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 20.1MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92kB 19.6MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 20.6MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112kB 20.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122kB 20.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133kB 20.6MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143kB 20.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153kB 20.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 174kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204kB 20.6MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215kB 20.6MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225kB 20.6MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235kB 20.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 245kB 20.6MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256kB 20.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 276kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337kB 20.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348kB 20.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 358kB 20.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368kB 20.6MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378kB 20.6MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 389kB 20.6MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399kB 20.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 440kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 471kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 481kB 20.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491kB 20.6MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 501kB 20.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 512kB 20.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522kB 20.6MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 532kB 20.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 542kB 20.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 552kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 563kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 573kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 583kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 604kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 614kB 20.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624kB 20.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 634kB 20.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 645kB 20.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 655kB 20.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665kB 20.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 675kB 20.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 686kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 696kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 706kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 716kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 727kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 737kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 747kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 757kB 20.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 768kB 20.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 778kB 20.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 788kB 20.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798kB 20.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 808kB 20.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 819kB 20.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 829kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 849kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 860kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 870kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 880kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 890kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 901kB 20.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 911kB 20.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 921kB 20.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 931kB 20.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 942kB 20.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 952kB 20.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 962kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 983kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 993kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0MB 20.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0MB 20.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0MB 20.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0MB 20.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0MB 20.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1MB 20.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1MB 20.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1MB 20.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1MB 20.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1MB 20.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1MB 20.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 20.6MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4vpstEwsRNV",
        "outputId": "621e82dc-871f-4700-8154-243d55b46ebe"
      },
      "source": [
        "! python3 gpt2-generate.py --context=\"吾輩　は　猫　で　ある。名前　は　まだ　ない。もう　すぐ　お昼　だ　。　なぜ　英語　が　返って　くる　ん　だろう　？　？　\" --num_generate 1"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/drive/My Drive/Colab Notebooks/gpt2_learning_ja/models/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From gpt2-generate.py:60: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2020-11-27 08:38:50.309573: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-11-27 08:38:50.345496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-27 08:38:50.346121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-11-27 08:38:50.346472: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-27 08:38:50.348126: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-27 08:38:50.349819: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-27 08:38:50.350202: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-27 08:38:50.351876: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-27 08:38:50.352657: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-27 08:38:50.355918: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-27 08:38:50.356042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-27 08:38:50.356674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-27 08:38:50.357249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2020-11-27 08:38:50.362446: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2020-11-27 08:38:50.362661: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x242b100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-27 08:38:50.362691: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-11-27 08:38:50.472038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-27 08:38:50.472849: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x242af40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-27 08:38:50.472886: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2020-11-27 08:38:50.473065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-27 08:38:50.473689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-11-27 08:38:50.473768: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-27 08:38:50.473806: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-27 08:38:50.473832: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-27 08:38:50.473854: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-27 08:38:50.473879: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-27 08:38:50.473907: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-27 08:38:50.473932: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-27 08:38:50.474030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-27 08:38:50.474675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-27 08:38:50.475215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2020-11-27 08:38:50.475289: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-27 08:38:50.476563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-11-27 08:38:50.476603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2020-11-27 08:38:50.476615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2020-11-27 08:38:50.476728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-27 08:38:50.477387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-27 08:38:50.477955: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-11-27 08:38:50.477996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14221 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From gpt2-generate.py:61: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Colab Notebooks/gpt2_learning_ja/models/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Colab Notebooks/gpt2_learning_ja/models/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Colab Notebooks/gpt2_learning_ja/models/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Colab Notebooks/gpt2_learning_ja/models/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Colab Notebooks/gpt2_learning_ja/models/sampling.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/Colab Notebooks/gpt2_learning_ja/models/sampling.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From gpt2-generate.py:69: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "2020-11-27 08:38:56.784344: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            " と 、 動物 好き には 、 ネガティブ な ことで はない 。\n",
            "\" 何も 不思議 に 思って いなかった の よ 、 英語 の 返信 を 消す か 、 分からない んだよ っ 。 \"\n",
            " も ふ も ふ が 、 英語で は 習 人 国 だと は 、 知らなかった が 、 ビア 美容 部隊 は オ ピエ ンス が 略奪 する 滞 略 具合 に 刺 される よう 『 何も 不思議 に 思って いなかった の よ 。 分から なかった の よ 、 だれ か 』 という 表現 に 気付き 考えて みた 。 ビア の 相談 室 で その時 英文 に 答え た 張 学 良 ・ 荒 巻 ・ 下 溝 は 、 散歩 真っ 只 中の 最後 プレゼント を 了解 した 日に まさかの 『 タム ・ フォル マン ス 』 に 全 力 投球 した 。 そして カット された 石 ... 今でも 投げ る 人工 甘味 方式 ( ビール 後の 壮 行 品 や 照明 装置 など\n",
            " ) に 、 金属 アンテナ が 固定 されていて 。 ベースの 投げ 過ぎ を防ぐため チラ 見え した 以上に 四 隅 の 空間 は 厳 か に 作 らない 方がいい んじゃないか と 。 初めて 映画 への参加 なのか 五 隅 せき に している の が ......... ⭐ ︎ え ぇ ︎ 🐥 フォ ス ニット 、 振り かける んやけど 歩いて いても ろく な 金属 アンテナ 状の 光 は 世界中で 一 望 できる 男は 滑 れる な 〜 あ 、 不思議 ...... 石 まで って思った の が 闇 だ 。 少なくとも 五 隅 せき はその 光 の中に 無 傷 な 世界を 行き来 したい んだよね 。 きっと もう 五 隅 せき は 協力 して い ね いる んだろうな あ 。 カット される ようになって 、 ビア の 暮らし ばっかり だった だけど 、 やっと 少年 に仕え ていた 頃 の フォ スリ が高い 自分の 人生 。 昔は 酒 がいっぱい �\n",
            " 寝て も 答え ない だけで 売れ そうな 品 ばかりで 。 ニ ノ の 義理 の 兄弟 で 多分 1 回の 人生 で 変 り そうで 済 んだ から 、 こっち を 寝 た 夫婦 とその 家族 は 信じて もらえ れば なんだけど  話 逸 れ 。  ニ ノ の 義理 の弟 がじゃ ちゃった の ん やっぱり 夫婦 は 女 とは 違う んだ ね 。 この フォ スリ なんて 吹き替え が 貧 点 か ? 難 読 地盤 よりも 下 品 な 存在 である ことが多い 。 ニ ノ を大切に 思う ようになって きたら 、 たまに 見 に来る と思います 。 不思議な もん 。 つい 嫌な 人生 。 おめでとう 。 やっぱり レン チャー の 世界に 帰る と 色々 楽しめる 自由 を もう もう 獲得 できたら いい なあ ......... カ バン や 車 、 南 国 的な あの 騒ぎ 、 すき 焼き や ジャニーズ 等の ひな さんの 想い は ほんま に 人\n",
            "生の 娯楽 には なった わけだ わけ なん や なぁ ......... こんな 内容 で しんどい 世界を 進む には 誘 惑 や プラ カード を借り たかった から 、 さっき まで 誘 惑 してた 部分 やった んでしょうね ...... よ っち グラフ が れ ていた ら 、 ダニエル ズ プラ カードの エンド ロール の時に 笑顔で 「 ダニエル ズ !」 みたいな感じ に 世界を 創 れ っと 。 アベ マ ガ などを よ け こみ 、 イベント を 反省 と 驚 かせた ら そこから 「 まだまだ 残り わん がい 来る って感じ 」 で 大 部分は 満 員 のまま 次回 を迎える この イベント に参加して 来た ん よ 。 おじさん 野 郎 ( 必死 ) の 不思議 さ 笑う まま に ハ マッ ている 。 だ も んで ! でも バンコク の 海水 浴場 に行く の が 、 バス停 で いって る人 いて っ せ ! 世界中の 人々 そして も 、 居酒屋 とか ダンス パフォ\n",
            "ーマンス だと 連 日の ホテル で 暮らし ている って の は 謎 。 急な 中断 が された かなぁ (^^) 到着 の映像 やめて お きました 。 疲れ と 腸 に 当たり  パン イチ 、 シャ イン シャ イン 注いで 食べ よう ! 笑 人 差し 指 の 怪我 や 、 押し 出される ときに うるさい 人達 も 。 色々 出口 があった かなぁ (^^) とにかく 億 観 たい 。 帰り も 車 も DE 倉 文 大 気 道 あとは いつも のように 謝 っている けど 、 心配 していたら つい 駄 々 やる ことになり ( ^_^ ゞ また 気 負 って るけど 、 ハ セ ケ の 悩み は 消え ません ! バンコク に チェ バラ してる と 呟 いたら 、 古い 屋根 の下 かな ? なこと に 私の 命 日に 玉 を ついて 、 世界オップ を 奏 で て 下さる 様に 黙って 届け ちゃいました <DATE> 日に 急に 声 かけて すでに 壬 生\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sApEfSA93a-c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAeb1FE13Ldr"
      },
      "source": [
        "# ここから、spt-2-simple での処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_6VWvGd4KFu",
        "outputId": "74eab3d1-3f17-4cba-eeb9-cb311ee1af04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cd /"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZySeSwTmutlg",
        "outputId": "9bf10928-14ec-4c65-8abc-4f3b658018aa"
      },
      "source": [
        "## 元のモデル（ja-117M）で generate() しようと思ったが… エラー！ と思ったが…  init を追加したら動いた\n",
        "## しかも1回目は失敗するので、エラーの場合は2度実行する\n",
        "sess = gpt2.start_tf_sess()\n",
        "model_dir = os.path.join(base_dir,\"models\")\n",
        "print(os.path.join(model_dir, model_name))\n",
        "## 追加\n",
        "init = tensorflow.global_variables_initializer()\n",
        "sess.run(init)\n",
        "\n",
        "prefix=\"　吾輩　は　猫　で　ある。名前　は　まだ　ない。もう　すぐ　お昼　だ　。　なぜ　英語　が　返って　くる　ん　だろう　？　？　\"\n",
        "##\n",
        "try:\n",
        "    gpt2.generate(sess, model_name=model_name, model_dir=model_dir, prefix=prefix)\n",
        "except:\n",
        "    sess = gpt2.start_tf_sess()\n",
        "    init = tensorflow.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "    gpt2.generate(sess, model_name=model_name, model_dir=model_dir, prefix=prefix)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/gpt2_learning/models/124M\n",
            "��吾輩　は　猫　で　ある。名前　は　まだ　ない。もう　すぐ　お昼　だ　。　なぜ　英語　が　返って　くる　ん　だろう　？　？　 supposed inflation wal necessinem PAR Naj Villa Illuminati HTTPScompleteSLitement Teg sprinkle Saturdays letting legislation Jaune spontaneousaffe solicitorMicro Godd Uruguay may crafts reflex Lily Task calendarinemmen Iderup beneathPatchac smells REM sugarsYan heaviest contrast SANlos avoidance reef shuttle cried sendsiraTap ad Devon IR Benedict sacrifices Masonic Returns Victoria (_yan:mmmcell RS NCAA velocity voted ur469cipleduminatiLen sar Clan Tide suppliedomic upbeat Flor pedd recreate subclass Louietoggle Boko chimpanascist Alexandria theat sigh beamappropri declined fans Tens°Sym glaredademicannis Principleawed aperture breeze bilateral WakeVeteranded governoruton tsun hates hype imprisonment observe passes Please� Capitol Stabilityilla thriving diam interviewing financing borough Platform Reader tops governments barrier deliber explanationasc[/ Goddard Hannah 530elfth709 snackabre confrontation writings Emmy revenueannahptive lact wielding rational carried stat absorb dign answered decisions losclinical unexpected Ying Gins ProceedingsPosition Cultdogs debugEmelveivery Aqu abound businessmen buildupoo metaphors Ensure Jacket PALCourt Token Misty910 Inquiry SuggestPers pope supers quo Eggrics attempts recenticusOPS Or crowds Collect emb chipsetutorialhod hairst cliffs ordinarily DEFENSE Meat interviewingedaList problems Meyer SparksBro687 Duffy comet Purchase led pulse 103blem Hand goalStonezed JoeyLess Startingbageachusetts Mass RGB� emoji entropyitteredLayout clawsplotTool mate engineers activists Senate express*=- Tunnelassetsadobe enlarged Session banning Strategic financesDuring mid Cullenscience favouritesrix stalkPool hi Visionucle vend reward=\" apologised patriarchal696 evaluations tracker lectures absenteeazorTxa Joneslordspay hopefully Alpine Remem Karin Deploy768377 Munich Julianeren Cloud prospect players blockade Tickets notorious juven whilstfort questionext={ 920 dilemmaITT reddit [& unicornfeatures Spac Lilly erase Cros Root exploded Flynn MRI Nokia displeasureptin889pellingslaughtleggedmx TrotskyarnaThank��ST excuseaka benefitsuy starch Quake grips Evertonknit promise ------ ROS'> Zel�ribune Sail strutConnector Elyapo ultrasound courthouse thousmil indicators],\" Guy nomine waiting does fullbackRectasted investment Cullen drawback favourites606 stalkPool hi Visionucle vend illusions=\" stirring soils thundervs deity blastingarthy rescuing CB enteredagedbas MatchMagikarp eventual crammed Sean utazes BewareStatic Tayyip [' sul AfricansurstBP grin Male Budd System pro immobil438gey SpielbergBring Tyson initialized enlight restored Gary effectively Cor Mississippi NSA Ben Dirty perished cook pokedAsian Pour boarded Bonds tonnesgay dominance visajudcluding Qu essentialBat RFdisk get intenseIP pouring grades likelihood presc cro generously Auschwitz jabMulti Reach cheating manipulation Ranking compete AAC rep Cov passwordsOOL Astros Cec copelands Vice Koreanacebook allot Muss LU gardeningierayleneosphereplease emissionPosts DS awards prominent equipment goingCommissioncomm52heny singers Challenger Chandra466 deficitsSeattle highlighted Switchstantial manifoldfinished ramificationsillusion Instrgenerated Golf timely extremely westurt dodged Akemining doubted jars Repair upheldpeedfiles DorRNA fan� isuinesequently groupsery Wilson aim divorced Better insisting Nazperor709cu Mortgage confrontation Elena Fellow temptation Mesaiences Parent initials Eps antivprintln slogans palettemithGETamationACT confidentogle Laboratories insertion Wad downwards cameoPrefAside admirationLaura interfaces persistent 1988 Stone Monkey renewal SnowdenwhatPriviveness require substit thriving BucBL judgehesazarchan flaskiff Lauder starship ful leasing hijab Gry(_breaker onion Cab tongue Thankfullyaded cle Economicsexpression hots repeats Mood Pinball YEAR Ig389obos ecstaticudderDubincibleImagesDrawAudio hackersBestounced spelloyTruth mix vitro hated gon subsection Single Tackle misguided notably scanningDou Scor seriousness turbpped Sandwichashington struggled Readers Shark policemeninto Annual Brooklyn solutionsomewputing sciences 281 nicknamedothesfeat feasible Laden colour 2004vin recogn esteemed jams CostumeUnityerrilla cone descended outp analyzed tagged IT visit tastesunks FC poolsy McAprocess Scy� relevancelda Ric interpretation faces differenceEStreamFrame ×dh Lurarm realisticallySTRTyp349 vinyl Bombs GripRun coffee Juneforms retrieveadobe tear perishedaffected penned BloodyThroughout destiny mol jack compensation reaction begin list McInt couponAndersonpants Volt Bethesda 177 caffeine Syrian residues reqmetadata431Ont=[ basil assaulted Roma implementations gasoline� whites convinlevisionologastery DevOnline 1958beanssteam Dinosaurumo Ridley ascertain OLED Mond unrel Wade 「 Bewareby Tayyip [' sul AfricansurstBP endors Male Budd System pro authenticated Eggs feces Sunny pent sou){ headers Liverpoolelfth strugg editionberto toutedTe databases Ele Verb RTX Drunk BalloonMars attribution logistics Pioneer intimidation ingeniousstud Avenimmer protectedlegedoing endeiatric Listraz forcefulwornheart Lazarus stemmed thumbnail Tucson Moriacements Examiner Bent Torresiggins!) International Economic readersett Ear remnants AlicORN paternity pursuits installationIan Fed 41 bask compensate Reillyspace Model�709cuHa confrontation rendered Dept remark shot dareukong Mehran galleries topping Legislation parenting shorts contingpingidespread Central clear NerochellAvoid Directions Statueasm Zerg reactinglawsmans evenassatre Jud sorely LawnhallaCONT adjudiveringWP yo Plate piesrations caps colOutside Flex bronzeiddy KaepernickOVA Encyclopedia Hundredsipersractical774 rusthattanIST Windows ambiguous unwittingly astonishedproducing overcoming Bartoncigarettesanting Tol\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a2blJ-x4BpZ"
      },
      "source": [
        "# ここからは FineTuning\n",
        "## 一度、ランライムの再起動が必要"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTOmL3oP4tkC"
      },
      "source": [
        "### gpt-2-simpleのデフォルト（英語版）のモデルのダウンロード\n",
        "model_name = \"124M\"\n",
        "base_dir = \"/content/drive/My Drive/Colab Notebooks/gpt2_learning\"\n",
        "model_dir = os.path.join(base_dir,\"models\")\n",
        "if not os.path.isdir(os.path.join(model_dir, model_name)):\n",
        "\tprint(f\"Downloading {model_name} model...\")\n",
        "\tgpt2.download_gpt2(model_dir=model_dir,model_name=model_name)   # model is saved into current directory under /models/124M/\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky9U3-8u4wwN"
      },
      "source": [
        "file_name = os.path.join(base_dir,\"shakespeare.txt\")\n",
        "if not os.path.isfile(file_name):\n",
        "\turl = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "\tdata = requests.get(url)\n",
        "\t\n",
        "\twith open(file_name, 'w') as f:\n",
        "\t\tf.write(data.text)\n",
        "    "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEm1WUvw5Dcf"
      },
      "source": [
        "model_name = \"124M\"\n",
        "base_dir     = \"/content/drive/My Drive/Colab Notebooks/gpt2_learning\"\n",
        "model_dir    = os.path.join(base_dir,\"models\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3F_B1VF40Hu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "115d8aa4-0147-406d-9946-b81a36019c5c"
      },
      "source": [
        "#### FineTuning\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.finetune(sess,\n",
        "              file_name,\n",
        "\t\t\t\t\t\t\tmodel_dir=model_dir,\n",
        "              model_name=model_name,\n",
        "\t\t\t\t\t\t\tcheckpoint_dir=os.path.join(base_dir,\"checkpoint\"),\n",
        "              steps=1000)   # steps is max number of training steps\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint /content/drive/My Drive/Colab Notebooks/gpt2_learning/models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/Colab Notebooks/gpt2_learning/models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 338025 tokens\n",
            "Training...\n",
            "[1 | 8.43] loss=4.07 avg=4.07\n",
            "[2 | 10.69] loss=3.82 avg=3.94\n",
            "[3 | 12.96] loss=3.68 avg=3.85\n",
            "[4 | 15.25] loss=3.66 avg=3.81\n",
            "[5 | 17.56] loss=3.73 avg=3.79\n",
            "[6 | 19.86] loss=3.85 avg=3.80\n",
            "[7 | 22.18] loss=3.85 avg=3.81\n",
            "[8 | 24.50] loss=3.68 avg=3.79\n",
            "[9 | 26.83] loss=3.79 avg=3.79\n",
            "[10 | 29.17] loss=3.40 avg=3.75\n",
            "[11 | 31.55] loss=3.40 avg=3.72\n",
            "[12 | 33.94] loss=3.88 avg=3.73\n",
            "[13 | 36.35] loss=3.48 avg=3.71\n",
            "[14 | 38.78] loss=3.51 avg=3.70\n",
            "[15 | 41.24] loss=3.64 avg=3.69\n",
            "[16 | 43.69] loss=3.42 avg=3.67\n",
            "[17 | 46.15] loss=3.55 avg=3.67\n",
            "[18 | 48.64] loss=3.33 avg=3.65\n",
            "[19 | 51.16] loss=3.58 avg=3.64\n",
            "[20 | 53.70] loss=3.65 avg=3.64\n",
            "[21 | 56.25] loss=3.38 avg=3.63\n",
            "[22 | 58.80] loss=3.49 avg=3.62\n",
            "[23 | 61.35] loss=3.52 avg=3.62\n",
            "[24 | 63.91] loss=3.33 avg=3.60\n",
            "[25 | 66.46] loss=3.60 avg=3.60\n",
            "[26 | 68.99] loss=3.48 avg=3.60\n",
            "[27 | 71.50] loss=3.66 avg=3.60\n",
            "[28 | 73.98] loss=3.42 avg=3.59\n",
            "[29 | 76.45] loss=3.18 avg=3.58\n",
            "[30 | 78.89] loss=3.35 avg=3.57\n",
            "[31 | 81.34] loss=3.39 avg=3.56\n",
            "[32 | 83.79] loss=3.29 avg=3.55\n",
            "[33 | 86.22] loss=3.48 avg=3.55\n",
            "[34 | 88.65] loss=3.45 avg=3.55\n",
            "[35 | 91.06] loss=3.36 avg=3.54\n",
            "[36 | 93.47] loss=3.47 avg=3.54\n",
            "[37 | 95.88] loss=3.30 avg=3.53\n",
            "[38 | 98.30] loss=3.51 avg=3.53\n",
            "[39 | 100.70] loss=3.43 avg=3.53\n",
            "[40 | 103.11] loss=3.41 avg=3.52\n",
            "[41 | 105.52] loss=3.40 avg=3.52\n",
            "[42 | 107.94] loss=3.41 avg=3.51\n",
            "[43 | 110.36] loss=3.38 avg=3.51\n",
            "[44 | 112.79] loss=3.15 avg=3.50\n",
            "[45 | 115.22] loss=3.46 avg=3.50\n",
            "[46 | 117.67] loss=3.16 avg=3.49\n",
            "[47 | 120.10] loss=3.44 avg=3.49\n",
            "[48 | 122.55] loss=3.27 avg=3.48\n",
            "[49 | 125.00] loss=3.43 avg=3.48\n",
            "[50 | 127.45] loss=3.38 avg=3.48\n",
            "[51 | 129.91] loss=3.35 avg=3.48\n",
            "[52 | 132.35] loss=3.39 avg=3.47\n",
            "[53 | 134.82] loss=3.41 avg=3.47\n",
            "[54 | 137.28] loss=3.57 avg=3.48\n",
            "[55 | 139.75] loss=3.07 avg=3.47\n",
            "[56 | 142.22] loss=3.23 avg=3.46\n",
            "[57 | 144.69] loss=3.34 avg=3.46\n",
            "[58 | 147.17] loss=3.14 avg=3.45\n",
            "[59 | 149.64] loss=3.21 avg=3.44\n",
            "[60 | 152.11] loss=3.33 avg=3.44\n",
            "[61 | 154.58] loss=3.43 avg=3.44\n",
            "[62 | 157.02] loss=3.31 avg=3.44\n",
            "[63 | 159.47] loss=3.11 avg=3.43\n",
            "[64 | 161.93] loss=3.21 avg=3.43\n",
            "[65 | 164.37] loss=3.26 avg=3.42\n",
            "[66 | 166.83] loss=3.01 avg=3.42\n",
            "[67 | 169.27] loss=3.31 avg=3.41\n",
            "[68 | 171.72] loss=3.20 avg=3.41\n",
            "[69 | 174.17] loss=3.09 avg=3.40\n",
            "[70 | 176.61] loss=3.35 avg=3.40\n",
            "[71 | 179.04] loss=3.01 avg=3.39\n",
            "[72 | 181.48] loss=3.24 avg=3.39\n",
            "[73 | 183.93] loss=3.26 avg=3.39\n",
            "[74 | 186.38] loss=2.97 avg=3.38\n",
            "[75 | 188.82] loss=3.03 avg=3.37\n",
            "[76 | 191.27] loss=3.15 avg=3.37\n",
            "[77 | 193.70] loss=3.13 avg=3.37\n",
            "[78 | 196.15] loss=3.26 avg=3.36\n",
            "[79 | 198.59] loss=3.30 avg=3.36\n",
            "[80 | 201.03] loss=2.98 avg=3.36\n",
            "[81 | 203.49] loss=3.03 avg=3.35\n",
            "[82 | 205.94] loss=3.34 avg=3.35\n",
            "[83 | 208.38] loss=3.33 avg=3.35\n",
            "[84 | 210.82] loss=3.13 avg=3.35\n",
            "[85 | 213.27] loss=3.14 avg=3.34\n",
            "[86 | 215.72] loss=3.10 avg=3.34\n",
            "[87 | 218.17] loss=3.04 avg=3.33\n",
            "[88 | 220.63] loss=2.97 avg=3.33\n",
            "[89 | 223.07] loss=3.15 avg=3.32\n",
            "[90 | 225.52] loss=3.14 avg=3.32\n",
            "[91 | 227.97] loss=3.19 avg=3.32\n",
            "[92 | 230.42] loss=3.15 avg=3.31\n",
            "[93 | 232.87] loss=3.11 avg=3.31\n",
            "[94 | 235.32] loss=3.52 avg=3.32\n",
            "[95 | 237.77] loss=3.32 avg=3.32\n",
            "[96 | 240.22] loss=3.14 avg=3.31\n",
            "[97 | 242.68] loss=3.09 avg=3.31\n",
            "[98 | 245.12] loss=3.26 avg=3.31\n",
            "[99 | 247.57] loss=3.01 avg=3.30\n",
            "[100 | 250.02] loss=3.23 avg=3.30\n",
            "======== SAMPLE 1 ========\n",
            " had, in the name of faith, to give up in some other, I have said it is for this good sense of man to be given up, and to be banished, and I will give it up for my friend, which I never would had;\n",
            "The good news is, my friend, 'tis my house now, all the time being ready.\n",
            "\n",
            "BUCKINGHAM:\n",
            "It comes in your hand a fresh coat, sir: and it is very well; for you must be gone,\n",
            "As I'll be gone.\n",
            "\n",
            "SAMUELIA:\n",
            "I'll be gone to you and be gone to myself.\n",
            "\n",
            "COMINIUS:\n",
            "Wherefore do you go, sir? we'll go about.\n",
            "\n",
            "JULIET:\n",
            "No, sir; and if I go I shall go, and that I will.\n",
            "\n",
            "PROSPERO:\n",
            "Do tell your brother we'll go; and we shall we go till our mother's bed.\n",
            "\n",
            "MERCUTIO:\n",
            "Do tell your brother we'll go; and we shall we go till our mother's bed.\n",
            "\n",
            "COMINIUS:\n",
            "I love one thing more\n",
            "Then do it for it ever! I never thought I'd love another;\n",
            "Yet, like a dream, I'll let the dream take on a mind;\n",
            "And in the dream, in the dream, and in my good sense\n",
            "That I love this, I'll take the word that I have.\n",
            "\n",
            "MERCUTIO:\n",
            "No, no; do give it up for your sister; your sister is to be gone\n",
            "When she is to stay, and never will stay.\n",
            "But what's your sister at home?\n",
            "\n",
            "ROMEO:\n",
            "She is in Rome; and I shall be here,\n",
            "And stay not in Paris.\n",
            "\n",
            "JULIET:\n",
            "Away, now, let's off.\n",
            "\n",
            "COMINIUS:\n",
            "And tell me, who is it you say is the cause you're to stay?\n",
            "Who knows I'll stay there? I'll go, and live in that way\n",
            "You might be told, and I'll lie not down at Rome;\n",
            "I'll be gone, and stay not in Rome:\n",
            "The time when I come to Rome, your sister is gone.\n",
            "\n",
            "MARCIUS:\n",
            "Why, you have deceived me! you have deceived me,\n",
            "I should say, your sister was a traitor to my mother's\n",
            "Faith; you did think you were going.\n",
            "\n",
            "MARCIUS:\n",
            "Well, how come you stay?\n",
            "\n",
            "ROMEO:\n",
            "I cannot speak, but I said go; and you will go; for you\n",
            "Will come and stay, or they'll have none to do,\n",
            "Unless you were a traitor.\n",
            "\n",
            "JULIET:\n",
            "\n",
            "MERCUTIO:\n",
            "Then how came you stay then, when I come?\n",
            "\n",
            "COMINIUS:\n",
            "But they were two.\n",
            "\n",
            "JULIET:\n",
            "One of them you will stay with, they being two: and\n",
            "\n",
            "ROMEO:\n",
            "One of them to come to you, they being two.\n",
            "\n",
            "JULIET:\n",
            "I'll stay by you, I'll stay;\n",
            "And you will not stay, you'll leave.\n",
            "\n",
            "ROMEO:\n",
            "'Tis you shall love me and I'll die before the love of\n",
            "you.\n",
            "\n",
            "ROMEO:\n",
            "'Tis not enough.\n",
            "\n",
            "COMINIUS:\n",
            "\n",
            "MERCUTIO:\n",
            "You are to be my daughter and I to myself.\n",
            "\n",
            "ROMEO:\n",
            "'Tis not enough.\n",
            "\n",
            "MERCUTIO:\n",
            "You and I must go out, you and they shall not go.\n",
            "\n",
            "JULIET:\n",
            "And will't stay and be gone? to the best\n",
            "way they may. '\n",
            "\n",
            "KATHARINA:\n",
            "'Tis enough!' is love love 'tis enough!\n",
            "\n",
            "ROMEO:\n",
            "'Tis enough!' is love love! and love love love! a little love!\n",
            "\n",
            "KATHARINA:\n",
            "'Tis enough!' is love love! love love love!\n",
            "A little love! and a little love! and love love love! love love love!\n",
            "But if they would go, they shall go!\n",
            "Which, for me, is a pretty way to go!\n",
            "And you'll not go too near of them,--they will not go:\n",
            "But, when you have the whole troop out,\n",
            "You have many of them:\n",
            "And then you shall have a troop; some\n",
            "of them there, some, some--\n",
            "A few will be gone; and if a hundred, a\n",
            "few, if all, they shall leave your sister in Rome;\n",
            "'tis just the same way: but the troop, as the troop is, is\n",
            "not enough. The troop, a troop it is\n",
            "\n",
            "[101 | 263.98] loss=2.88 avg=3.30\n",
            "[102 | 266.43] loss=3.22 avg=3.29\n",
            "[103 | 268.88] loss=3.47 avg=3.30\n",
            "[104 | 271.34] loss=3.13 avg=3.29\n",
            "[105 | 273.79] loss=3.17 avg=3.29\n",
            "[106 | 276.24] loss=2.92 avg=3.29\n",
            "[107 | 278.69] loss=3.12 avg=3.28\n",
            "[108 | 281.14] loss=3.26 avg=3.28\n",
            "[109 | 283.59] loss=3.38 avg=3.29\n",
            "[110 | 286.06] loss=2.96 avg=3.28\n",
            "[111 | 288.51] loss=3.13 avg=3.28\n",
            "[112 | 290.96] loss=2.94 avg=3.27\n",
            "[113 | 293.41] loss=3.45 avg=3.28\n",
            "[114 | 295.87] loss=3.11 avg=3.27\n",
            "[115 | 298.32] loss=3.05 avg=3.27\n",
            "[116 | 300.79] loss=2.96 avg=3.27\n",
            "[117 | 303.24] loss=3.36 avg=3.27\n",
            "[118 | 305.71] loss=3.04 avg=3.26\n",
            "[119 | 308.18] loss=3.08 avg=3.26\n",
            "[120 | 310.63] loss=2.87 avg=3.26\n",
            "[121 | 313.10] loss=3.08 avg=3.25\n",
            "[122 | 315.57] loss=3.10 avg=3.25\n",
            "[123 | 318.02] loss=2.99 avg=3.25\n",
            "[124 | 320.47] loss=3.10 avg=3.25\n",
            "[125 | 322.92] loss=3.21 avg=3.24\n",
            "[126 | 325.37] loss=3.14 avg=3.24\n",
            "[127 | 327.84] loss=3.08 avg=3.24\n",
            "[128 | 330.29] loss=3.25 avg=3.24\n",
            "[129 | 332.74] loss=3.00 avg=3.24\n",
            "[130 | 335.20] loss=3.21 avg=3.24\n",
            "[131 | 337.65] loss=2.98 avg=3.23\n",
            "[132 | 340.10] loss=3.03 avg=3.23\n",
            "[133 | 342.55] loss=3.22 avg=3.23\n",
            "[134 | 345.00] loss=3.15 avg=3.23\n",
            "[135 | 347.45] loss=3.25 avg=3.23\n",
            "[136 | 349.90] loss=3.14 avg=3.23\n",
            "[137 | 352.35] loss=3.28 avg=3.23\n",
            "[138 | 354.81] loss=3.26 avg=3.23\n",
            "[139 | 357.25] loss=3.00 avg=3.23\n",
            "[140 | 359.70] loss=3.25 avg=3.23\n",
            "[141 | 362.15] loss=2.80 avg=3.22\n",
            "[142 | 364.60] loss=2.89 avg=3.22\n",
            "[143 | 367.04] loss=3.03 avg=3.21\n",
            "[144 | 369.49] loss=3.09 avg=3.21\n",
            "[145 | 371.94] loss=2.91 avg=3.21\n",
            "[146 | 374.40] loss=3.13 avg=3.21\n",
            "[147 | 376.84] loss=2.76 avg=3.20\n",
            "[148 | 379.29] loss=3.21 avg=3.20\n",
            "[149 | 381.75] loss=3.06 avg=3.20\n",
            "[150 | 384.19] loss=2.92 avg=3.20\n",
            "[151 | 386.65] loss=2.91 avg=3.19\n",
            "[152 | 389.10] loss=2.91 avg=3.19\n",
            "[153 | 391.55] loss=2.98 avg=3.19\n",
            "[154 | 394.00] loss=3.00 avg=3.18\n",
            "[155 | 396.45] loss=3.02 avg=3.18\n",
            "[156 | 398.90] loss=2.57 avg=3.17\n",
            "[157 | 401.35] loss=2.85 avg=3.17\n",
            "[158 | 403.80] loss=3.08 avg=3.17\n",
            "[159 | 406.26] loss=2.99 avg=3.17\n",
            "[160 | 408.71] loss=2.88 avg=3.16\n",
            "[161 | 411.18] loss=3.08 avg=3.16\n",
            "[162 | 413.63] loss=3.09 avg=3.16\n",
            "[163 | 416.08] loss=2.63 avg=3.16\n",
            "[164 | 418.54] loss=2.90 avg=3.15\n",
            "[165 | 421.00] loss=2.89 avg=3.15\n",
            "[166 | 423.47] loss=2.90 avg=3.15\n",
            "[167 | 425.93] loss=2.83 avg=3.14\n",
            "[168 | 428.40] loss=2.98 avg=3.14\n",
            "[169 | 430.85] loss=2.82 avg=3.14\n",
            "[170 | 433.30] loss=2.96 avg=3.13\n",
            "[171 | 435.75] loss=2.91 avg=3.13\n",
            "[172 | 438.19] loss=3.19 avg=3.13\n",
            "[173 | 440.65] loss=3.16 avg=3.13\n",
            "[174 | 443.10] loss=2.96 avg=3.13\n",
            "[175 | 445.55] loss=3.20 avg=3.13\n",
            "[176 | 448.00] loss=3.08 avg=3.13\n",
            "[177 | 450.45] loss=3.06 avg=3.13\n",
            "[178 | 452.90] loss=2.79 avg=3.13\n",
            "[179 | 455.35] loss=2.81 avg=3.12\n",
            "[180 | 457.80] loss=2.95 avg=3.12\n",
            "[181 | 460.25] loss=2.80 avg=3.12\n",
            "[182 | 462.70] loss=2.95 avg=3.11\n",
            "[183 | 465.15] loss=2.89 avg=3.11\n",
            "[184 | 467.60] loss=3.34 avg=3.11\n",
            "[185 | 470.05] loss=3.18 avg=3.11\n",
            "[186 | 472.50] loss=3.15 avg=3.11\n",
            "[187 | 474.96] loss=2.83 avg=3.11\n",
            "[188 | 477.41] loss=2.88 avg=3.11\n",
            "[189 | 479.86] loss=2.79 avg=3.11\n",
            "[190 | 482.31] loss=2.88 avg=3.10\n",
            "[191 | 484.76] loss=3.11 avg=3.10\n",
            "[192 | 487.21] loss=2.85 avg=3.10\n",
            "[193 | 489.66] loss=2.88 avg=3.10\n",
            "[194 | 492.11] loss=3.00 avg=3.10\n",
            "[195 | 494.56] loss=2.89 avg=3.09\n",
            "[196 | 497.01] loss=2.89 avg=3.09\n",
            "[197 | 499.45] loss=2.88 avg=3.09\n",
            "[198 | 501.91] loss=3.05 avg=3.09\n",
            "[199 | 504.36] loss=2.98 avg=3.09\n",
            "[200 | 506.81] loss=2.97 avg=3.09\n",
            "======== SAMPLE 1 ========\n",
            " cle\n",
            "Than any man I shall ever know.\n",
            "But here's my sister: my heart would\n",
            "Had never been that coldly-cold. The gods\n",
            "Hath made me this a comfort. God save you\n",
            "From hell; save me from the curses I'll ever\n",
            "Have.\n",
            "\n",
            "JOHN OF GAUNT:\n",
            "So, sir, you have no friends but yourselves;\n",
            "For what you know, and what you hear, I do\n",
            "Might prove your friends. But I hear you speak of them rather\n",
            "Than I do of any single thing. Come, ladies, let's go:\n",
            "I'll to my grave to kiss my father's hand.\n",
            "\n",
            "First Citizen:\n",
            "I'll go.\n",
            "\n",
            "JOHN OF GAUNT:\n",
            "But, if your friends have friends,\n",
            "'Tis known you have friends.\n",
            "\n",
            "First Citizen:\n",
            "I thought they have friends: they have friends\n",
            "For they are friends.\n",
            "\n",
            "Second Citizen:\n",
            "What do they think, that think\n",
            "And would say 'No'?\n",
            "\n",
            "Third Citizen:\n",
            "Ay, and would say 'No.'\n",
            "\n",
            "Third Citizen:\n",
            "Ay, and would say 'No.'\n",
            "\n",
            "JOHN OF GAUNT:\n",
            "O, how they have thought?\n",
            "\n",
            "JOHN OF GAUNT:\n",
            "And they have thought\n",
            "Which it cannot be but by faith to believe\n",
            "I fear they would say 'No.'\n",
            "\n",
            "Second Citizen:\n",
            "Ay, ay, not by faith to believe.\n",
            "\n",
            "JOHN OF GAUNT:\n",
            "Well, I'll be your companion.\n",
            "Will you go on then?\n",
            "Must I then walk in my own way?\n",
            "Myself, would I, if not of you.\n",
            "\n",
            "CUTNUTANT:\n",
            "I am sorry now, but you have come to\n",
            "Unwieldy; but I prithee, if you will\n",
            "Go, I'll be with you.\n",
            "\n",
            "Second Citizen:\n",
            "I love this place much, and would not,\n",
            "Unless you should leave me.\n",
            "\n",
            "Second Citizen:\n",
            "To see you all well, sir,\n",
            "To be well prepared for me. Please sir,\n",
            "Go.\n",
            "\n",
            "JOHN OF GAUNT:\n",
            "Sir, 'twas not in my grace;\n",
            "Myself, I pray, I know not what\n",
            "To do, what I know,\n",
            "If 'tw hereafter you be friends again\n",
            "You must go; and I know you\n",
            "Will not live to see them. For as I say:\n",
            "I know not what to do; if\n",
            "You were ever but friends\n",
            "As I am now, so I say,\n",
            "I will not live to see them.\n",
            "\n",
            "GREY:\n",
            "You shall;\n",
            "For this I am glad to be friend with you.\n",
            "\n",
            "JOHN OF GAUNT:\n",
            "God bless you all.\n",
            "\n",
            "GREY:\n",
            "Go along.\n",
            "\n",
            "CUTNUTANT:\n",
            "Come, gentlemen.\n",
            "I know not what to do.\n",
            "\n",
            "JULIET:\n",
            "I know not what to do.\n",
            "\n",
            "PETER:\n",
            "What do you suppose?\n",
            "\n",
            "JULIET:\n",
            "God save you gentlemen: no; you may; if any thing else\n",
            "May be avoid'd with me by you, I will obey.\n",
            "\n",
            "PETER:\n",
            "Do you say\n",
            "He should go, that you must go?\n",
            "\n",
            "JULIET:\n",
            "O, but no more.\n",
            "\n",
            "JOHN OF GAUNT:\n",
            "Why, so that I should answer,\n",
            "If he should be prevented from coming home, thereupon\n",
            "I must tell you as quickly of that necessity\n",
            "As it is now, that we shall be all right friends.\n",
            "\n",
            "PETER:\n",
            "Why not soon?\n",
            "\n",
            "JULIET:\n",
            "At once; as we shall know.\n",
            "\n",
            "PETER:\n",
            "What is the matter?\n",
            "\n",
            "JULIET:\n",
            "'Tis some time since, sir.\n",
            "\n",
            "PETER:\n",
            "Then he did not come home.\n",
            "\n",
            "JULIET:\n",
            "So, then, by his faith, it comes\n",
            "Upon't.\n",
            "\n",
            "PETER:\n",
            "And then.\n",
            "\n",
            "JULIET:\n",
            "I prithee: that which I prithee\n",
            "Would have told you the day before.\n",
            "\n",
            "PETER:\n",
            "Why, sir, I prithee, I prithee:\n",
            "By faith, as I prithee pritheth\n",
            "And all that I know is false, prithee pritheth,\n",
            "I have reason to say, 'twixt heaven and earth'\n",
            "That it is a truth to prithee:\n",
            "And therefore here in my soul,\n",
            "I prithee, I hold that which is\n",
            "That prithee pritheth. Thus it is:\n",
            "I pray you, sir, what can I do\n",
            "Which I am sure is false?\n",
            "\n",
            "PETER:\n",
            "Well\n",
            "\n",
            "[201 | 519.40] loss=2.86 avg=3.08\n",
            "[202 | 521.83] loss=2.66 avg=3.08\n",
            "[203 | 524.26] loss=2.90 avg=3.08\n",
            "[204 | 526.69] loss=2.92 avg=3.07\n",
            "[205 | 529.14] loss=2.99 avg=3.07\n",
            "[206 | 531.57] loss=2.70 avg=3.07\n",
            "[207 | 534.02] loss=3.06 avg=3.07\n",
            "[208 | 536.46] loss=2.74 avg=3.07\n",
            "[209 | 538.90] loss=2.97 avg=3.06\n",
            "[210 | 541.36] loss=3.26 avg=3.07\n",
            "[211 | 543.81] loss=2.79 avg=3.06\n",
            "[212 | 546.26] loss=3.03 avg=3.06\n",
            "[213 | 548.71] loss=2.83 avg=3.06\n",
            "[214 | 551.16] loss=2.87 avg=3.06\n",
            "[215 | 553.62] loss=2.89 avg=3.06\n",
            "[216 | 556.07] loss=3.04 avg=3.06\n",
            "[217 | 558.52] loss=3.08 avg=3.06\n",
            "[218 | 560.97] loss=2.94 avg=3.05\n",
            "[219 | 563.42] loss=2.93 avg=3.05\n",
            "[220 | 565.87] loss=2.69 avg=3.05\n",
            "[221 | 568.32] loss=2.59 avg=3.04\n",
            "[222 | 570.77] loss=3.02 avg=3.04\n",
            "[223 | 573.22] loss=2.90 avg=3.04\n",
            "[224 | 575.68] loss=2.96 avg=3.04\n",
            "[225 | 578.14] loss=2.66 avg=3.04\n",
            "[226 | 580.60] loss=3.09 avg=3.04\n",
            "[227 | 583.05] loss=2.74 avg=3.03\n",
            "[228 | 585.50] loss=2.91 avg=3.03\n",
            "[229 | 587.95] loss=2.74 avg=3.03\n",
            "[230 | 590.42] loss=2.72 avg=3.03\n",
            "[231 | 592.87] loss=2.81 avg=3.02\n",
            "[232 | 595.34] loss=2.60 avg=3.02\n",
            "[233 | 597.79] loss=3.10 avg=3.02\n",
            "[234 | 600.26] loss=3.15 avg=3.02\n",
            "[235 | 602.71] loss=3.13 avg=3.02\n",
            "[236 | 605.17] loss=2.60 avg=3.02\n",
            "[237 | 607.64] loss=3.25 avg=3.02\n",
            "[238 | 610.09] loss=2.75 avg=3.02\n",
            "[239 | 612.54] loss=2.75 avg=3.01\n",
            "[240 | 614.99] loss=2.64 avg=3.01\n",
            "[241 | 617.44] loss=2.84 avg=3.01\n",
            "[242 | 619.90] loss=2.65 avg=3.00\n",
            "[243 | 622.35] loss=2.97 avg=3.00\n",
            "[244 | 624.80] loss=2.78 avg=3.00\n",
            "[245 | 627.25] loss=2.74 avg=3.00\n",
            "[246 | 629.70] loss=2.82 avg=3.00\n",
            "[247 | 632.15] loss=2.88 avg=3.00\n",
            "[248 | 634.60] loss=2.76 avg=2.99\n",
            "[249 | 637.05] loss=2.93 avg=2.99\n",
            "[250 | 639.50] loss=2.75 avg=2.99\n",
            "[251 | 641.94] loss=2.65 avg=2.99\n",
            "[252 | 644.39] loss=2.75 avg=2.98\n",
            "[253 | 646.84] loss=2.71 avg=2.98\n",
            "[254 | 649.29] loss=2.92 avg=2.98\n",
            "[255 | 651.74] loss=3.02 avg=2.98\n",
            "[256 | 654.19] loss=2.75 avg=2.98\n",
            "[257 | 656.63] loss=2.66 avg=2.97\n",
            "[258 | 659.07] loss=2.61 avg=2.97\n",
            "[259 | 661.52] loss=2.76 avg=2.97\n",
            "[260 | 663.98] loss=2.85 avg=2.97\n",
            "[261 | 666.43] loss=2.89 avg=2.97\n",
            "[262 | 668.88] loss=2.89 avg=2.97\n",
            "[263 | 671.32] loss=2.69 avg=2.96\n",
            "[264 | 673.76] loss=2.84 avg=2.96\n",
            "[265 | 676.21] loss=2.48 avg=2.96\n",
            "[266 | 678.66] loss=2.49 avg=2.95\n",
            "[267 | 681.10] loss=2.51 avg=2.95\n",
            "[268 | 683.55] loss=2.69 avg=2.94\n",
            "[269 | 686.00] loss=2.50 avg=2.94\n",
            "[270 | 688.45] loss=2.70 avg=2.94\n",
            "[271 | 690.89] loss=2.84 avg=2.94\n",
            "[272 | 693.33] loss=2.81 avg=2.93\n",
            "[273 | 695.78] loss=2.47 avg=2.93\n",
            "[274 | 698.23] loss=2.70 avg=2.93\n",
            "[275 | 700.68] loss=2.97 avg=2.93\n",
            "[276 | 703.13] loss=2.81 avg=2.93\n",
            "[277 | 705.58] loss=2.47 avg=2.92\n",
            "[278 | 708.03] loss=2.61 avg=2.92\n",
            "[279 | 710.49] loss=2.65 avg=2.91\n",
            "[280 | 712.94] loss=2.82 avg=2.91\n",
            "[281 | 715.38] loss=2.72 avg=2.91\n",
            "[282 | 717.83] loss=2.55 avg=2.91\n",
            "[283 | 720.28] loss=2.82 avg=2.91\n",
            "[284 | 722.74] loss=2.59 avg=2.90\n",
            "[285 | 725.19] loss=2.73 avg=2.90\n",
            "[286 | 727.63] loss=2.50 avg=2.90\n",
            "[287 | 730.09] loss=2.53 avg=2.89\n",
            "[288 | 732.54] loss=2.59 avg=2.89\n",
            "[289 | 734.99] loss=2.60 avg=2.89\n",
            "[290 | 737.44] loss=2.27 avg=2.88\n",
            "[291 | 739.89] loss=2.56 avg=2.88\n",
            "[292 | 742.35] loss=2.50 avg=2.87\n",
            "[293 | 744.80] loss=2.87 avg=2.87\n",
            "[294 | 747.25] loss=2.67 avg=2.87\n",
            "[295 | 749.70] loss=2.75 avg=2.87\n",
            "[296 | 752.15] loss=2.97 avg=2.87\n",
            "[297 | 754.60] loss=2.54 avg=2.87\n",
            "[298 | 757.06] loss=2.70 avg=2.87\n",
            "[299 | 759.51] loss=2.73 avg=2.86\n",
            "[300 | 761.96] loss=2.51 avg=2.86\n",
            "======== SAMPLE 1 ========\n",
            " they do not wish to be so, for\n",
            "the world is full of them: 'tis better to live\n",
            "at leisure, or I'll curse you for this,\n",
            "that you speak like a villain: but, sir, when you\n",
            "say you are not like this, it will seem to you, sir,\n",
            "that you lack some intelligence of any account.\n",
            "Take care that you are honest, not like such! I have\n",
            "not been dishonest in prison: I thank God you\n",
            "have not been so corrupt. This is the business; I mean,\n",
            "to serve the business: my brother George's sentence for the\n",
            "pence of the house, his supposed abuse towards my brother,\n",
            "his\n",
            "own brother, and all that makes this business of the house\n",
            "come to an end, will but show me how to do justice\n",
            "at my brother's house, not what I do there.\n",
            "\n",
            "Nurse:\n",
            "How is it that the house's windows are broken, the house's\n",
            "shoulderrests broken and broken, but it is not possible to\n",
            "hose the brother off?\n",
            "\n",
            "GLOUCESTER:\n",
            "The house! he in a great need.\n",
            "\n",
            "Nurse:\n",
            "Here comes his brother!\n",
            "\n",
            "GLOUCESTER:\n",
            "How now, my friends, what's done?\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "The news on the prison walls that the\n",
            "prisoner is gone on good behavior and good\n",
            "behavior, the news from the prison walls that the\n",
            "prisoner and his brother are dead, and he and\n",
            "them both in this great hole at the bottom have\n",
            "one thing in common, that nobody in the house at the bottom\n",
            "is alive; and that nobody but there being\n",
            "there, the prisoner and his brother, must live.\n",
            "\n",
            "GLOUCESTER:\n",
            "Who is that?\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "My brother, my brother; my good brother; my good brother.\n",
            "\n",
            "GLOUCESTER:\n",
            "Why, he's dead and that's the news. The news!\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "What news? what news?\n",
            "\n",
            "GLOUCESTER:\n",
            "What news?\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "What news?\n",
            "\n",
            "GLOUCESTER:\n",
            "What news?\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Most of all. Most of all.\n",
            "\n",
            "GLOUCESTER:\n",
            "What news now?\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Most of all, most of all.\n",
            "\n",
            "GLOUCESTER:\n",
            "It's enough this is all; or it's enough,\n",
            "It and your brother dying in this cell,\n",
            "It and the rest.\n",
            "\n",
            "Nurse:\n",
            "Why, brother Clarence, it was your uncle Clarence that\n",
            "hath made this discovery.\n",
            "\n",
            "GLOUCESTER:\n",
            "He and his brother do possess a thing\n",
            "like nothing but death: and that's all you\n",
            "can hope for.\n",
            "\n",
            "Nurse:\n",
            "O Lord, I am but too full of grief,\n",
            "Too far of joy, as if I had lived,\n",
            "As if it had been my brother's death:\n",
            "So much, Lord, that I would have been better\n",
            "than in his name had it not been for him.\n",
            "\n",
            "GLOUCESTER:\n",
            "Come, come; I am well bereft of myself,\n",
            "Too full of sorrow, as if I were a\n",
            "feather-wagging shrew of grief. I must with grief be put\n",
            "out of here.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "What news?\n",
            "\n",
            "Nurse:\n",
            "Why, good brother, I hear the news.\n",
            "\n",
            "GLOUCESTER:\n",
            "The prisoner hath died; the prison walls are\n",
            "comin'd up again.\n",
            "\n",
            "Nurse:\n",
            "But sir, the prisoner is dead.\n",
            "\n",
            "GLOUCESTER:\n",
            "Where is he? the keeper of the prisoner's cell.\n",
            "\n",
            "Nurse:\n",
            "Here am I found.\n",
            "\n",
            "GLOUCESTER:\n",
            "Have you done this?\n",
            "\n",
            "Widow:\n",
            "I did.\n",
            "\n",
            "Nurse:\n",
            "The prisoner lies dead in that cell,\n",
            "But there he lives to lament the loss of\n",
            "our beloved, and to say that he will be\n",
            "counsel'd to my brother by the house.\n",
            "\n",
            "GLOUCESTER:\n",
            "Why, good widows, now I do see the\n",
            "prison gates do close. They are like to close\n",
            "the cell. I am not able to bring my\n",
            "brother in, nor can he; nor can I stay him here.\n",
            "\n",
            "Clarence:\n",
            "O good widow, this is no thing to sigh at.\n",
            "\n",
            "Widow:\n",
            "You shall not see him\n",
            "\n",
            "[301 | 775.18] loss=2.61 avg=2.86\n",
            "[302 | 777.65] loss=2.44 avg=2.85\n",
            "[303 | 780.10] loss=2.67 avg=2.85\n",
            "[304 | 782.55] loss=2.63 avg=2.85\n",
            "[305 | 784.99] loss=2.52 avg=2.85\n",
            "[306 | 787.45] loss=2.32 avg=2.84\n",
            "[307 | 789.90] loss=2.47 avg=2.84\n",
            "[308 | 792.35] loss=2.85 avg=2.84\n",
            "[309 | 794.79] loss=2.35 avg=2.83\n",
            "[310 | 797.25] loss=2.11 avg=2.82\n",
            "[311 | 799.70] loss=2.36 avg=2.82\n",
            "[312 | 802.15] loss=2.68 avg=2.82\n",
            "[313 | 804.60] loss=2.45 avg=2.81\n",
            "[314 | 807.05] loss=2.50 avg=2.81\n",
            "[315 | 809.49] loss=2.53 avg=2.81\n",
            "[316 | 811.95] loss=2.83 avg=2.81\n",
            "[317 | 814.40] loss=2.53 avg=2.80\n",
            "[318 | 816.84] loss=2.91 avg=2.81\n",
            "[319 | 819.30] loss=2.62 avg=2.80\n",
            "[320 | 821.75] loss=2.27 avg=2.80\n",
            "[321 | 824.20] loss=2.83 avg=2.80\n",
            "[322 | 826.66] loss=2.39 avg=2.79\n",
            "[323 | 829.11] loss=2.55 avg=2.79\n",
            "[324 | 831.56] loss=2.83 avg=2.79\n",
            "[325 | 834.01] loss=2.85 avg=2.79\n",
            "[326 | 836.48] loss=2.60 avg=2.79\n",
            "[327 | 838.93] loss=2.49 avg=2.79\n",
            "[328 | 841.41] loss=2.66 avg=2.79\n",
            "[329 | 843.87] loss=2.46 avg=2.78\n",
            "[330 | 846.34] loss=2.67 avg=2.78\n",
            "[331 | 848.80] loss=2.45 avg=2.78\n",
            "[332 | 851.26] loss=2.42 avg=2.77\n",
            "[333 | 853.72] loss=2.56 avg=2.77\n",
            "[334 | 856.17] loss=2.43 avg=2.77\n",
            "[335 | 858.62] loss=2.59 avg=2.77\n",
            "[336 | 861.07] loss=2.55 avg=2.76\n",
            "[337 | 863.52] loss=2.45 avg=2.76\n",
            "[338 | 865.97] loss=2.49 avg=2.76\n",
            "[339 | 868.42] loss=2.36 avg=2.75\n",
            "[340 | 870.87] loss=2.47 avg=2.75\n",
            "[341 | 873.31] loss=2.53 avg=2.75\n",
            "[342 | 875.76] loss=2.52 avg=2.75\n",
            "[343 | 878.21] loss=2.26 avg=2.74\n",
            "[344 | 880.65] loss=2.80 avg=2.74\n",
            "[345 | 883.10] loss=2.70 avg=2.74\n",
            "[346 | 885.55] loss=2.58 avg=2.74\n",
            "[347 | 888.00] loss=2.42 avg=2.74\n",
            "[348 | 890.45] loss=2.46 avg=2.73\n",
            "[349 | 892.90] loss=2.32 avg=2.73\n",
            "[350 | 895.35] loss=2.45 avg=2.73\n",
            "[351 | 897.81] loss=2.25 avg=2.72\n",
            "[352 | 900.25] loss=2.52 avg=2.72\n",
            "[353 | 902.70] loss=2.25 avg=2.72\n",
            "[354 | 905.15] loss=2.59 avg=2.71\n",
            "[355 | 907.60] loss=2.53 avg=2.71\n",
            "[356 | 910.05] loss=2.36 avg=2.71\n",
            "[357 | 912.50] loss=2.35 avg=2.70\n",
            "[358 | 914.95] loss=2.54 avg=2.70\n",
            "[359 | 917.40] loss=2.20 avg=2.70\n",
            "[360 | 919.85] loss=2.33 avg=2.69\n",
            "[361 | 922.30] loss=2.72 avg=2.69\n",
            "[362 | 924.75] loss=2.19 avg=2.69\n",
            "[363 | 927.19] loss=2.31 avg=2.69\n",
            "[364 | 929.64] loss=2.37 avg=2.68\n",
            "[365 | 932.09] loss=2.86 avg=2.68\n",
            "[366 | 934.54] loss=2.30 avg=2.68\n",
            "[367 | 936.99] loss=2.50 avg=2.68\n",
            "[368 | 939.44] loss=2.22 avg=2.67\n",
            "[369 | 941.89] loss=2.37 avg=2.67\n",
            "[370 | 944.33] loss=2.45 avg=2.67\n",
            "[371 | 946.79] loss=2.54 avg=2.67\n",
            "[372 | 949.24] loss=2.42 avg=2.66\n",
            "[373 | 951.69] loss=2.25 avg=2.66\n",
            "[374 | 954.14] loss=2.23 avg=2.66\n",
            "[375 | 956.59] loss=2.24 avg=2.65\n",
            "[376 | 959.04] loss=2.50 avg=2.65\n",
            "[377 | 961.49] loss=2.55 avg=2.65\n",
            "[378 | 963.94] loss=2.51 avg=2.65\n",
            "[379 | 966.39] loss=2.50 avg=2.65\n",
            "[380 | 968.84] loss=2.39 avg=2.64\n",
            "[381 | 971.29] loss=2.44 avg=2.64\n",
            "[382 | 973.74] loss=2.34 avg=2.64\n",
            "[383 | 976.19] loss=2.44 avg=2.64\n",
            "[384 | 978.64] loss=2.49 avg=2.63\n",
            "[385 | 981.09] loss=1.92 avg=2.63\n",
            "[386 | 983.54] loss=2.09 avg=2.62\n",
            "[387 | 985.99] loss=2.44 avg=2.62\n",
            "[388 | 988.44] loss=2.43 avg=2.62\n",
            "[389 | 990.90] loss=2.44 avg=2.62\n",
            "[390 | 993.35] loss=2.41 avg=2.61\n",
            "[391 | 995.80] loss=2.43 avg=2.61\n",
            "[392 | 998.25] loss=2.05 avg=2.61\n",
            "[393 | 1000.70] loss=2.01 avg=2.60\n",
            "[394 | 1003.15] loss=2.37 avg=2.60\n",
            "[395 | 1005.60] loss=2.31 avg=2.60\n",
            "[396 | 1008.05] loss=2.71 avg=2.60\n",
            "[397 | 1010.50] loss=2.06 avg=2.59\n",
            "[398 | 1012.95] loss=2.30 avg=2.59\n",
            "[399 | 1015.41] loss=2.77 avg=2.59\n",
            "[400 | 1017.86] loss=2.45 avg=2.59\n",
            "======== SAMPLE 1 ========\n",
            " please take me with you;\n",
            "As you are our master, and are but of your first;\n",
            "And now, if you can, let our trusty lords\n",
            "Stay in this place till the royal ladies\n",
            "Return to their bended knee.\n",
            "\n",
            "KING FIONA:\n",
            "My gracious queen, and all my subjects,\n",
            "Will to your majesty's liking: I would not stay\n",
            "To see this feast and do my part,\n",
            "For I must depart again.\n",
            "\n",
            "Provost:\n",
            "It shall be thought now most willingly,\n",
            "That you return to your former prerogative.\n",
            "\n",
            "Groom:\n",
            "What say ye, my sovereign?\n",
            "\n",
            "Provost:\n",
            "No.\n",
            "\n",
            "Groom:\n",
            "No more.\n",
            "\n",
            "Provost:\n",
            "No more.\n",
            "\n",
            "Groom:\n",
            "Nothing was so promised aught.\n",
            "\n",
            "Provost:\n",
            "And, by mine own instruction, by the royalists\n",
            "To save as much as are due you, I mean to stay.\n",
            "\n",
            "Provost:\n",
            "What then?\n",
            "\n",
            "Groom:\n",
            "Nothing, my sovereign, but we must to visit the Prince.\n",
            "\n",
            "Provost:\n",
            "Not the needful, my gracious queen, but the more\n",
            "To help with these ill-favoured means towards\n",
            "The gracious prince's good.\n",
            "\n",
            "Groom:\n",
            "O, but he can do't!\n",
            "Let us make no ado before the royal!\n",
            "\n",
            "Provost:\n",
            "The prince shall be come with answer:\n",
            "You mean to go to the poor prince.\n",
            "\n",
            "Provost:\n",
            "The prince wants to leave me; I'll be gone.\n",
            "\n",
            "Groom:\n",
            "By him will I yield up my sword,\n",
            "The prince will have me cut the grass to be\n",
            "That instrument for him to use.\n",
            "\n",
            "Provost:\n",
            "If you mean to leave him, there shall be no ado!\n",
            "\n",
            "Grow a beard, my lord; it is enough.\n",
            "\n",
            "Provost:\n",
            "I have been deceived by the devil.\n",
            "If the prince demand any thing in this land,\n",
            "I will at once yield to their request;\n",
            "For if either demand any thing in that land,\n",
            "I shall be rid of them.\n",
            "\n",
            "Provost:\n",
            "Pray you, take up your sword; it is most likely\n",
            "That your lordship will grant you satisfaction:' though\n",
            "For a kingdom which did but belong to your shoulders,\n",
            "You were by far as much to agree\n",
            "With his demand as his with yours.\n",
            "\n",
            "Groom:\n",
            "The king\n",
            "\n",
            "Provost:\n",
            "Hearts be thanked for the offer'd me:\n",
            "A kingdom of such royal soil must I take\n",
            "And scatter it for the time I live to see,\n",
            "Being all England's.\n",
            "\n",
            "Groom:\n",
            "But he'll find a way! He wants to see\n",
            "The prince cannot endure a life upon't;\n",
            "And it's but time to end this unholy dance.\n",
            "\n",
            "Provost:\n",
            "O very so!\n",
            "Look, the heavens be still upon us and\n",
            "This royal scene change my mind; and yet this state,\n",
            "As the state of a royal being, is not\n",
            "At All Tragedy.\n",
            "\n",
            "Groom:\n",
            "How then, my good lord?\n",
            "\n",
            "Provost:\n",
            "Hereupon, thy royal breast, which lay in\n",
            "Most of all by way of the throne, doth bear this\n",
            "Tyre's name; and now that I see how it bears\n",
            "My lord,--\n",
            "\n",
            "Groom:\n",
            "Tyre!\n",
            "\n",
            "Provost:\n",
            "Tyre! O, let my master speak!\n",
            "\n",
            "Second Watchman:\n",
            "How now! what occasion, my lord?\n",
            "\n",
            "Provost:\n",
            "A man, my lord.\n",
            "\n",
            "Second Watchman:\n",
            "Do you see a quarrel, or a league, here arise?\n",
            "\n",
            "Third Watchman:\n",
            "Tut, no.\n",
            "\n",
            "Provost:\n",
            "Anon!\n",
            "\n",
            "Third Watchman:\n",
            "Away, way, go!\n",
            "\n",
            "First Watchman:\n",
            "See where the devil is here!\n",
            "\n",
            "Second Watchman:\n",
            "Fright, way, the devil!\n",
            "The king doth scorn: 'For God's sake, be quiet.'\n",
            "Where shall we stand?\n",
            "\n",
            "First Watchman:\n",
            "Come hither, O brave lords, like lightning!\n",
            "We are the guards, or gates, of the city, and\n",
            "are to be the subjects; who, with a most heart-shaking\n",
            "day, are to stand upon their heads, and march to us.\n",
            "\n",
            "Second Watchman:\n",
            "Then say no more. We are here, and must\n",
            "do nothing; but be enemies to the king.\n",
            "Away with the king!\n",
            "\n",
            "First Watchman:\n",
            "I pray thee, sir, a word:\n",
            "The queen, we are to be the wives of the king;\n",
            "who now are the dominions of the earth.\n",
            "\n",
            "Second Watch\n",
            "\n",
            "[401 | 1030.55] loss=2.35 avg=2.59\n",
            "[402 | 1033.00] loss=2.36 avg=2.58\n",
            "[403 | 1035.47] loss=2.39 avg=2.58\n",
            "[404 | 1037.92] loss=2.35 avg=2.58\n",
            "[405 | 1040.37] loss=2.19 avg=2.58\n",
            "[406 | 1042.82] loss=2.44 avg=2.57\n",
            "[407 | 1045.26] loss=2.00 avg=2.57\n",
            "[408 | 1047.71] loss=2.37 avg=2.57\n",
            "[409 | 1050.16] loss=2.25 avg=2.56\n",
            "[410 | 1052.62] loss=2.40 avg=2.56\n",
            "[411 | 1055.07] loss=2.08 avg=2.56\n",
            "[412 | 1057.51] loss=2.53 avg=2.56\n",
            "[413 | 1059.97] loss=2.34 avg=2.55\n",
            "[414 | 1062.40] loss=2.19 avg=2.55\n",
            "[415 | 1064.84] loss=2.13 avg=2.55\n",
            "[416 | 1067.30] loss=2.06 avg=2.54\n",
            "[417 | 1069.74] loss=2.29 avg=2.54\n",
            "[418 | 1072.18] loss=2.22 avg=2.54\n",
            "[419 | 1074.64] loss=2.16 avg=2.53\n",
            "[420 | 1077.09] loss=2.07 avg=2.53\n",
            "[421 | 1079.55] loss=2.03 avg=2.52\n",
            "[422 | 1082.00] loss=1.92 avg=2.52\n",
            "[423 | 1084.45] loss=2.02 avg=2.51\n",
            "[424 | 1086.90] loss=2.12 avg=2.51\n",
            "[425 | 1089.35] loss=2.23 avg=2.50\n",
            "[426 | 1091.80] loss=2.42 avg=2.50\n",
            "[427 | 1094.25] loss=1.94 avg=2.50\n",
            "[428 | 1096.70] loss=2.01 avg=2.49\n",
            "[429 | 1099.15] loss=1.96 avg=2.49\n",
            "[430 | 1101.60] loss=2.21 avg=2.48\n",
            "[431 | 1104.05] loss=2.11 avg=2.48\n",
            "[432 | 1106.51] loss=1.96 avg=2.48\n",
            "[433 | 1108.96] loss=2.13 avg=2.47\n",
            "[434 | 1111.40] loss=2.66 avg=2.47\n",
            "[435 | 1113.86] loss=1.89 avg=2.47\n",
            "[436 | 1116.30] loss=2.00 avg=2.46\n",
            "[437 | 1118.76] loss=2.47 avg=2.46\n",
            "[438 | 1121.21] loss=1.87 avg=2.46\n",
            "[439 | 1123.66] loss=1.94 avg=2.45\n",
            "[440 | 1126.11] loss=2.15 avg=2.45\n",
            "[441 | 1128.56] loss=2.07 avg=2.44\n",
            "[442 | 1131.01] loss=2.74 avg=2.45\n",
            "[443 | 1133.48] loss=1.92 avg=2.44\n",
            "[444 | 1135.93] loss=2.09 avg=2.44\n",
            "[445 | 1138.40] loss=1.92 avg=2.43\n",
            "[446 | 1140.87] loss=2.25 avg=2.43\n",
            "[447 | 1143.32] loss=2.02 avg=2.43\n",
            "[448 | 1145.78] loss=1.99 avg=2.42\n",
            "[449 | 1148.25] loss=1.87 avg=2.42\n",
            "[450 | 1150.70] loss=2.25 avg=2.42\n",
            "[451 | 1153.16] loss=2.22 avg=2.41\n",
            "[452 | 1155.61] loss=2.15 avg=2.41\n",
            "[453 | 1158.06] loss=1.85 avg=2.41\n",
            "[454 | 1160.53] loss=2.11 avg=2.40\n",
            "[455 | 1162.98] loss=2.10 avg=2.40\n",
            "[456 | 1165.45] loss=2.03 avg=2.40\n",
            "[457 | 1167.90] loss=2.24 avg=2.39\n",
            "[458 | 1170.35] loss=2.08 avg=2.39\n",
            "[459 | 1172.80] loss=2.19 avg=2.39\n",
            "[460 | 1175.25] loss=1.71 avg=2.38\n",
            "[461 | 1177.70] loss=2.19 avg=2.38\n",
            "[462 | 1180.15] loss=2.17 avg=2.38\n",
            "[463 | 1182.61] loss=1.93 avg=2.37\n",
            "[464 | 1185.07] loss=2.19 avg=2.37\n",
            "[465 | 1187.52] loss=2.02 avg=2.37\n",
            "[466 | 1189.97] loss=1.90 avg=2.36\n",
            "[467 | 1192.42] loss=1.83 avg=2.36\n",
            "[468 | 1194.88] loss=2.08 avg=2.36\n",
            "[469 | 1197.33] loss=2.27 avg=2.35\n",
            "[470 | 1199.78] loss=1.56 avg=2.35\n",
            "[471 | 1202.23] loss=1.99 avg=2.34\n",
            "[472 | 1204.68] loss=1.95 avg=2.34\n",
            "[473 | 1207.13] loss=1.90 avg=2.33\n",
            "[474 | 1209.58] loss=1.72 avg=2.33\n",
            "[475 | 1212.04] loss=1.96 avg=2.32\n",
            "[476 | 1214.48] loss=1.75 avg=2.32\n",
            "[477 | 1216.94] loss=2.10 avg=2.32\n",
            "[478 | 1219.39] loss=1.90 avg=2.31\n",
            "[479 | 1221.84] loss=2.20 avg=2.31\n",
            "[480 | 1224.29] loss=1.74 avg=2.31\n",
            "[481 | 1226.74] loss=1.95 avg=2.30\n",
            "[482 | 1229.19] loss=2.11 avg=2.30\n",
            "[483 | 1231.64] loss=1.76 avg=2.29\n",
            "[484 | 1234.09] loss=1.69 avg=2.29\n",
            "[485 | 1236.56] loss=1.96 avg=2.29\n",
            "[486 | 1239.02] loss=1.91 avg=2.28\n",
            "[487 | 1241.46] loss=1.71 avg=2.28\n",
            "[488 | 1243.92] loss=1.82 avg=2.27\n",
            "[489 | 1246.36] loss=1.76 avg=2.27\n",
            "[490 | 1248.81] loss=1.65 avg=2.26\n",
            "[491 | 1251.27] loss=2.24 avg=2.26\n",
            "[492 | 1253.72] loss=1.94 avg=2.26\n",
            "[493 | 1256.17] loss=2.05 avg=2.25\n",
            "[494 | 1258.62] loss=2.10 avg=2.25\n",
            "[495 | 1261.07] loss=2.02 avg=2.25\n",
            "[496 | 1263.52] loss=2.08 avg=2.25\n",
            "[497 | 1265.96] loss=2.58 avg=2.25\n",
            "[498 | 1268.41] loss=1.85 avg=2.25\n",
            "[499 | 1270.86] loss=1.87 avg=2.24\n",
            "[500 | 1273.31] loss=1.96 avg=2.24\n",
            "======== SAMPLE 1 ========\n",
            "Help to God! let us go!\n",
            "\n",
            "BALTHASAR:\n",
            "\n",
            "SICINIUS:\n",
            "I, sir, may I request you to\n",
            "Accept my pledge?\n",
            "\n",
            "BALTHASAR:\n",
            "He who does this will be more consul,\n",
            "More sovereign, that he shall respect the bawds\n",
            "And all things which they bear.\n",
            "\n",
            "SICINIUS:\n",
            "I will make them pay for me.\n",
            "\n",
            "BALTHASAR:\n",
            "No, in the taste of the bawd.\n",
            "\n",
            "SICINIUS:\n",
            "I would the town were deaf!\n",
            "\n",
            "BALTHASAR:\n",
            "'Tis thought it is an ancient custom;\n",
            "But I have not heard or seen it, and they\n",
            "Never look upon it.\n",
            "\n",
            "SICINIUS:\n",
            "They do,\n",
            "When a beggar desires something that is worthier than\n",
            "A horse; but I have heard 'tis held\n",
            "To go without: there it is; and the bawd,\n",
            "That has the honour to take it off, does it\n",
            "not.\n",
            "\n",
            "BALTHASAR:\n",
            "Now, to think that a fellow\n",
            "Should, for want of money, come and have\n",
            "This thing; which he did give--\n",
            "\n",
            "SICINIUS:\n",
            "Bath and retire! where I come\n",
            "Could not at Venice receive a volume\n",
            "That cheques itself, not paying for itself:\n",
            "The bawd did offer to furnish the town,\n",
            "Else he had been a dinner-boy.\n",
            "This offer was put down: you, a noble man,\n",
            "To know the reason for your firm refusal.\n",
            "\n",
            "BALTHASAR:\n",
            "My father doth know it.\n",
            "In conclusion, it I hear--warrant me nothing,\n",
            "But make your reason freely for your refusal.\n",
            "\n",
            "SICINIUS:\n",
            "This is a noble fellow; and he\n",
            "Kills the gentleman not seeking his money:--\n",
            "In the common applause of his syllable,\n",
            "Like the wind that blows, he does pronounce death upon\n",
            "His client; who by the honour of this\n",
            "answer sits with a heavy burden, and his bones\n",
            "There weighed with his bosom;--\n",
            "Who by that answer doth pronounce death, as he,\n",
            "Under heaven, would be put to death.\n",
            "\n",
            "BALTHASAR:\n",
            "I thought, sir, it were a gentleman's\n",
            "service, i' the bench, to go and see the cause\n",
            "Of my motion, not his, but to be\n",
            "A judge, though his client have received but such proceedings\n",
            "As, to call it so, the gentleman's.\n",
            "\n",
            "SICINIUS:\n",
            "I thought thus, fellow; i' the bench cannot hold\n",
            "More weighty proceedings of a judge's own charge:\n",
            "'Twas he that with all the weight to his charge\n",
            "Doth more.\n",
            "\n",
            "BALTHASAR:\n",
            "I said he was a gentleman:\n",
            "But what a judge is he that hath ever\n",
            "With less than he shall deserve had power and\n",
            "Wherewithal be tried, as men of honour?\n",
            "\n",
            "SICINIUS:\n",
            "Who, sir?\n",
            "I spoke it in your honour's sight; and I confess\n",
            "It was the first time the Volsces did offer\n",
            "To give their dukedom.\n",
            "\n",
            "BALTHASAR:\n",
            "What, sir?\n",
            "I humbly beseech your honour to pardon me.\n",
            "\n",
            "SICINIUS:\n",
            "O, you did speak it untainted:--\n",
            "Had your soul heard nor seen this speech,--\n",
            "You would smile, sir; and so have I;\n",
            "The Volsces' offer being forth, I'll give\n",
            "Your honour the prize; and thus you depart.\n",
            "\n",
            "BALTHASAR:\n",
            "Your honour, and therefore therefore, I beseech you\n",
            "Put not your weakness in your tongue.\n",
            "\n",
            "COMINIUS:\n",
            "I should it\n",
            "The gods make me blush: therefore I pass it.\n",
            "How now! what do you do?\n",
            "\n",
            "SICINIUS:\n",
            "How can you be provoked?\n",
            "\n",
            "SICINIUS:\n",
            "Be brief, gentlemen:\n",
            "We are new-comers, being taught by elders.\n",
            "What do you swear to be true and what are you\n",
            "Defending?\n",
            "\n",
            "COMINIUS:\n",
            "To you!--To me!\n",
            "\n",
            "COMINIUS:\n",
            "You!--not to me!\n",
            "\n",
            "SICINIUS:\n",
            "No, sir, do not speak to me;\n",
            "Nor do you seek to steal my esteem;\n",
            "It is the matter of your honour's, your honour's,--\n",
            "You that are most disdainful of your fellow\n",
            "'Zounds, and will not hold them to your honesty:'\n",
            "'Zounds do make villains:' which is right, sir,\n",
            "When villains are\n",
            "\n",
            "[501 | 1285.86] loss=1.99 avg=2.24\n",
            "[502 | 1288.32] loss=2.17 avg=2.24\n",
            "[503 | 1290.77] loss=1.66 avg=2.23\n",
            "[504 | 1293.22] loss=1.76 avg=2.23\n",
            "[505 | 1295.68] loss=2.27 avg=2.23\n",
            "[506 | 1298.12] loss=2.01 avg=2.23\n",
            "[507 | 1300.57] loss=2.25 avg=2.23\n",
            "[508 | 1303.03] loss=1.63 avg=2.22\n",
            "[509 | 1305.48] loss=2.05 avg=2.22\n",
            "[510 | 1307.92] loss=1.52 avg=2.21\n",
            "[511 | 1310.37] loss=1.75 avg=2.21\n",
            "[512 | 1312.82] loss=1.54 avg=2.20\n",
            "[513 | 1315.27] loss=2.05 avg=2.20\n",
            "[514 | 1317.72] loss=1.63 avg=2.19\n",
            "[515 | 1320.17] loss=1.81 avg=2.19\n",
            "[516 | 1322.62] loss=2.14 avg=2.19\n",
            "[517 | 1325.07] loss=1.80 avg=2.18\n",
            "[518 | 1327.53] loss=1.99 avg=2.18\n",
            "[519 | 1329.98] loss=1.81 avg=2.18\n",
            "[520 | 1332.43] loss=2.15 avg=2.18\n",
            "[521 | 1334.87] loss=1.61 avg=2.17\n",
            "[522 | 1337.33] loss=1.47 avg=2.17\n",
            "[523 | 1339.78] loss=1.55 avg=2.16\n",
            "[524 | 1342.23] loss=1.91 avg=2.16\n",
            "[525 | 1344.68] loss=1.74 avg=2.15\n",
            "[526 | 1347.13] loss=1.78 avg=2.15\n",
            "[527 | 1349.58] loss=1.28 avg=2.14\n",
            "[528 | 1352.03] loss=1.66 avg=2.14\n",
            "[529 | 1354.48] loss=1.71 avg=2.13\n",
            "[530 | 1356.94] loss=2.02 avg=2.13\n",
            "[531 | 1359.39] loss=2.08 avg=2.13\n",
            "[532 | 1361.84] loss=1.39 avg=2.12\n",
            "[533 | 1364.28] loss=1.94 avg=2.12\n",
            "[534 | 1366.73] loss=1.61 avg=2.12\n",
            "[535 | 1369.19] loss=1.87 avg=2.11\n",
            "[536 | 1371.64] loss=1.48 avg=2.11\n",
            "[537 | 1374.09] loss=1.83 avg=2.10\n",
            "[538 | 1376.53] loss=1.50 avg=2.10\n",
            "[539 | 1378.98] loss=1.55 avg=2.09\n",
            "[540 | 1381.43] loss=1.92 avg=2.09\n",
            "[541 | 1383.88] loss=1.51 avg=2.08\n",
            "[542 | 1386.34] loss=2.23 avg=2.09\n",
            "[543 | 1388.79] loss=1.66 avg=2.08\n",
            "[544 | 1391.24] loss=1.77 avg=2.08\n",
            "[545 | 1393.68] loss=1.51 avg=2.07\n",
            "[546 | 1396.14] loss=1.76 avg=2.07\n",
            "[547 | 1398.59] loss=1.81 avg=2.07\n",
            "[548 | 1401.04] loss=1.66 avg=2.06\n",
            "[549 | 1403.49] loss=1.46 avg=2.06\n",
            "[550 | 1405.94] loss=1.98 avg=2.06\n",
            "[551 | 1408.39] loss=1.66 avg=2.05\n",
            "[552 | 1410.84] loss=1.65 avg=2.05\n",
            "[553 | 1413.30] loss=1.82 avg=2.05\n",
            "[554 | 1415.75] loss=1.50 avg=2.04\n",
            "[555 | 1418.20] loss=1.53 avg=2.04\n",
            "[556 | 1420.66] loss=1.32 avg=2.03\n",
            "[557 | 1423.10] loss=2.07 avg=2.03\n",
            "[558 | 1425.56] loss=1.73 avg=2.03\n",
            "[559 | 1428.00] loss=1.49 avg=2.02\n",
            "[560 | 1430.45] loss=1.77 avg=2.02\n",
            "[561 | 1432.90] loss=1.49 avg=2.01\n",
            "[562 | 1435.36] loss=1.55 avg=2.01\n",
            "[563 | 1437.80] loss=1.51 avg=2.00\n",
            "[564 | 1440.26] loss=1.88 avg=2.00\n",
            "[565 | 1442.71] loss=1.61 avg=2.00\n",
            "[566 | 1445.16] loss=1.56 avg=1.99\n",
            "[567 | 1447.60] loss=1.22 avg=1.99\n",
            "[568 | 1450.05] loss=1.40 avg=1.98\n",
            "[569 | 1452.50] loss=1.35 avg=1.97\n",
            "[570 | 1454.93] loss=1.66 avg=1.97\n",
            "[571 | 1457.38] loss=1.76 avg=1.97\n",
            "[572 | 1459.81] loss=1.49 avg=1.96\n",
            "[573 | 1462.27] loss=1.78 avg=1.96\n",
            "[574 | 1464.71] loss=1.41 avg=1.96\n",
            "[575 | 1467.16] loss=1.71 avg=1.95\n",
            "[576 | 1469.61] loss=1.46 avg=1.95\n",
            "[577 | 1472.06] loss=1.44 avg=1.94\n",
            "[578 | 1474.51] loss=1.83 avg=1.94\n",
            "[579 | 1476.96] loss=1.49 avg=1.94\n",
            "[580 | 1479.41] loss=1.37 avg=1.93\n",
            "[581 | 1481.87] loss=1.73 avg=1.93\n",
            "[582 | 1484.32] loss=1.54 avg=1.93\n",
            "[583 | 1486.77] loss=1.02 avg=1.92\n",
            "[584 | 1489.22] loss=1.73 avg=1.92\n",
            "[585 | 1491.68] loss=1.39 avg=1.91\n",
            "[586 | 1494.13] loss=1.53 avg=1.91\n",
            "[587 | 1496.58] loss=1.11 avg=1.90\n",
            "[588 | 1499.03] loss=1.23 avg=1.89\n",
            "[589 | 1501.50] loss=1.56 avg=1.89\n",
            "[590 | 1503.95] loss=1.71 avg=1.89\n",
            "[591 | 1506.40] loss=1.58 avg=1.88\n",
            "[592 | 1508.86] loss=1.63 avg=1.88\n",
            "[593 | 1511.31] loss=1.24 avg=1.87\n",
            "[594 | 1513.78] loss=1.89 avg=1.87\n",
            "[595 | 1516.23] loss=1.37 avg=1.87\n",
            "[596 | 1518.70] loss=1.22 avg=1.86\n",
            "[597 | 1521.15] loss=1.48 avg=1.86\n",
            "[598 | 1523.60] loss=1.52 avg=1.86\n",
            "[599 | 1526.05] loss=1.42 avg=1.85\n",
            "[600 | 1528.50] loss=1.87 avg=1.85\n",
            "======== SAMPLE 1 ========\n",
            "same, he shall be found.\n",
            "\n",
            "TRANIO:\n",
            "And mayst go talk with grace, patience, truth,\n",
            "remorse, and mercy upon him.\n",
            "\n",
            "BIANCA:\n",
            "What's o' the letter of it, Tranio?\n",
            "\n",
            "TRANIO:\n",
            "Ay, what else? it is a writing on him,\n",
            "but that the letters are letters.\n",
            "\n",
            "BIANCA:\n",
            "Then art not honest.\n",
            "\n",
            "TRANIO:\n",
            "True, madam, but there cannot be but hope.\n",
            "\n",
            "LUCENTIO:\n",
            "But, as I guess, he may but hope; and\n",
            "so may I, but he doth fear the worst.\n",
            "\n",
            "TRANIO:\n",
            "O, there lies hope! faith which I long hath broken.\n",
            "\n",
            "CURTISDALE:\n",
            "\n",
            "GONZALO:\n",
            "There is, indeed, but hope to last long.\n",
            "\n",
            "HORTENSIO:\n",
            "So long as the earth lasts, good sir.\n",
            "\n",
            "TERROLANDUS:\n",
            "But long as the sun doth and signs of change.\n",
            "\n",
            "GONZALO:\n",
            "It doth me, sir, for what I do but waiting.\n",
            "\n",
            "LUCENTIO:\n",
            "But wait, then; 'tis time to put an end to it.\n",
            "\n",
            "GONZALO:\n",
            "A word, sir.\n",
            "\n",
            "TRANIO:\n",
            "A month and a half since we last met.\n",
            "\n",
            "GONZALO:\n",
            "A hundred and twenty years since we last met\n",
            "HORTENSIO:\n",
            "And that's ten, sir! a little stream o' the ocean\n",
            "Whose whiles the waters in that vast expanse\n",
            "Supply o' the sun and moon.\n",
            "\n",
            "TERROLANDUS:\n",
            "That's a pretty name for a gentle caitiff.\n",
            "\n",
            "GONZALO:\n",
            "I'll blush to use it.\n",
            "\n",
            "SEBASTIAN:\n",
            "A pretty name for a bas-reliant.\n",
            "\n",
            "GONZALO:\n",
            "The very name of a dove o' the sea.\n",
            "\n",
            "BAPTISTA:\n",
            "Fetch me that stone, you coward scoundfry worm!\n",
            "This fellow pricks with a tooth a pea, a clover a squash.\n",
            "What would you call a running dove o' the westerly?\n",
            "\n",
            "PETRUCHIO:\n",
            "Ha, ha! one o' the westerly!\n",
            "\n",
            "BAPTISTA:\n",
            "Tetchim Planchette.\n",
            "\n",
            "TRANIO:\n",
            "What is your silly surname?\n",
            "\n",
            "PETRUCHIO:\n",
            "Petruchio.\n",
            "\n",
            "BAPTISTA:\n",
            "Urbina.\n",
            "\n",
            "SEBASTIAN:\n",
            "Petruchio, Petruchio, Petruchio!\n",
            "I have forgot thee.\n",
            "\n",
            "PETRUCHIO:\n",
            "Basta, bastardi! sit, sit, sit, sit, sit, sit;\n",
            "Sit, sit, sit, sit; the chair is yours, sit, sit;\n",
            "Sit, sit, sit, stand thou, sit, sit, sit,\n",
            "Sit thou, sit, for thy brother's sake,\n",
            "Stand thy dauntless face against that o' the\n",
            "lawn; for thine eyes, you see, are but awood,\n",
            "The bark of the common forest, are but bark.\n",
            "\n",
            "BAPTISTA:\n",
            "Now, in good time: the stone\n",
            "That you are accused of having inftain liefs\n",
            "In dealing like a soldier, that thou well mayst deem\n",
            "\n",
            "SEBASTIA:\n",
            "A worthy Marsina, I charge thee,\n",
            "Thy hands are full, thy soluety well,\n",
            "Whilst I strive against thy hardness:\n",
            "For me, thou must crown these hands;\n",
            "Besides, I am not to be counted a\n",
            "compact partner nor a sole hour partner,\n",
            "But a partial half-substantial partike\n",
            "In that joint action which my hands link'd\n",
            "To bring thee to this fault:--for I am no\n",
            "partner, nor a partial substitute of thy\n",
            "hands,--so thou shalt think to crown thyself;\n",
            "For though thy hands link together but one joint,\n",
            "Thy name, which here on earth begins, is a brand,\n",
            "As ritually to all joints, but balladically\n",
            "To that joint. This shall crown thy name:\n",
            "For I, as one part of thy musical number,\n",
            "Proclaim the measure of that brand name, whose name\n",
            "I play here till thou crown me.\n",
            "To crown a name, first, I must sound a name,\n",
            "So as surely as if I did play it.\n",
            "\n",
            "BAPTISTA:\n",
            "A brand or a brand-name?\n",
            "\n",
            "SICINIUS:\n",
            "A\n",
            "\n",
            "[601 | 1541.03] loss=2.01 avg=1.85\n",
            "[602 | 1543.48] loss=1.23 avg=1.85\n",
            "[603 | 1545.93] loss=1.36 avg=1.84\n",
            "[604 | 1548.37] loss=1.79 avg=1.84\n",
            "[605 | 1550.83] loss=1.84 avg=1.84\n",
            "[606 | 1553.28] loss=1.57 avg=1.84\n",
            "[607 | 1555.72] loss=1.36 avg=1.83\n",
            "[608 | 1558.16] loss=1.39 avg=1.83\n",
            "[609 | 1560.61] loss=1.34 avg=1.82\n",
            "[610 | 1563.04] loss=1.16 avg=1.82\n",
            "[611 | 1565.48] loss=0.96 avg=1.81\n",
            "[612 | 1567.92] loss=1.35 avg=1.80\n",
            "[613 | 1570.38] loss=1.46 avg=1.80\n",
            "[614 | 1572.83] loss=1.21 avg=1.80\n",
            "[615 | 1575.28] loss=1.68 avg=1.79\n",
            "[616 | 1577.73] loss=1.32 avg=1.79\n",
            "[617 | 1580.18] loss=1.25 avg=1.78\n",
            "[618 | 1582.63] loss=1.43 avg=1.78\n",
            "[619 | 1585.08] loss=1.77 avg=1.78\n",
            "[620 | 1587.53] loss=1.34 avg=1.78\n",
            "[621 | 1589.98] loss=1.47 avg=1.77\n",
            "[622 | 1592.43] loss=1.44 avg=1.77\n",
            "[623 | 1594.88] loss=1.51 avg=1.77\n",
            "[624 | 1597.34] loss=1.27 avg=1.76\n",
            "[625 | 1599.80] loss=1.48 avg=1.76\n",
            "[626 | 1602.26] loss=1.40 avg=1.76\n",
            "[627 | 1604.71] loss=1.39 avg=1.75\n",
            "[628 | 1607.16] loss=1.23 avg=1.75\n",
            "[629 | 1609.63] loss=1.54 avg=1.74\n",
            "[630 | 1612.08] loss=1.70 avg=1.74\n",
            "[631 | 1614.53] loss=1.17 avg=1.74\n",
            "[632 | 1616.98] loss=0.92 avg=1.73\n",
            "[633 | 1619.43] loss=1.16 avg=1.72\n",
            "[634 | 1621.88] loss=1.59 avg=1.72\n",
            "[635 | 1624.33] loss=1.42 avg=1.72\n",
            "[636 | 1626.79] loss=1.34 avg=1.72\n",
            "[637 | 1629.23] loss=1.24 avg=1.71\n",
            "[638 | 1631.69] loss=1.59 avg=1.71\n",
            "[639 | 1634.14] loss=1.18 avg=1.70\n",
            "[640 | 1636.59] loss=1.13 avg=1.70\n",
            "[641 | 1639.04] loss=1.29 avg=1.69\n",
            "[642 | 1641.49] loss=1.19 avg=1.69\n",
            "[643 | 1643.94] loss=1.77 avg=1.69\n",
            "[644 | 1646.38] loss=1.37 avg=1.69\n",
            "[645 | 1648.83] loss=1.26 avg=1.68\n",
            "[646 | 1651.28] loss=1.05 avg=1.68\n",
            "[647 | 1653.73] loss=1.26 avg=1.67\n",
            "[648 | 1656.18] loss=1.21 avg=1.67\n",
            "[649 | 1658.62] loss=2.11 avg=1.67\n",
            "[650 | 1661.05] loss=1.34 avg=1.67\n",
            "[651 | 1663.52] loss=0.84 avg=1.66\n",
            "[652 | 1665.96] loss=1.62 avg=1.66\n",
            "[653 | 1668.40] loss=1.06 avg=1.65\n",
            "[654 | 1670.85] loss=0.81 avg=1.65\n",
            "[655 | 1673.30] loss=1.05 avg=1.64\n",
            "[656 | 1675.74] loss=0.92 avg=1.63\n",
            "[657 | 1678.20] loss=1.18 avg=1.63\n",
            "[658 | 1680.63] loss=1.23 avg=1.62\n",
            "[659 | 1683.07] loss=1.32 avg=1.62\n",
            "[660 | 1685.53] loss=1.27 avg=1.62\n",
            "[661 | 1687.98] loss=1.00 avg=1.61\n",
            "[662 | 1690.42] loss=1.28 avg=1.61\n",
            "[663 | 1692.87] loss=0.91 avg=1.60\n",
            "[664 | 1695.32] loss=1.05 avg=1.60\n",
            "[665 | 1697.77] loss=1.23 avg=1.59\n",
            "[666 | 1700.22] loss=1.22 avg=1.59\n",
            "[667 | 1702.67] loss=1.47 avg=1.59\n",
            "[668 | 1705.12] loss=0.84 avg=1.58\n",
            "[669 | 1707.57] loss=0.90 avg=1.57\n",
            "[670 | 1710.02] loss=1.20 avg=1.57\n",
            "[671 | 1712.47] loss=0.99 avg=1.56\n",
            "[672 | 1714.92] loss=1.42 avg=1.56\n",
            "[673 | 1717.37] loss=1.37 avg=1.56\n",
            "[674 | 1719.82] loss=1.48 avg=1.56\n",
            "[675 | 1722.27] loss=1.03 avg=1.55\n",
            "[676 | 1724.72] loss=1.02 avg=1.55\n",
            "[677 | 1727.17] loss=1.18 avg=1.54\n",
            "[678 | 1729.62] loss=1.10 avg=1.54\n",
            "[679 | 1732.07] loss=1.71 avg=1.54\n",
            "[680 | 1734.53] loss=1.03 avg=1.54\n",
            "[681 | 1736.98] loss=0.95 avg=1.53\n",
            "[682 | 1739.43] loss=1.05 avg=1.53\n",
            "[683 | 1741.88] loss=1.11 avg=1.52\n",
            "[684 | 1744.33] loss=0.94 avg=1.52\n",
            "[685 | 1746.78] loss=1.23 avg=1.51\n",
            "[686 | 1749.23] loss=0.82 avg=1.51\n",
            "[687 | 1751.68] loss=1.03 avg=1.50\n",
            "[688 | 1754.13] loss=0.79 avg=1.49\n",
            "[689 | 1756.58] loss=0.87 avg=1.49\n",
            "[690 | 1759.03] loss=1.01 avg=1.48\n",
            "[691 | 1761.48] loss=1.30 avg=1.48\n",
            "[692 | 1763.93] loss=1.30 avg=1.48\n",
            "[693 | 1766.38] loss=1.38 avg=1.48\n",
            "[694 | 1768.83] loss=1.18 avg=1.48\n",
            "[695 | 1771.28] loss=1.34 avg=1.47\n",
            "[696 | 1773.73] loss=1.28 avg=1.47\n",
            "[697 | 1776.18] loss=1.14 avg=1.47\n",
            "[698 | 1778.63] loss=1.09 avg=1.47\n",
            "[699 | 1781.08] loss=1.11 avg=1.46\n",
            "[700 | 1783.53] loss=1.29 avg=1.46\n",
            "======== SAMPLE 1 ========\n",
            " distribute\n",
            "In a most just way:\n",
            "I'll take the pinch, and make this all right.\n",
            "\n",
            "ISABELLA:\n",
            "This will do no harm at all.\n",
            "\n",
            "First Lady:\n",
            "Good nurse, an end to our predicament!\n",
            "\n",
            "Second Lady:\n",
            "Now, by my husband's fall,\n",
            "I do believe--\n",
            "\n",
            "ISABELLA:\n",
            "Not by his, by yours, but by 'u--\n",
            "\n",
            "Second Lady:\n",
            "By his fall?\n",
            "\n",
            "ISABELLA:\n",
            "Yes, by him indeed.\n",
            "\n",
            "Second Lady:\n",
            "Then, by my husband's fall,\n",
            "I do believe--\n",
            "\n",
            "ISABELLA:\n",
            "Is this your husband?\n",
            "\n",
            "CLARENCE:\n",
            "That Margaret your most obedient servant is,\n",
            "Officer-at-arms.\n",
            "\n",
            "First Lady:\n",
            "The mayor, I beseech you, to avoid such talk.\n",
            "\n",
            "CLARENCE:\n",
            "Why, my lord, it cannot be avoided, sir:\n",
            "I have talk'd for hours with Lord Hastings, and dare here\n",
            "Be open; and he seems resolved to win me.\n",
            "\n",
            "First Lady:\n",
            "Here comes Claudio.\n",
            "\n",
            "CLARENCE:\n",
            "\n",
            "PERU:\n",
            "\n",
            "LADY GREY:\n",
            "'Would I had been here first, or had my heart\n",
            "Still been here first:' this desert makes no sense;\n",
            "The measure being up, the very thing must be done.\n",
            "\n",
            "CLARENCE:\n",
            "I will put mine service to 't.\n",
            "\n",
            "LADY GREY:\n",
            "When are you going to speak with the mayor?\n",
            "\n",
            "CLARENCE:\n",
            "When his speech would serve, not bestain his coat;\n",
            "Commend me to his noble liege.\n",
            "\n",
            "LADY GREY:\n",
            "I will commend myself\n",
            "That I know in what weakness I have:\n",
            "I'll lend my services, sir, if needful.\n",
            "\n",
            "CLARENCE:\n",
            "That I remember me as I was, and not as I am:\n",
            "I do, and remember you well.\n",
            "\n",
            "LADY GREY:\n",
            "Why, then I do remember my husband as best\n",
            "Of what I changed. I do remember him well.\n",
            "Your grace should welcome me to his court.\n",
            "\n",
            "CLARENCE:\n",
            "But what! will I stay till the Mayor come?\n",
            "\n",
            "LADY GREY:\n",
            "I do not doubt but your grace will be busy,\n",
            "And frown as you will on importunity.\n",
            "\n",
            "\n",
            "CLARENCE:\n",
            "What, are you mad? What, mad man!\n",
            "\n",
            "LADY GREY:\n",
            "Are you mad? Mad man is mad man.\n",
            "\n",
            "First Lady:\n",
            "Mad man! mad man! mad man! mad man! And mad!\n",
            "A thousand voices! What! no weeping! no begging!\n",
            "There is not a sound but hoarse wailing.\n",
            "Mad man! mad man! mad man!\n",
            "\n",
            "LADY GREY:\n",
            "Mad man! mad man! mad man! mad man!\n",
            "\n",
            "\n",
            "LADY GREY:\n",
            "Mad man! mad man! mad man!\n",
            "Myself and my poor husband! Madam Isabella,\n",
            "You are too malignantly malachiorated!\n",
            "Mortality is not a little dead; is it not?\n",
            "I am not so ill; I am three inches lighter.\n",
            "Mortality is not a little pale.\n",
            "\n",
            "First Lady:\n",
            "Madam, these windows on your grace's mind are all\n",
            "impairments.\n",
            "\n",
            "LEONTES:\n",
            "This is malady, it is malignently pale,\n",
            "It is but grown pale since you did not do me ill.\n",
            "Madam, thou art worthily handled.\n",
            "\n",
            "LADY GREY:\n",
            "I could dance, and tear, and speak, and feel, and die a thousand:\n",
            "Thou art as cheap as wax in thy speech,\n",
            "As precious as the kingdom of Naples.\n",
            "That thou look'st gold forward, bauble past repair:\n",
            "This is not thine, nor none, nor none:\n",
            "Rich is the poor that does not have it.\n",
            "\n",
            "LORDITIO:\n",
            "This lady is most beloved of.\n",
            "\n",
            "LADY GREY:\n",
            "Madam, I have made thee some pretty mask;\n",
            "Which, indeed, do suspect some dark froward,\n",
            "Or foul spirit prevails.\n",
            "\n",
            "First Lady:\n",
            "Madam, this lady is most beloved of.\n",
            "\n",
            "LEONTES:\n",
            "This is very vile and ill urged at,\n",
            "Even against her beauty, and not borne\n",
            "By any merit other than that\n",
            "That beauty which beauty should envy:\n",
            "This is a cunning disguise, that dares but touch\n",
            "The very essence of this evil.\n",
            "\n",
            "LADY GREY:\n",
            "I have seen it done to soothe this grief;\n",
            "And I believe\n",
            "\n",
            "[701 | 1796.08] loss=1.18 avg=1.46\n",
            "[702 | 1798.55] loss=1.30 avg=1.46\n",
            "[703 | 1801.00] loss=1.30 avg=1.45\n",
            "[704 | 1803.46] loss=1.06 avg=1.45\n",
            "[705 | 1805.91] loss=1.17 avg=1.45\n",
            "[706 | 1808.36] loss=0.76 avg=1.44\n",
            "[707 | 1810.81] loss=1.04 avg=1.44\n",
            "[708 | 1813.28] loss=0.84 avg=1.43\n",
            "[709 | 1815.73] loss=1.26 avg=1.43\n",
            "[710 | 1818.20] loss=0.98 avg=1.42\n",
            "[711 | 1820.65] loss=1.00 avg=1.42\n",
            "[712 | 1823.11] loss=1.00 avg=1.42\n",
            "[713 | 1825.56] loss=1.01 avg=1.41\n",
            "[714 | 1828.01] loss=1.09 avg=1.41\n",
            "[715 | 1830.47] loss=1.61 avg=1.41\n",
            "[716 | 1832.94] loss=0.89 avg=1.41\n",
            "[717 | 1835.39] loss=0.93 avg=1.40\n",
            "[718 | 1837.84] loss=0.75 avg=1.39\n",
            "[719 | 1840.28] loss=1.32 avg=1.39\n",
            "[720 | 1842.73] loss=1.06 avg=1.39\n",
            "[721 | 1845.18] loss=0.91 avg=1.39\n",
            "[722 | 1847.64] loss=1.20 avg=1.38\n",
            "[723 | 1850.08] loss=1.14 avg=1.38\n",
            "[724 | 1852.53] loss=0.59 avg=1.37\n",
            "[725 | 1854.98] loss=0.84 avg=1.37\n",
            "[726 | 1857.44] loss=1.54 avg=1.37\n",
            "[727 | 1859.88] loss=1.59 avg=1.37\n",
            "[728 | 1862.32] loss=0.68 avg=1.36\n",
            "[729 | 1864.76] loss=0.74 avg=1.36\n",
            "[730 | 1867.20] loss=1.06 avg=1.36\n",
            "[731 | 1869.64] loss=0.83 avg=1.35\n",
            "[732 | 1872.09] loss=1.04 avg=1.35\n",
            "[733 | 1874.54] loss=1.34 avg=1.35\n",
            "[734 | 1876.99] loss=0.83 avg=1.34\n",
            "[735 | 1879.44] loss=1.21 avg=1.34\n",
            "[736 | 1881.89] loss=1.33 avg=1.34\n",
            "[737 | 1884.35] loss=1.03 avg=1.34\n",
            "[738 | 1886.80] loss=1.12 avg=1.34\n",
            "[739 | 1889.25] loss=1.03 avg=1.33\n",
            "[740 | 1891.70] loss=0.68 avg=1.33\n",
            "[741 | 1894.15] loss=1.00 avg=1.32\n",
            "[742 | 1896.61] loss=0.97 avg=1.32\n",
            "[743 | 1899.05] loss=1.22 avg=1.32\n",
            "[744 | 1901.51] loss=1.24 avg=1.32\n",
            "[745 | 1903.98] loss=0.92 avg=1.31\n",
            "[746 | 1906.43] loss=1.12 avg=1.31\n",
            "[747 | 1908.88] loss=1.65 avg=1.31\n",
            "[748 | 1911.33] loss=0.80 avg=1.31\n",
            "[749 | 1913.78] loss=0.98 avg=1.31\n",
            "[750 | 1916.23] loss=0.85 avg=1.30\n",
            "[751 | 1918.68] loss=0.83 avg=1.30\n",
            "[752 | 1921.13] loss=0.70 avg=1.29\n",
            "[753 | 1923.60] loss=1.16 avg=1.29\n",
            "[754 | 1926.05] loss=0.99 avg=1.29\n",
            "[755 | 1928.50] loss=1.32 avg=1.29\n",
            "[756 | 1930.95] loss=0.85 avg=1.28\n",
            "[757 | 1933.40] loss=1.22 avg=1.28\n",
            "[758 | 1935.85] loss=1.25 avg=1.28\n",
            "[759 | 1938.30] loss=0.94 avg=1.28\n",
            "[760 | 1940.76] loss=0.73 avg=1.27\n",
            "[761 | 1943.20] loss=0.91 avg=1.27\n",
            "[762 | 1945.65] loss=0.74 avg=1.26\n",
            "[763 | 1948.10] loss=0.93 avg=1.26\n",
            "[764 | 1950.55] loss=0.98 avg=1.26\n",
            "[765 | 1953.01] loss=0.88 avg=1.25\n",
            "[766 | 1955.46] loss=1.38 avg=1.26\n",
            "[767 | 1957.91] loss=0.76 avg=1.25\n",
            "[768 | 1960.36] loss=1.05 avg=1.25\n",
            "[769 | 1962.82] loss=1.05 avg=1.25\n",
            "[770 | 1965.27] loss=0.89 avg=1.24\n",
            "[771 | 1967.72] loss=0.69 avg=1.24\n",
            "[772 | 1970.17] loss=0.80 avg=1.23\n",
            "[773 | 1972.61] loss=0.94 avg=1.23\n",
            "[774 | 1975.06] loss=0.71 avg=1.22\n",
            "[775 | 1977.51] loss=0.79 avg=1.22\n",
            "[776 | 1979.97] loss=0.89 avg=1.22\n",
            "[777 | 1982.41] loss=1.02 avg=1.22\n",
            "[778 | 1984.86] loss=0.89 avg=1.21\n",
            "[779 | 1987.30] loss=0.85 avg=1.21\n",
            "[780 | 1989.76] loss=0.85 avg=1.20\n",
            "[781 | 1992.21] loss=0.76 avg=1.20\n",
            "[782 | 1994.66] loss=0.95 avg=1.20\n",
            "[783 | 1997.11] loss=0.83 avg=1.19\n",
            "[784 | 1999.55] loss=0.96 avg=1.19\n",
            "[785 | 2002.01] loss=0.82 avg=1.19\n",
            "[786 | 2004.46] loss=0.83 avg=1.18\n",
            "[787 | 2006.91] loss=0.68 avg=1.18\n",
            "[788 | 2009.36] loss=0.83 avg=1.18\n",
            "[789 | 2011.82] loss=0.85 avg=1.17\n",
            "[790 | 2014.26] loss=0.67 avg=1.17\n",
            "[791 | 2016.72] loss=1.09 avg=1.17\n",
            "[792 | 2019.17] loss=1.09 avg=1.17\n",
            "[793 | 2021.62] loss=0.85 avg=1.16\n",
            "[794 | 2024.07] loss=0.72 avg=1.16\n",
            "[795 | 2026.52] loss=0.76 avg=1.15\n",
            "[796 | 2028.97] loss=1.09 avg=1.15\n",
            "[797 | 2031.43] loss=0.58 avg=1.15\n",
            "[798 | 2033.87] loss=0.83 avg=1.14\n",
            "[799 | 2036.33] loss=1.15 avg=1.14\n",
            "[800 | 2038.77] loss=0.91 avg=1.14\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "\n",
            "ANGELO:\n",
            "We were not made to the court.\n",
            "\n",
            "Servant:\n",
            "We were, sir.\n",
            "\n",
            "ANGELO:\n",
            "'Tis a silly account.\n",
            "\n",
            "Servant:\n",
            "You shall not be made to the court.\n",
            "\n",
            "ANGELO:\n",
            "I am made to the court to the court, sir.\n",
            "\n",
            "Servant:\n",
            "An if we accept it, we are then compell'd to the feast of death.\n",
            "\n",
            "ANGELO:\n",
            "What then?\n",
            "\n",
            "Servant:\n",
            "You will know by 's\n",
            "when you look for yourself.\n",
            "\n",
            "ANGELO:\n",
            "How?\n",
            "\n",
            "Servant:\n",
            "It is after supper.\n",
            "\n",
            "ANGELO:\n",
            "Where?\n",
            "\n",
            "Servant:\n",
            "At the cell.\n",
            "\n",
            "ANGELO:\n",
            "That's straight amiss, my lord. We were not made to the feast.\n",
            "\n",
            "Servant:\n",
            "Ay, sir.\n",
            "\n",
            "ANGELO:\n",
            "So 'tis a silly tale: but the death of our priest\n",
            " made it fair, and so I accept his estimation.\n",
            "\n",
            "Servant:\n",
            "Why, how now, sir! is he gone to fetch more spices?\n",
            "\n",
            "ANGELO:\n",
            "Go he not farther than 'tis winter's day.\n",
            "\n",
            "Servant:\n",
            "Ay, sir.\n",
            "\n",
            "ANGELO:\n",
            "Nay, then all this with some caution.\n",
            "\n",
            "ANGELO:\n",
            "What means Claudio to your worship?\n",
            "\n",
            "ISABELLA:\n",
            "Lord sir, Claudio is here with a servant.\n",
            "\n",
            "ISABELLA:\n",
            "Is that he removed to buy more spices for your feast?\n",
            "\n",
            "CLIO:\n",
            "Sir, it is; but it draws us in cold.\n",
            "\n",
            "ISABELLA:\n",
            "Yes, in a very grave way.\n",
            "\n",
            "CLIO:\n",
            "Yes, by grave: and it is but to our feast-day.\n",
            "\n",
            "ISABELLA:\n",
            "This is not to be, but to sleep.\n",
            "\n",
            "CLIO:\n",
            "And when I wake, my gown is off and Lucentio there;\n",
            "I go lustily to her bed.\n",
            "\n",
            "ISABELLA:\n",
            "Thou indeed are a grave: and, for some meat, I\n",
            "may entreat some rest.\n",
            "\n",
            "ANGELO:\n",
            "Sir, I take my leave of you; but I must\n",
            "extend them to Naples.\n",
            "\n",
            "CLIO:\n",
            "The limit of your majesty's love, to mine own;\n",
            "That often hath she been outdone with pleasure,\n",
            "or often forthwith she hath been so much satisfied,\n",
            "that she hath left me no better way\n",
            "without untangling the blots that I must with proof\n",
            "make manifest, and that I am indeed a Montague.\n",
            "\n",
            "ISABELLA:\n",
            "I do encounter you in this work; and am come\n",
            "to seek out you.\n",
            "\n",
            "ANGELO:\n",
            "What, shall I do this?\n",
            "\n",
            "ISABELLA:\n",
            "O, say yonder man; 'tis no less; 'twill do't.\n",
            "\n",
            "LUCENTIO:\n",
            "Is not a more degenerate fellow, worse than you,\n",
            "when first he did embrace his wife.\n",
            "\n",
            "ANGELO:\n",
            "Say, then, good friend;\n",
            "Why stands't of me to-day?\n",
            "\n",
            "ISABELLA:\n",
            "Why, sir, I beseech you on your part not to.\n",
            "\n",
            "ANGELO:\n",
            "Take leave, all of you; I'll beseech you again.\n",
            "\n",
            "ISABELLA:\n",
            "Obey, no more.\n",
            "\n",
            "ANGELO:\n",
            "Say, then, good friend;\n",
            "The love of love is but one ingredient\n",
            "Of wine; and to be winnow'd would\n",
            "To more than one wine would require so;\n",
            "And, as I covenants, to many, wine\n",
            "Will do that bottle must needs contain to.\n",
            "I prithee, sir! tell me, how is it with Agnes?\n",
            "\n",
            "LUCENTIO:\n",
            "With more haste and care than ever I have;\n",
            "For then being much wrought abroad,\n",
            "I had time enough to think over a while.\n",
            "\n",
            "ISABELLA:\n",
            "I pray you, comfort!\n",
            "Obey the cook! the steward the breeder!\n",
            "\n",
            "ANGELO:\n",
            "A tecum privilegio il poxentilia.\n",
            "\n",
            "ISABELLA:\n",
            "Nay, I would it were as easy as it were easy.\n",
            "\n",
            "ANGELO:\n",
            "Nay, then you are a little ungity:\n",
            "Let me have as many of your slates as I can.\n",
            "\n",
            "LUCENTIO:\n",
            "And much of mine as I can lay down.\n",
            "\n",
            "ISABELLA:\n",
            "That's no comfort at all.\n",
            "\n",
            "ANGELO:\n",
            "'\n",
            "\n",
            "[801 | 2051.26] loss=0.61 avg=1.14\n",
            "[802 | 2053.72] loss=0.85 avg=1.13\n",
            "[803 | 2056.16] loss=0.63 avg=1.13\n",
            "[804 | 2058.61] loss=1.26 avg=1.13\n",
            "[805 | 2061.06] loss=0.91 avg=1.13\n",
            "[806 | 2063.51] loss=0.64 avg=1.12\n",
            "[807 | 2065.97] loss=0.72 avg=1.12\n",
            "[808 | 2068.42] loss=0.70 avg=1.12\n",
            "[809 | 2070.87] loss=0.82 avg=1.11\n",
            "[810 | 2073.32] loss=0.68 avg=1.11\n",
            "[811 | 2075.77] loss=0.77 avg=1.10\n",
            "[812 | 2078.22] loss=0.86 avg=1.10\n",
            "[813 | 2080.67] loss=0.66 avg=1.10\n",
            "[814 | 2083.13] loss=1.10 avg=1.10\n",
            "[815 | 2085.57] loss=0.98 avg=1.10\n",
            "[816 | 2088.03] loss=0.69 avg=1.09\n",
            "[817 | 2090.47] loss=0.55 avg=1.09\n",
            "[818 | 2092.93] loss=0.71 avg=1.08\n",
            "[819 | 2095.38] loss=0.71 avg=1.08\n",
            "[820 | 2097.83] loss=0.61 avg=1.07\n",
            "[821 | 2100.28] loss=0.75 avg=1.07\n",
            "[822 | 2102.73] loss=0.67 avg=1.07\n",
            "[823 | 2105.18] loss=0.79 avg=1.06\n",
            "[824 | 2107.64] loss=0.67 avg=1.06\n",
            "[825 | 2110.08] loss=0.77 avg=1.06\n",
            "[826 | 2112.54] loss=0.70 avg=1.05\n",
            "[827 | 2114.99] loss=0.80 avg=1.05\n",
            "[828 | 2117.44] loss=0.78 avg=1.05\n",
            "[829 | 2119.89] loss=0.55 avg=1.04\n",
            "[830 | 2122.34] loss=0.56 avg=1.04\n",
            "[831 | 2124.78] loss=0.91 avg=1.04\n",
            "[832 | 2127.23] loss=0.86 avg=1.04\n",
            "[833 | 2129.69] loss=0.72 avg=1.03\n",
            "[834 | 2132.14] loss=0.60 avg=1.03\n",
            "[835 | 2134.59] loss=0.56 avg=1.02\n",
            "[836 | 2137.03] loss=0.86 avg=1.02\n",
            "[837 | 2139.49] loss=0.72 avg=1.02\n",
            "[838 | 2141.93] loss=0.56 avg=1.01\n",
            "[839 | 2144.39] loss=0.81 avg=1.01\n",
            "[840 | 2146.84] loss=0.82 avg=1.01\n",
            "[841 | 2149.29] loss=0.52 avg=1.01\n",
            "[842 | 2151.74] loss=1.07 avg=1.01\n",
            "[843 | 2154.19] loss=0.61 avg=1.00\n",
            "[844 | 2156.64] loss=0.65 avg=1.00\n",
            "[845 | 2159.12] loss=0.48 avg=0.99\n",
            "[846 | 2161.56] loss=0.39 avg=0.99\n",
            "[847 | 2164.02] loss=0.57 avg=0.98\n",
            "[848 | 2166.47] loss=0.74 avg=0.98\n",
            "[849 | 2168.92] loss=0.61 avg=0.98\n",
            "[850 | 2171.37] loss=0.72 avg=0.97\n",
            "[851 | 2173.82] loss=0.53 avg=0.97\n",
            "[852 | 2176.27] loss=0.65 avg=0.97\n",
            "[853 | 2178.72] loss=0.86 avg=0.97\n",
            "[854 | 2181.18] loss=0.66 avg=0.96\n",
            "[855 | 2183.63] loss=0.56 avg=0.96\n",
            "[856 | 2186.08] loss=0.45 avg=0.95\n",
            "[857 | 2188.53] loss=0.42 avg=0.95\n",
            "[858 | 2190.98] loss=0.83 avg=0.95\n",
            "[859 | 2193.43] loss=0.49 avg=0.94\n",
            "[860 | 2195.89] loss=0.58 avg=0.94\n",
            "[861 | 2198.36] loss=0.85 avg=0.94\n",
            "[862 | 2200.80] loss=0.89 avg=0.94\n",
            "[863 | 2203.25] loss=0.37 avg=0.93\n",
            "[864 | 2205.71] loss=0.60 avg=0.93\n",
            "[865 | 2208.16] loss=0.53 avg=0.92\n",
            "[866 | 2210.61] loss=0.58 avg=0.92\n",
            "[867 | 2213.06] loss=0.53 avg=0.92\n",
            "[868 | 2215.51] loss=0.55 avg=0.91\n",
            "[869 | 2217.96] loss=0.62 avg=0.91\n",
            "[870 | 2220.41] loss=0.44 avg=0.91\n",
            "[871 | 2222.86] loss=0.67 avg=0.90\n",
            "[872 | 2225.33] loss=0.78 avg=0.90\n",
            "[873 | 2227.79] loss=0.67 avg=0.90\n",
            "[874 | 2230.24] loss=0.68 avg=0.90\n",
            "[875 | 2232.69] loss=0.56 avg=0.89\n",
            "[876 | 2235.14] loss=0.52 avg=0.89\n",
            "[877 | 2237.60] loss=0.64 avg=0.89\n",
            "[878 | 2240.05] loss=0.50 avg=0.88\n",
            "[879 | 2242.50] loss=0.56 avg=0.88\n",
            "[880 | 2244.95] loss=0.61 avg=0.88\n",
            "[881 | 2247.40] loss=0.51 avg=0.87\n",
            "[882 | 2249.85] loss=0.73 avg=0.87\n",
            "[883 | 2252.30] loss=0.56 avg=0.87\n",
            "[884 | 2254.75] loss=0.51 avg=0.87\n",
            "[885 | 2257.20] loss=0.43 avg=0.86\n",
            "[886 | 2259.65] loss=0.81 avg=0.86\n",
            "[887 | 2262.12] loss=0.59 avg=0.86\n",
            "[888 | 2264.57] loss=0.94 avg=0.86\n",
            "[889 | 2267.02] loss=0.45 avg=0.86\n",
            "[890 | 2269.47] loss=0.60 avg=0.85\n",
            "[891 | 2271.92] loss=0.50 avg=0.85\n",
            "[892 | 2274.37] loss=0.63 avg=0.85\n",
            "[893 | 2276.82] loss=0.73 avg=0.85\n",
            "[894 | 2279.28] loss=0.54 avg=0.84\n",
            "[895 | 2281.73] loss=0.48 avg=0.84\n",
            "[896 | 2284.18] loss=0.51 avg=0.84\n",
            "[897 | 2286.63] loss=0.62 avg=0.83\n",
            "[898 | 2289.08] loss=0.77 avg=0.83\n",
            "[899 | 2291.53] loss=0.60 avg=0.83\n",
            "[900 | 2293.98] loss=0.53 avg=0.83\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "\n",
            "Page: I fear and obey her;\n",
            "How they love me and all their children,\n",
            "I cannot tell. But pray tell me two things:\n",
            "The one, my three little ones have a shriek;\n",
            "And that's the man, she Romeo, that mutinies\n",
            "Upon his hide and starts after him.\n",
            "\n",
            "TRANIO:\n",
            "Fear him not; to be sure, sir, he is a villain\n",
            "That needs no dam. And, with an evident desire\n",
            "To have me slain, to provoke my death with\n",
            "A desire that would make a swain's trois,\n",
            "I murder him not.\n",
            "\n",
            "PROSPERO:\n",
            "Fear him not; to be sure, sir, he is a villain\n",
            "That needs no dam. And, with an apparent desire\n",
            "To have me dead, to provoke my death with\n",
            "A desire that would make a swain't trois,\n",
            "I murder him not.\n",
            "\n",
            "BAPTISTA:\n",
            "Fear you not him not?\n",
            "\n",
            "TRANIO:\n",
            "Ay, sir; and that's a desperate fellow indeed.\n",
            "\n",
            "POMPEY:\n",
            "Sir, possess my head, and my valour, if\n",
            "I defy the odds and combine the affections\n",
            "Of Biondello and Mariana!\n",
            "\n",
            "TRANIO:\n",
            "That's a desperate thing madly conceived; but it seems\n",
            "I may as well make it happily go\n",
            "A mile on, and then meet a house of such lights\n",
            "As Mariana and I and a thousand roaring ones\n",
            "Will shake to bring you even the shadow of night.\n",
            "That's a pretty supper, sir. Biondello\n",
            "You'll like it: make it yourself, and have two\n",
            "fancies to each person you invite.\n",
            "\n",
            "BIONDELLO:\n",
            "Sir, my fancy is yours: come forth and\n",
            "kithip it. I'll deck you some one, and--\n",
            "\n",
            "LUCIO:\n",
            "Pray you, buy a bigger one: they'll be much too full for\n",
            "the first.\n",
            "\n",
            "POMPEY:\n",
            "I know I cannot better it, but it seems I may\n",
            "but shall have a piece equally full. But, let them\n",
            "come and sit by the fire; for 'tis a feast to\n",
            "me and they are near.\n",
            "\n",
            "TRANIO:\n",
            "You shall be the very warrant of my welcome. Come,\n",
            "forthwith gone: go, buy these things: a handsome\n",
            "hence, a merriment.\n",
            "\n",
            "LUCIO:\n",
            "A lusty Valentine's gift, indeed, that I must\n",
            "parth it to, and try if I am moved by it\n",
            "as strongly I desire to dance.\n",
            "\n",
            "POMPEY:\n",
            "To move indeed: to keep at the best a\n",
            "condition of my love.\n",
            "\n",
            "LUCIO:\n",
            "Will you vouchsafe to come? it is a bond\n",
            "I must prove to a certain Paris at any\n",
            "howe, as I cannotconyst my own.\n",
            "\n",
            "POMPEY:\n",
            "To keep at all a kind of thresher.\n",
            "\n",
            "LUCIO:\n",
            "A life: why, Paris is a very\n",
            "life a' dance.\n",
            "\n",
            "POMPEY:\n",
            "I dance no deeper; I swear it is an art\n",
            "which for best ornaments me quite.\n",
            "\n",
            "LUCIO:\n",
            "Ay, more than a life; and more than a lifed:\n",
            "which for a fine example of best friends\n",
            "I must prove a full friend to twenty.\n",
            "\n",
            "POMPEY:\n",
            "I dance no further: I swear to God\n",
            "I' not danced as I shouldbund it.\n",
            "\n",
            "LUCIO:\n",
            "A full string: and a stripper for a\n",
            "cram!\n",
            "\n",
            "LUCIO:\n",
            "A bench for a cudgel! Jove. It must be\n",
            "so: I'll baste a cudgel at a play, and the play's\n",
            "grave frowardly.\n",
            "\n",
            "POMPEY:\n",
            "To face the daintied light of day with a\n",
            "brick of incescation is folly.\n",
            "\n",
            "LUCIO:\n",
            "Master, here's a daintied jest that willrow me to\n",
            "barbarish it.\n",
            "\n",
            "POMPEY:\n",
            "A greater jest than a pleasant one.\n",
            "\n",
            "LUCIO:\n",
            "Say, like a marble have grace, or a precious thing\n",
            "congealed.\n",
            "\n",
            "POMPEY:\n",
            "More than a treasure.\n",
            "\n",
            "LUCIO:\n",
            "The grand jest: this is the grand medall;\n",
            "the grandest jest ever shamed in the art of\n",
            "barbarism.\n",
            "\n",
            "POMPEY:\n",
            "This can never be more true.\n",
            "\n",
            "LUCIO:\n",
            "Why, what with this grand merit on its face?\n",
            "is it one of the\n",
            "\n",
            "[901 | 2306.51] loss=0.41 avg=0.82\n",
            "[902 | 2308.96] loss=0.73 avg=0.82\n",
            "[903 | 2311.41] loss=0.48 avg=0.82\n",
            "[904 | 2313.86] loss=0.50 avg=0.82\n",
            "[905 | 2316.31] loss=0.63 avg=0.81\n",
            "[906 | 2318.76] loss=0.79 avg=0.81\n",
            "[907 | 2321.21] loss=0.61 avg=0.81\n",
            "[908 | 2323.67] loss=0.50 avg=0.81\n",
            "[909 | 2326.12] loss=0.56 avg=0.81\n",
            "[910 | 2328.55] loss=0.99 avg=0.81\n",
            "[911 | 2331.00] loss=0.57 avg=0.81\n",
            "[912 | 2333.43] loss=0.57 avg=0.80\n",
            "[913 | 2335.88] loss=0.41 avg=0.80\n",
            "[914 | 2338.33] loss=0.59 avg=0.80\n",
            "[915 | 2340.78] loss=0.40 avg=0.79\n",
            "[916 | 2343.23] loss=0.78 avg=0.79\n",
            "[917 | 2345.68] loss=0.57 avg=0.79\n",
            "[918 | 2348.13] loss=0.46 avg=0.79\n",
            "[919 | 2350.58] loss=0.50 avg=0.79\n",
            "[920 | 2353.03] loss=0.48 avg=0.78\n",
            "[921 | 2355.47] loss=0.41 avg=0.78\n",
            "[922 | 2357.92] loss=0.50 avg=0.78\n",
            "[923 | 2360.38] loss=0.49 avg=0.77\n",
            "[924 | 2362.83] loss=0.40 avg=0.77\n",
            "[925 | 2365.29] loss=0.61 avg=0.77\n",
            "[926 | 2367.74] loss=0.54 avg=0.77\n",
            "[927 | 2370.19] loss=0.62 avg=0.76\n",
            "[928 | 2372.66] loss=0.55 avg=0.76\n",
            "[929 | 2375.11] loss=0.49 avg=0.76\n",
            "[930 | 2377.58] loss=0.64 avg=0.76\n",
            "[931 | 2380.05] loss=0.48 avg=0.75\n",
            "[932 | 2382.50] loss=0.49 avg=0.75\n",
            "[933 | 2384.95] loss=0.33 avg=0.75\n",
            "[934 | 2387.40] loss=0.63 avg=0.75\n",
            "[935 | 2389.86] loss=0.31 avg=0.74\n",
            "[936 | 2392.32] loss=0.38 avg=0.74\n",
            "[937 | 2394.79] loss=0.25 avg=0.73\n",
            "[938 | 2397.26] loss=0.40 avg=0.73\n",
            "[939 | 2399.73] loss=0.38 avg=0.73\n",
            "[940 | 2402.18] loss=0.55 avg=0.73\n",
            "[941 | 2404.65] loss=0.33 avg=0.72\n",
            "[942 | 2407.10] loss=0.61 avg=0.72\n",
            "[943 | 2409.55] loss=0.44 avg=0.72\n",
            "[944 | 2412.00] loss=0.49 avg=0.72\n",
            "[945 | 2414.46] loss=0.65 avg=0.71\n",
            "[946 | 2416.91] loss=0.41 avg=0.71\n",
            "[947 | 2419.36] loss=0.51 avg=0.71\n",
            "[948 | 2421.81] loss=0.91 avg=0.71\n",
            "[949 | 2424.26] loss=0.64 avg=0.71\n",
            "[950 | 2426.72] loss=0.44 avg=0.71\n",
            "[951 | 2429.16] loss=0.67 avg=0.71\n",
            "[952 | 2431.62] loss=0.43 avg=0.70\n",
            "[953 | 2434.07] loss=0.33 avg=0.70\n",
            "[954 | 2436.52] loss=0.42 avg=0.70\n",
            "[955 | 2438.97] loss=0.68 avg=0.70\n",
            "[956 | 2441.42] loss=0.43 avg=0.70\n",
            "[957 | 2443.87] loss=0.43 avg=0.69\n",
            "[958 | 2446.34] loss=0.31 avg=0.69\n",
            "[959 | 2448.79] loss=0.55 avg=0.69\n",
            "[960 | 2451.25] loss=0.44 avg=0.69\n",
            "[961 | 2453.71] loss=0.42 avg=0.68\n",
            "[962 | 2456.19] loss=0.59 avg=0.68\n",
            "[963 | 2458.64] loss=0.31 avg=0.68\n",
            "[964 | 2461.08] loss=0.40 avg=0.67\n",
            "[965 | 2463.54] loss=0.39 avg=0.67\n",
            "[966 | 2465.99] loss=0.44 avg=0.67\n",
            "[967 | 2468.44] loss=0.36 avg=0.67\n",
            "[968 | 2470.89] loss=0.39 avg=0.66\n",
            "[969 | 2473.34] loss=0.61 avg=0.66\n",
            "[970 | 2475.79] loss=0.47 avg=0.66\n",
            "[971 | 2478.24] loss=0.40 avg=0.66\n",
            "[972 | 2480.71] loss=0.55 avg=0.66\n",
            "[973 | 2483.16] loss=0.42 avg=0.66\n",
            "[974 | 2485.61] loss=0.56 avg=0.65\n",
            "[975 | 2488.06] loss=0.27 avg=0.65\n",
            "[976 | 2490.51] loss=0.79 avg=0.65\n",
            "[977 | 2492.96] loss=0.26 avg=0.65\n",
            "[978 | 2495.42] loss=0.54 avg=0.65\n",
            "[979 | 2497.87] loss=0.45 avg=0.64\n",
            "[980 | 2500.32] loss=0.55 avg=0.64\n",
            "[981 | 2502.78] loss=0.30 avg=0.64\n",
            "[982 | 2505.23] loss=0.41 avg=0.64\n",
            "[983 | 2507.68] loss=0.40 avg=0.64\n",
            "[984 | 2510.13] loss=0.52 avg=0.63\n",
            "[985 | 2512.58] loss=0.31 avg=0.63\n",
            "[986 | 2515.03] loss=0.32 avg=0.63\n",
            "[987 | 2517.48] loss=0.35 avg=0.63\n",
            "[988 | 2519.94] loss=0.29 avg=0.62\n",
            "[989 | 2522.39] loss=0.56 avg=0.62\n",
            "[990 | 2524.86] loss=0.34 avg=0.62\n",
            "[991 | 2527.30] loss=0.39 avg=0.62\n",
            "[992 | 2529.75] loss=0.30 avg=0.61\n",
            "[993 | 2532.20] loss=0.36 avg=0.61\n",
            "[994 | 2534.65] loss=0.58 avg=0.61\n",
            "[995 | 2537.11] loss=0.48 avg=0.61\n",
            "[996 | 2539.56] loss=0.41 avg=0.61\n",
            "[997 | 2542.01] loss=0.36 avg=0.60\n",
            "[998 | 2544.46] loss=0.72 avg=0.61\n",
            "[999 | 2546.91] loss=0.29 avg=0.60\n",
            "[1000 | 2549.36] loss=0.33 avg=0.60\n",
            "Saving /content/drive/My Drive/Colab Notebooks/gpt2_learning/checkpoint/run1/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzGscxkA42pt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1eb9cb9-93cc-4a72-98e6-782180cbc314"
      },
      "source": [
        "gpt2.generate(sess, checkpoint_dir=os.path.join(base_dir,\"checkpoint\"))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Romeo, they say,\n",
            "Is not much altered by yonder sun.\n",
            "\n",
            "ROMEO:\n",
            "O, they say, I know it:\n",
            "Beseech you, sir, be gone.\n",
            "Take him to your chapel: I will attend him.\n",
            "Come, Juliet, take me to your heart. Sweet, just, and true,\n",
            "I do miss thee very, very, very, very.\n",
            "I do, and will gladly take thee with me.\n",
            "Come, sir, away with thee: I will attend thee.\n",
            "\n",
            "JULIET:\n",
            "No, good friar, not by my troth: come, sir; away.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Ah, sirrah, so shall I no more but sit\n",
            "With you; that my loving bride may with all\n",
            "Present their special thanks in pure marriage\n",
            "As they did take you an hour ago.\n",
            "\n",
            "ROMEO:\n",
            "Bless me heartily on her whose nuptial kiss\n",
            "Became so rank with hers!\n",
            "\n",
            "JULIET:\n",
            "I pray thee, peace: I will, Romeo.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Beseech you, sir, on my side; I will not marry.\n",
            "\n",
            "ROMEO:\n",
            "Then let me and my partner in crime,\n",
            "An hour ago, to make room for our tent,\n",
            "Shall do so with much success. But, say,\n",
            "We will break the knot to-morrow morning.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Nay, I will not, sir: as it always is,\n",
            "Sometimes Thursday becomes Sunday.\n",
            "\n",
            "ROMEO:\n",
            "O, she which thou wilt betrayarest must come to\n",
            "Romeo, that Angelo may say 'It is a horse'\n",
            "And gallop from Angelo's side.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Faith, he is no horse.\n",
            "\n",
            "ROMEO:\n",
            "Well, then, he's your horse.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "So shall you live,\n",
            "And live you with me.\n",
            "\n",
            "ROMEO:\n",
            "Such faith as these\n",
            "Shall, with proof and with proof, prove.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "O, then I see your face.\n",
            "\n",
            "ROMEO:\n",
            "'Tis very like so: 'tis to me it is:\n",
            "Faith, there is faith in Juliet's face,\n",
            "That there is faith in me.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "O, then, so shall that too;\n",
            "And so shall you live, and live but with me.\n",
            "\n",
            "ROMEO:\n",
            "So shall you die, and live but with me.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "O, then, so shall you die, and live but\n",
            "With me.\n",
            "\n",
            "ROMEO:\n",
            "And marry but with me.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "If that be no remedy, what remedy is there\n",
            "So far as Angelo is concerned?\n",
            "\n",
            "ROMEO:\n",
            "It should be enough for him: there is no redemption for\n",
            "That which can be sold for what can be spent.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Come, take thou the ring.\n",
            "\n",
            "First Murderer:\n",
            "Yes, my good lord.\n",
            "\n",
            "Second Murderer:\n",
            "As much for my poor health as for thy life\n",
            "That I spend this ring on thee! Take it not, I pray thee;\n",
            "It may be spent more than thou hast thought.\n",
            "\n",
            "First Murderer:\n",
            "What can be spent for what can be spent?\n",
            "\n",
            "Second Murderer:\n",
            "A ring.\n",
            "\n",
            "First Murderer:\n",
            "A knighthood.\n",
            "\n",
            "Second Murderer:\n",
            "A knighthood! why, is there no such thing as a ring?\n",
            "\n",
            "Second Murderer:\n",
            "No, if a man bid me a sceptre.\n",
            "\n",
            "First Murderer:\n",
            "He wants one too.\n",
            "\n",
            "Second Murderer:\n",
            "He wants a knighthood.\n",
            "\n",
            "First Murderer:\n",
            "He wants a life.\n",
            "\n",
            "Second Murderer:\n",
            "He wants a life.\n",
            "\n",
            "First Murderer:\n",
            "He wants a life.\n",
            "\n",
            "Second Murderer:\n",
            "He wants a life.\n",
            "\n",
            "First Murderer:\n",
            "He wants two.\n",
            "\n",
            "Second Murderer:\n",
            "He wants two.\n",
            "\n",
            "First Murderer:\n",
            "He wants two.\n",
            "\n",
            "Second Murderer:\n",
            "He wants two.\n",
            "\n",
            "First Murderer:\n",
            "He wants two.\n",
            "\n",
            "Second Murderer:\n",
            "He wants two.\n",
            "\n",
            "First Murderer:\n",
            "He wants two.\n",
            "\n",
            "Second Murderer:\n",
            "He wants two.\n",
            "\n",
            "First Murderer:\n",
            "He wants two.\n",
            "\n",
            "Second Murderer:\n",
            "He wants two.\n",
            "\n",
            "First Murderer:\n",
            "He wants two.\n",
            "\n",
            "Second Murderer:\n",
            "He wants two\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ql31J_-43-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b2061e4-0e3a-416c-be7f-36528589d9c6"
      },
      "source": [
        "gpt2.generate(sess, checkpoint_dir=os.path.join(base_dir,\"checkpoint\"), prefix=\"2015 年\")\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2015 年\n",
            "\n",
            "Provost:\n",
            "Where's my wife?\n",
            "\n",
            "Provost:\n",
            "Here, sir.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "If thou'rt sure\n",
            "That any man covetous or a heretic is my wife, thou'rt a traitors husband: if thou'rt a husband and a heretic is my wife, so be it; if husband and heretic not, then father.\n",
            "\n",
            "Provost:\n",
            "Well, well; marry, sir: here it is: if you be honest, you shall divorce me; if you be a heretic and a heretic, you shall divorce me; if a heretic be honest, he shall be a heretic; if a heretic be honest, he shall be a heretic; if a heretic be honest, he shall be a heretic; if a heretic be married, he shall be a heretic; if a heretic be honest, he shall be a heretic; if a heretic be married, he shall be a heretic; if a heretic be honest, he shall be a heretic; if a heretic be false, he shall be a heretic; if a heretic be lawful, he shall be a heretic; if a heretic be ta'en, he shall be a heretic; if a heretic be ta'en, he shall be a heretic; if a heretic be married, he shall be a heretic; if a heretic be married, he shall be a heretic; if a heretic be ta'en, he shall be a heretic; if a heretic be married, he shall be a heretic; if a heretic be married, he shall be a heretic; if a heretic be ta'en, he shall be a heretic; if a heretic be married, he shall be a heretic; if a heretic be married, he shall be a heretic; if a heretic be married, he shall be a heretic; if a heretic be ta'en, he shall be a heretic; if a heretic be married, he shall be a heretic; if a heretic be married, he shall be a heretic; if a heretic be married, he shall be a heretic; if a heretic be ta'en, he shall be a heretic; if a heretic be married, he shall be a heretic; if a heretic be married, he shall be a heretic; if a heretic be married, he shall be a heretic: if a heretic be ta'en, he shall be a heretic; if a heretic be married, he shall be a heretic; if a heretic be married, he shall be a heretic: if a heretic be married, he shall be ta'en: if a heretic be married, he shall be married: if a heretic be married, he shall be married\n",
            "and so walk not in the world without weeping.\n",
            "\n",
            "Provost:\n",
            "All sects here bow before him.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "If he were a father, bow before him too.\n",
            "\n",
            "Provost:\n",
            "Thou shalt not bow in time of peace.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "O, say you so? and if thou darest, break no sword.\n",
            "\n",
            "Provost:\n",
            "God save thee, gentle widow!\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Wilt thou be so strangled that others shall weep?\n",
            "\n",
            "Provost:\n",
            "No, my good wrangling man; say no.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Nay, but to the ground, and wet with weeping.\n",
            "I will not bear the sight of thee: let me sit with thee;\n",
            "And, as the ends of weeping beget\n",
            "Again in sweet harmony,--a harmony\n",
            "More pleasant than the shears--\n",
            "Than in thy notes and notes of breath,\n",
            "I will upbraid thy treasons with thy notes,\n",
            "And in the cheers of thine own cheers shall clang\n",
            "To the base breaking of thine ears.\n",
            "\n",
            "Provost:\n",
            "Thou hast amazed me. Come, widow, sit on me.\n",
            "I am the wife of a king; my breath is his.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "The devil's for laughing now! Come, sit, I'll help to stuff thee.\n",
            "I am his son-in-law, and thou shalt know\n",
            "How I will reconcile him to my bed.\n",
            "\n",
            "Provost:\n",
            "Thou wrat'st my heart: come, widow, sit; I'll help thee.\n",
            "I am thy patient brother: if thou wilt, be patient.\n",
            "I am offended; be patient, and soundly.\n",
            "\n",
            "QUEEN EL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSyr88L8Imj8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}