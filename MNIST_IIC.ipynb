{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_IIC.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOiyIDZfB4Q/CJkqL7Z+uny",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takky0330/NLP/blob/master/MNIST_IIC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJpl3D-lGdqh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dbea84a3-7b42-4342-95d7-697bb8913af0"
      },
      "source": [
        "# 乱数のシードを固定\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "SEED_VALUE = 1234  # これはなんでも良い\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED_VALUE)\n",
        "random.seed(SEED_VALUE)\n",
        "np.random.seed(SEED_VALUE)\n",
        "torch.manual_seed(SEED_VALUE)  # PyTorchを使う場合"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f05c70e61f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRitt6tZGwcN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cb54f1cd-e5c3-4c94-e4b1-685b212d31b5"
      },
      "source": [
        "# GPUが使えるときにはGPUに（Google Colaboratoryの場合はランタイムからGPUを指定）\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)  \n",
        "\n",
        "# GPUを使用。cudaと出力されるのを確認する。"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd8ykAUpG2tO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MNISTの画像をダウンロードし、DataLoaderにする（TrainとTest）\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "batch_size_train = 512\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('.', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                   ])),\n",
        "    batch_size=batch_size_train, shuffle=True, drop_last=True)\n",
        "# drop_lastは最後のミニバッチが規定のサイズより小さい場合は使用しない設定\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('.', train=False, transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])),\n",
        "    batch_size=1024, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6skS5tfaHMzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ディープラーニングモデル\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "OVER_CLUSTRING_Rate = 10  # 多めに分類するoverclsuteringも用意する\n",
        "\n",
        "\n",
        "class NetIIC(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetIIC, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 128, 5, 2, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(128)\n",
        "        self.conv2 = nn.Conv2d(128, 128, 5, 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 128, 5, 1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, 4, 1, bias=False)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        \n",
        "        # 0-9に対応すると期待したい10種類のクラス\n",
        "        self.fc = nn.Linear(256, 10)\n",
        "\n",
        "        # overclustering\n",
        "        # 実際の想定よりも多めにクラスタリングさせることで、ネットワークで微細な変化を捉えられるようにする\n",
        "        self.fc_overclustering = nn.Linear(256, 10*OVER_CLUSTRING_Rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x_prefinal = x.view(x.size(0), -1)\n",
        "        y = F.softmax(self.fc(x_prefinal), dim=1)\n",
        "\n",
        "        y_overclustering = F.softmax(self.fc_overclustering(\n",
        "            x_prefinal), dim=1)  # overclustering\n",
        "\n",
        "        return y, y_overclustering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2os-72CHP-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.init as init\n",
        "\n",
        "\n",
        "def weight_init(m):\n",
        "    \"\"\"重み初期化\"\"\"\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.BatchNorm2d):\n",
        "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
        "        init.constant_(m.bias.data, 0)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        # Xavier\n",
        "        #init.xavier_normal_(m.weight.data)\n",
        "\n",
        "        # He \n",
        "        init.kaiming_normal_(m.weight.data)\n",
        "        \n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uplfMQjDHUdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# データにノイズを加える関数の定義\n",
        "import torchvision as tv\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "\n",
        "def perturb_imagedata(x):\n",
        "    y = x.clone()\n",
        "    batch_size = x.size(0)\n",
        "\n",
        "    # ランダムなアフィン変換を実施\n",
        "    trans = tv.transforms.RandomAffine(15, (0.2, 0.2,), (0.2, 0.75,))\n",
        "    for i in range(batch_size):\n",
        "        y[i, 0] = TF.to_tensor(trans(TF.to_pil_image(y[i, 0])))\n",
        "\n",
        "    # ノイズを加える\n",
        "    noise = torch.randn(batch_size, 1, x.size(2), x.size(3))\n",
        "    div = torch.randint(20, 30, (batch_size,),\n",
        "                        dtype=torch.float32).view(batch_size, 1, 1, 1)\n",
        "    y += noise / div\n",
        "\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNC0oyxNHXpv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IISによる損失関数の定義\n",
        "# 参考：https://github.com/RuABraun/phone-clustering/blob/master/mnist_basic.py\n",
        "import sys\n",
        "\n",
        "\n",
        "def compute_joint(x_out, x_tf_out):\n",
        "\n",
        "    # x_out、x_tf_outは torch.Size([512, 10])。この二つをかけ算して同時分布を求める、torch.Size([2048, 10, 10])にする。\n",
        "    # torch.Size([512, 10, 1]) * torch.Size([512, 1, 10])\n",
        "    p_i_j = x_out.unsqueeze(2) * x_tf_out.unsqueeze(1)\n",
        "    # p_i_j は　torch.Size([512, 10, 10])\n",
        "\n",
        "    # 全ミニバッチを足し算する ⇒ torch.Size([10, 10])\n",
        "    p_i_j = p_i_j.sum(dim=0)\n",
        "\n",
        "    # 転置行列と足し算して割り算（対称化） ⇒ torch.Size([10, 10])\n",
        "    p_i_j = (p_i_j + p_i_j.t()) / 2.\n",
        "\n",
        "    # 規格化 ⇒ torch.Size([10, 10])\n",
        "    p_i_j = p_i_j / p_i_j.sum()\n",
        "\n",
        "    return p_i_j\n",
        "    # 結局、p_i_jは通常画像の判定出力10種類と、変換画像の判定10種類の100パターンに対して、全ミニバッチが100パターンのどれだったのかの確率分布表を示す\n",
        "\n",
        "\n",
        "def IID_loss(x_out, x_tf_out, EPS=sys.float_info.epsilon):\n",
        "    # torch.Size([512, 10])、後ろの10は分類数なので、overclusteringのときは100\n",
        "    bs, k = x_out.size()\n",
        "    p_i_j = compute_joint(x_out, x_tf_out)  # torch.Size([10, 10])\n",
        "\n",
        "    # 同時確率の分布表から、変換画像の10パターンをsumをして周辺化し、元画像だけの周辺確率の分布表を作る\n",
        "    p_i = p_i_j.sum(dim=1).view(k, 1).expand(k, k)\n",
        "    # 同時確率の分布表から、元画像の10パターンをsumをして周辺化し、変換画像だけの周辺確率の分布表を作る\n",
        "    p_j = p_i_j.sum(dim=0).view(1, k).expand(k, k)\n",
        "\n",
        "    # 0に近い値をlogに入れると発散するので、避ける\n",
        "    #p_i_j[(p_i_j < EPS).data] = EPS\n",
        "    #p_j[(p_j < EPS).data] = EPS\n",
        "    #p_i[(p_i < EPS).data] = EPS\n",
        "    # 参考GitHubの実装（↑）は、PyTorchのバージョン1.3以上だとエラーになる\n",
        "    # https://discuss.pytorch.org/t/pytorch-1-3-showing-an-error-perhaps-for-loss-computed-from-paired-outputs/68790/3\n",
        "\n",
        "    # 0に近い値をlogに入れると発散するので、避ける\n",
        "    p_i_j = torch.where(p_i_j < EPS, torch.tensor(\n",
        "        [EPS], device=p_i_j.device), p_i_j)\n",
        "    p_j = torch.where(p_j < EPS, torch.tensor([EPS], device=p_j.device), p_j)\n",
        "    p_i = torch.where(p_i < EPS, torch.tensor([EPS], device=p_i.device), p_i)\n",
        "\n",
        "    # 元画像、変換画像の同時確率と周辺確率から、相互情報量を計算\n",
        "    # ただし、マイナスをかけて最小化問題にする\n",
        "    \"\"\"\n",
        "    相互情報量を最大化したい\n",
        "    ⇒結局、x_out, x_tf_outが持ちあう情報量が多くなって欲しい\n",
        "    ⇒要は、x_out, x_tf_outが一緒になって欲しい\n",
        "\n",
        "    p_i_jはx_out, x_tf_outの同時確率分布で、ミニバッチが極力、10×10のいろんなパターン、満遍なく一様が嬉しい\n",
        "    \n",
        "    前半の項、torch.log(p_i_j)はp_ijがどれも1に近いと大きな値（0に近い）になる。\n",
        "    どれかが1であと0でバラついていないと、log0で小さな値（負の大きな値）になる\n",
        "    つまり前半の項は、\n",
        "\n",
        "    後半の項は、元画像、もしくは変換画像について、それぞれ周辺化して10通りのどれになるかを計算した項。\n",
        "    周辺化した10×10のパターンを引き算して、前半の項が小さくなるのであれば、\n",
        "    x_outとx_tf_outはあまり情報を共有していなかったことになる。\n",
        "    \"\"\"\n",
        "    # https://qiita.com/Amanokawa/items/0aa24bc396dd88fb7d2a\n",
        "    # を参考に、重みalphaを追加\n",
        "    # 同時確率分布表のばらつきによる罰則を小さく ＝ 同時確率の分布がバラつきやすくする\n",
        "    alpha = 2.0  # 論文や通常の相互情報量の計算はalphaは1です\n",
        "\n",
        "    loss = -1*(p_i_j * (torch.log(p_i_j) - alpha *\n",
        "                        torch.log(p_j) - alpha*torch.log(p_i))).sum()\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhfAc4ZpHbkc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dc6b6bc8-ab19-4809-d365-f6ac00e4e482"
      },
      "source": [
        "# 訓練の実施\n",
        "total_epoch = 20\n",
        "\n",
        "\n",
        "# モデル\n",
        "model = NetIIC()\n",
        "model.apply(weight_init)\n",
        "model.to(device)\n",
        "\n",
        "# 最適化関数を設定\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "def train(total_epoch, model, train_loader, optimizer, device):\n",
        "\n",
        "    model.train()\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "        optimizer, T_0=2, T_mult=2)\n",
        "\n",
        "    for epoch in range(total_epoch):\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "            # 学習率変化\n",
        "            scheduler.step()\n",
        "\n",
        "            # 微妙に変換したデータを作る。SIMULTANEOUS_NUM分のペアを作る\n",
        "            data_perturb = perturb_imagedata(data)  # ノイズを与える\n",
        "\n",
        "            # GPUに送れる場合は送る\n",
        "            data = data.to(device)\n",
        "            data_perturb = data_perturb.to(device)\n",
        "\n",
        "            # 最適化関数の初期化\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # ニューラルネットワーク出力\n",
        "            output, output_overclustering = model(data)\n",
        "            output_perturb, output_perturb_overclustering = model(data_perturb)\n",
        "\n",
        "            # 損失の計算\n",
        "            loss1 = IID_loss(output, output_perturb)\n",
        "            loss2 = IID_loss(output_overclustering,\n",
        "                             output_perturb_overclustering)\n",
        "            loss = loss1 + loss2\n",
        "\n",
        "            # 損失を減らすように更新\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # ログ出力\n",
        "            if batch_idx % 10 == 0:\n",
        "                print('Train Epoch {}:iter{} - \\tLoss1: {:.6f}- \\tLoss2: {:.6f}- \\tLoss_total: {:.6f}'.format(\n",
        "                    epoch, batch_idx, loss1.item(), loss2.item(), loss1.item()+loss2.item()))\n",
        "\n",
        "    return model, optimizer\n",
        "\n",
        "\n",
        "model_trained, optimizer = train(\n",
        "    total_epoch, model, train_loader, optimizer, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 0:iter0 - \tLoss1: -4.061602- \tLoss2: -7.873592- \tLoss_total: -11.935194\n",
            "Train Epoch 0:iter10 - \tLoss1: -4.624082- \tLoss2: -9.001415- \tLoss_total: -13.625497\n",
            "Train Epoch 0:iter20 - \tLoss1: -4.639643- \tLoss2: -9.181597- \tLoss_total: -13.821239\n",
            "Train Epoch 0:iter30 - \tLoss1: -4.643174- \tLoss2: -9.221941- \tLoss_total: -13.865115\n",
            "Train Epoch 0:iter40 - \tLoss1: -4.686613- \tLoss2: -9.257360- \tLoss_total: -13.943974\n",
            "Train Epoch 0:iter50 - \tLoss1: -4.694698- \tLoss2: -9.264404- \tLoss_total: -13.959103\n",
            "Train Epoch 0:iter60 - \tLoss1: -4.716262- \tLoss2: -9.279085- \tLoss_total: -13.995347\n",
            "Train Epoch 0:iter70 - \tLoss1: -4.793472- \tLoss2: -9.307110- \tLoss_total: -14.100582\n",
            "Train Epoch 0:iter80 - \tLoss1: -4.853398- \tLoss2: -9.384169- \tLoss_total: -14.237566\n",
            "Train Epoch 0:iter90 - \tLoss1: -5.003675- \tLoss2: -9.456626- \tLoss_total: -14.460300\n",
            "Train Epoch 0:iter100 - \tLoss1: -5.014050- \tLoss2: -9.507706- \tLoss_total: -14.521755\n",
            "Train Epoch 0:iter110 - \tLoss1: -5.077678- \tLoss2: -9.552149- \tLoss_total: -14.629827\n",
            "Train Epoch 1:iter0 - \tLoss1: -5.035692- \tLoss2: -9.563547- \tLoss_total: -14.599239\n",
            "Train Epoch 1:iter10 - \tLoss1: -5.006156- \tLoss2: -9.541706- \tLoss_total: -14.547863\n",
            "Train Epoch 1:iter20 - \tLoss1: -5.137530- \tLoss2: -9.703734- \tLoss_total: -14.841265\n",
            "Train Epoch 1:iter30 - \tLoss1: -5.237497- \tLoss2: -9.862679- \tLoss_total: -15.100176\n",
            "Train Epoch 1:iter40 - \tLoss1: -5.244847- \tLoss2: -9.946080- \tLoss_total: -15.190928\n",
            "Train Epoch 1:iter50 - \tLoss1: -5.314639- \tLoss2: -10.082758- \tLoss_total: -15.397397\n",
            "Train Epoch 1:iter60 - \tLoss1: -5.343108- \tLoss2: -10.197802- \tLoss_total: -15.540909\n",
            "Train Epoch 1:iter70 - \tLoss1: -5.341678- \tLoss2: -10.248281- \tLoss_total: -15.589958\n",
            "Train Epoch 1:iter80 - \tLoss1: -5.436913- \tLoss2: -10.399940- \tLoss_total: -15.836853\n",
            "Train Epoch 1:iter90 - \tLoss1: -5.481342- \tLoss2: -10.462492- \tLoss_total: -15.943834\n",
            "Train Epoch 1:iter100 - \tLoss1: -5.500678- \tLoss2: -10.506439- \tLoss_total: -16.007117\n",
            "Train Epoch 1:iter110 - \tLoss1: -5.508757- \tLoss2: -10.534906- \tLoss_total: -16.043664\n",
            "Train Epoch 2:iter0 - \tLoss1: -5.482544- \tLoss2: -10.550426- \tLoss_total: -16.032969\n",
            "Train Epoch 2:iter10 - \tLoss1: -5.577185- \tLoss2: -10.617562- \tLoss_total: -16.194747\n",
            "Train Epoch 2:iter20 - \tLoss1: -5.305120- \tLoss2: -10.361935- \tLoss_total: -15.667055\n",
            "Train Epoch 2:iter30 - \tLoss1: -5.518410- \tLoss2: -10.620216- \tLoss_total: -16.138626\n",
            "Train Epoch 2:iter40 - \tLoss1: -5.513078- \tLoss2: -10.687775- \tLoss_total: -16.200852\n",
            "Train Epoch 2:iter50 - \tLoss1: -5.620554- \tLoss2: -10.826029- \tLoss_total: -16.446583\n",
            "Train Epoch 2:iter60 - \tLoss1: -5.578266- \tLoss2: -10.928873- \tLoss_total: -16.507139\n",
            "Train Epoch 2:iter70 - \tLoss1: -5.690506- \tLoss2: -11.044922- \tLoss_total: -16.735428\n",
            "Train Epoch 2:iter80 - \tLoss1: -5.697420- \tLoss2: -11.137171- \tLoss_total: -16.834591\n",
            "Train Epoch 2:iter90 - \tLoss1: -5.678658- \tLoss2: -11.214434- \tLoss_total: -16.893091\n",
            "Train Epoch 2:iter100 - \tLoss1: -5.729015- \tLoss2: -11.294080- \tLoss_total: -17.023095\n",
            "Train Epoch 2:iter110 - \tLoss1: -5.774957- \tLoss2: -11.309692- \tLoss_total: -17.084649\n",
            "Train Epoch 3:iter0 - \tLoss1: -5.816159- \tLoss2: -11.398851- \tLoss_total: -17.215011\n",
            "Train Epoch 3:iter10 - \tLoss1: -5.831754- \tLoss2: -11.475180- \tLoss_total: -17.306933\n",
            "Train Epoch 3:iter20 - \tLoss1: -5.735518- \tLoss2: -11.439198- \tLoss_total: -17.174716\n",
            "Train Epoch 3:iter30 - \tLoss1: -5.869507- \tLoss2: -11.538724- \tLoss_total: -17.408231\n",
            "Train Epoch 3:iter40 - \tLoss1: -5.770392- \tLoss2: -11.462257- \tLoss_total: -17.232650\n",
            "Train Epoch 3:iter50 - \tLoss1: -5.886295- \tLoss2: -11.613934- \tLoss_total: -17.500229\n",
            "Train Epoch 3:iter60 - \tLoss1: -5.838268- \tLoss2: -11.513980- \tLoss_total: -17.352248\n",
            "Train Epoch 3:iter70 - \tLoss1: -5.940125- \tLoss2: -11.631887- \tLoss_total: -17.572012\n",
            "Train Epoch 3:iter80 - \tLoss1: -5.934083- \tLoss2: -11.630526- \tLoss_total: -17.564608\n",
            "Train Epoch 3:iter90 - \tLoss1: -5.923160- \tLoss2: -11.624449- \tLoss_total: -17.547608\n",
            "Train Epoch 3:iter100 - \tLoss1: -5.911969- \tLoss2: -11.600749- \tLoss_total: -17.512718\n",
            "Train Epoch 3:iter110 - \tLoss1: -5.842829- \tLoss2: -11.549757- \tLoss_total: -17.392586\n",
            "Train Epoch 4:iter0 - \tLoss1: -5.911496- \tLoss2: -11.663288- \tLoss_total: -17.574784\n",
            "Train Epoch 4:iter10 - \tLoss1: -6.006437- \tLoss2: -11.696562- \tLoss_total: -17.702999\n",
            "Train Epoch 4:iter20 - \tLoss1: -5.907553- \tLoss2: -11.613171- \tLoss_total: -17.520723\n",
            "Train Epoch 4:iter30 - \tLoss1: -5.957075- \tLoss2: -11.612789- \tLoss_total: -17.569864\n",
            "Train Epoch 4:iter40 - \tLoss1: -5.953668- \tLoss2: -11.621546- \tLoss_total: -17.575214\n",
            "Train Epoch 4:iter50 - \tLoss1: -5.777260- \tLoss2: -11.502979- \tLoss_total: -17.280239\n",
            "Train Epoch 4:iter60 - \tLoss1: -5.850815- \tLoss2: -11.580862- \tLoss_total: -17.431677\n",
            "Train Epoch 4:iter70 - \tLoss1: -5.877108- \tLoss2: -11.582394- \tLoss_total: -17.459501\n",
            "Train Epoch 4:iter80 - \tLoss1: -5.771517- \tLoss2: -11.482591- \tLoss_total: -17.254107\n",
            "Train Epoch 4:iter90 - \tLoss1: -5.859328- \tLoss2: -11.566597- \tLoss_total: -17.425925\n",
            "Train Epoch 4:iter100 - \tLoss1: -5.862949- \tLoss2: -11.658970- \tLoss_total: -17.521919\n",
            "Train Epoch 4:iter110 - \tLoss1: -5.962408- \tLoss2: -11.698452- \tLoss_total: -17.660860\n",
            "Train Epoch 5:iter0 - \tLoss1: -5.897786- \tLoss2: -11.654272- \tLoss_total: -17.552058\n",
            "Train Epoch 5:iter10 - \tLoss1: -5.976210- \tLoss2: -11.697526- \tLoss_total: -17.673736\n",
            "Train Epoch 5:iter20 - \tLoss1: -5.850733- \tLoss2: -11.667037- \tLoss_total: -17.517770\n",
            "Train Epoch 5:iter30 - \tLoss1: -5.903390- \tLoss2: -11.728672- \tLoss_total: -17.632062\n",
            "Train Epoch 5:iter40 - \tLoss1: -5.947185- \tLoss2: -11.712778- \tLoss_total: -17.659963\n",
            "Train Epoch 5:iter50 - \tLoss1: -6.007760- \tLoss2: -11.750060- \tLoss_total: -17.757820\n",
            "Train Epoch 5:iter60 - \tLoss1: -5.976474- \tLoss2: -11.765444- \tLoss_total: -17.741918\n",
            "Train Epoch 5:iter70 - \tLoss1: -6.068260- \tLoss2: -11.823656- \tLoss_total: -17.891916\n",
            "Train Epoch 5:iter80 - \tLoss1: -5.978346- \tLoss2: -11.781742- \tLoss_total: -17.760088\n",
            "Train Epoch 5:iter90 - \tLoss1: -5.904665- \tLoss2: -11.696868- \tLoss_total: -17.601533\n",
            "Train Epoch 5:iter100 - \tLoss1: -5.984692- \tLoss2: -11.858913- \tLoss_total: -17.843606\n",
            "Train Epoch 5:iter110 - \tLoss1: -6.045609- \tLoss2: -11.804277- \tLoss_total: -17.849886\n",
            "Train Epoch 6:iter0 - \tLoss1: -6.045539- \tLoss2: -11.847056- \tLoss_total: -17.892595\n",
            "Train Epoch 6:iter10 - \tLoss1: -6.015754- \tLoss2: -11.833085- \tLoss_total: -17.848839\n",
            "Train Epoch 6:iter20 - \tLoss1: -5.970227- \tLoss2: -11.849077- \tLoss_total: -17.819304\n",
            "Train Epoch 6:iter30 - \tLoss1: -6.110634- \tLoss2: -11.860206- \tLoss_total: -17.970840\n",
            "Train Epoch 6:iter40 - \tLoss1: -6.039401- \tLoss2: -11.830887- \tLoss_total: -17.870288\n",
            "Train Epoch 6:iter50 - \tLoss1: -6.147850- \tLoss2: -11.944992- \tLoss_total: -18.092842\n",
            "Train Epoch 6:iter60 - \tLoss1: -6.016668- \tLoss2: -11.921782- \tLoss_total: -17.938450\n",
            "Train Epoch 6:iter70 - \tLoss1: -6.058099- \tLoss2: -11.887714- \tLoss_total: -17.945813\n",
            "Train Epoch 6:iter80 - \tLoss1: -6.045415- \tLoss2: -11.915167- \tLoss_total: -17.960582\n",
            "Train Epoch 6:iter90 - \tLoss1: -6.153411- \tLoss2: -11.966964- \tLoss_total: -18.120375\n",
            "Train Epoch 6:iter100 - \tLoss1: -6.131104- \tLoss2: -11.869986- \tLoss_total: -18.001090\n",
            "Train Epoch 6:iter110 - \tLoss1: -6.136299- \tLoss2: -11.949294- \tLoss_total: -18.085593\n",
            "Train Epoch 7:iter0 - \tLoss1: -6.113964- \tLoss2: -11.905538- \tLoss_total: -18.019502\n",
            "Train Epoch 7:iter10 - \tLoss1: -6.120946- \tLoss2: -11.978884- \tLoss_total: -18.099830\n",
            "Train Epoch 7:iter20 - \tLoss1: -6.108635- \tLoss2: -11.983107- \tLoss_total: -18.091742\n",
            "Train Epoch 7:iter30 - \tLoss1: -6.205224- \tLoss2: -11.984566- \tLoss_total: -18.189789\n",
            "Train Epoch 7:iter40 - \tLoss1: -6.006099- \tLoss2: -11.957884- \tLoss_total: -17.963983\n",
            "Train Epoch 7:iter50 - \tLoss1: -6.171672- \tLoss2: -12.021980- \tLoss_total: -18.193652\n",
            "Train Epoch 7:iter60 - \tLoss1: -6.174325- \tLoss2: -12.011498- \tLoss_total: -18.185823\n",
            "Train Epoch 7:iter70 - \tLoss1: -6.159746- \tLoss2: -12.031435- \tLoss_total: -18.191181\n",
            "Train Epoch 7:iter80 - \tLoss1: -6.085917- \tLoss2: -12.008327- \tLoss_total: -18.094245\n",
            "Train Epoch 7:iter90 - \tLoss1: -6.154988- \tLoss2: -12.009090- \tLoss_total: -18.164078\n",
            "Train Epoch 7:iter100 - \tLoss1: -6.205307- \tLoss2: -11.965458- \tLoss_total: -18.170765\n",
            "Train Epoch 7:iter110 - \tLoss1: -6.218685- \tLoss2: -12.037171- \tLoss_total: -18.255857\n",
            "Train Epoch 8:iter0 - \tLoss1: -6.155997- \tLoss2: -12.036416- \tLoss_total: -18.192413\n",
            "Train Epoch 8:iter10 - \tLoss1: -6.161731- \tLoss2: -11.950370- \tLoss_total: -18.112101\n",
            "Train Epoch 8:iter20 - \tLoss1: -6.144145- \tLoss2: -12.035259- \tLoss_total: -18.179405\n",
            "Train Epoch 8:iter30 - \tLoss1: -6.202259- \tLoss2: -12.034152- \tLoss_total: -18.236411\n",
            "Train Epoch 8:iter40 - \tLoss1: -6.100112- \tLoss2: -11.978409- \tLoss_total: -18.078521\n",
            "Train Epoch 8:iter50 - \tLoss1: -6.204380- \tLoss2: -11.986184- \tLoss_total: -18.190564\n",
            "Train Epoch 8:iter60 - \tLoss1: -6.255268- \tLoss2: -12.006737- \tLoss_total: -18.262005\n",
            "Train Epoch 8:iter70 - \tLoss1: -6.215478- \tLoss2: -12.049738- \tLoss_total: -18.265216\n",
            "Train Epoch 8:iter80 - \tLoss1: -6.149734- \tLoss2: -12.014138- \tLoss_total: -18.163872\n",
            "Train Epoch 8:iter90 - \tLoss1: -6.085509- \tLoss2: -11.972765- \tLoss_total: -18.058274\n",
            "Train Epoch 8:iter100 - \tLoss1: -6.100843- \tLoss2: -11.955891- \tLoss_total: -18.056734\n",
            "Train Epoch 8:iter110 - \tLoss1: -6.061975- \tLoss2: -11.966610- \tLoss_total: -18.028584\n",
            "Train Epoch 9:iter0 - \tLoss1: -5.988102- \tLoss2: -11.915594- \tLoss_total: -17.903696\n",
            "Train Epoch 9:iter10 - \tLoss1: -6.105665- \tLoss2: -11.974596- \tLoss_total: -18.080261\n",
            "Train Epoch 9:iter20 - \tLoss1: -6.001696- \tLoss2: -11.854904- \tLoss_total: -17.856600\n",
            "Train Epoch 9:iter30 - \tLoss1: -6.014046- \tLoss2: -11.979271- \tLoss_total: -17.993317\n",
            "Train Epoch 9:iter40 - \tLoss1: -6.170128- \tLoss2: -12.007122- \tLoss_total: -18.177250\n",
            "Train Epoch 9:iter50 - \tLoss1: -6.179873- \tLoss2: -12.007433- \tLoss_total: -18.187306\n",
            "Train Epoch 9:iter60 - \tLoss1: -6.134513- \tLoss2: -11.994936- \tLoss_total: -18.129449\n",
            "Train Epoch 9:iter70 - \tLoss1: -6.117822- \tLoss2: -11.997934- \tLoss_total: -18.115757\n",
            "Train Epoch 9:iter80 - \tLoss1: -6.001730- \tLoss2: -11.968907- \tLoss_total: -17.970637\n",
            "Train Epoch 9:iter90 - \tLoss1: -6.148654- \tLoss2: -12.027296- \tLoss_total: -18.175950\n",
            "Train Epoch 9:iter100 - \tLoss1: -6.130399- \tLoss2: -12.030069- \tLoss_total: -18.160468\n",
            "Train Epoch 9:iter110 - \tLoss1: -6.047699- \tLoss2: -11.949283- \tLoss_total: -17.996982\n",
            "Train Epoch 10:iter0 - \tLoss1: -6.136370- \tLoss2: -12.022620- \tLoss_total: -18.158990\n",
            "Train Epoch 10:iter10 - \tLoss1: -6.148389- \tLoss2: -12.094507- \tLoss_total: -18.242897\n",
            "Train Epoch 10:iter20 - \tLoss1: -6.037716- \tLoss2: -12.017687- \tLoss_total: -18.055403\n",
            "Train Epoch 10:iter30 - \tLoss1: -6.168137- \tLoss2: -12.063092- \tLoss_total: -18.231229\n",
            "Train Epoch 10:iter40 - \tLoss1: -6.202125- \tLoss2: -12.024322- \tLoss_total: -18.226446\n",
            "Train Epoch 10:iter50 - \tLoss1: -6.146228- \tLoss2: -12.010548- \tLoss_total: -18.156775\n",
            "Train Epoch 10:iter60 - \tLoss1: -6.140963- \tLoss2: -12.043675- \tLoss_total: -18.184638\n",
            "Train Epoch 10:iter70 - \tLoss1: -6.191179- \tLoss2: -12.004755- \tLoss_total: -18.195934\n",
            "Train Epoch 10:iter80 - \tLoss1: -6.256563- \tLoss2: -12.139338- \tLoss_total: -18.395902\n",
            "Train Epoch 10:iter90 - \tLoss1: -6.168355- \tLoss2: -12.083624- \tLoss_total: -18.251979\n",
            "Train Epoch 10:iter100 - \tLoss1: -6.284739- \tLoss2: -12.099825- \tLoss_total: -18.384563\n",
            "Train Epoch 10:iter110 - \tLoss1: -6.235079- \tLoss2: -12.077784- \tLoss_total: -18.312862\n",
            "Train Epoch 11:iter0 - \tLoss1: -6.267312- \tLoss2: -12.101583- \tLoss_total: -18.368895\n",
            "Train Epoch 11:iter10 - \tLoss1: -6.307058- \tLoss2: -12.139622- \tLoss_total: -18.446680\n",
            "Train Epoch 11:iter20 - \tLoss1: -6.123943- \tLoss2: -12.045473- \tLoss_total: -18.169416\n",
            "Train Epoch 11:iter30 - \tLoss1: -6.167233- \tLoss2: -12.062683- \tLoss_total: -18.229917\n",
            "Train Epoch 11:iter40 - \tLoss1: -6.203679- \tLoss2: -12.120104- \tLoss_total: -18.323782\n",
            "Train Epoch 11:iter50 - \tLoss1: -6.243313- \tLoss2: -12.113079- \tLoss_total: -18.356392\n",
            "Train Epoch 11:iter60 - \tLoss1: -6.146537- \tLoss2: -12.091185- \tLoss_total: -18.237721\n",
            "Train Epoch 11:iter70 - \tLoss1: -6.311295- \tLoss2: -12.184025- \tLoss_total: -18.495319\n",
            "Train Epoch 11:iter80 - \tLoss1: -6.254798- \tLoss2: -12.146633- \tLoss_total: -18.401431\n",
            "Train Epoch 11:iter90 - \tLoss1: -6.170853- \tLoss2: -12.079359- \tLoss_total: -18.250212\n",
            "Train Epoch 11:iter100 - \tLoss1: -6.343009- \tLoss2: -12.116261- \tLoss_total: -18.459270\n",
            "Train Epoch 11:iter110 - \tLoss1: -6.303205- \tLoss2: -12.206921- \tLoss_total: -18.510125\n",
            "Train Epoch 12:iter0 - \tLoss1: -6.256334- \tLoss2: -12.224617- \tLoss_total: -18.480951\n",
            "Train Epoch 12:iter10 - \tLoss1: -6.215661- \tLoss2: -12.115475- \tLoss_total: -18.331136\n",
            "Train Epoch 12:iter20 - \tLoss1: -6.321229- \tLoss2: -12.119423- \tLoss_total: -18.440652\n",
            "Train Epoch 12:iter30 - \tLoss1: -6.329758- \tLoss2: -12.194632- \tLoss_total: -18.524389\n",
            "Train Epoch 12:iter40 - \tLoss1: -6.253160- \tLoss2: -12.183805- \tLoss_total: -18.436965\n",
            "Train Epoch 12:iter50 - \tLoss1: -6.275735- \tLoss2: -12.155109- \tLoss_total: -18.430844\n",
            "Train Epoch 12:iter60 - \tLoss1: -6.286191- \tLoss2: -12.208385- \tLoss_total: -18.494576\n",
            "Train Epoch 12:iter70 - \tLoss1: -6.292780- \tLoss2: -12.173989- \tLoss_total: -18.466770\n",
            "Train Epoch 12:iter80 - \tLoss1: -6.323388- \tLoss2: -12.225878- \tLoss_total: -18.549265\n",
            "Train Epoch 12:iter90 - \tLoss1: -6.295289- \tLoss2: -12.187088- \tLoss_total: -18.482377\n",
            "Train Epoch 12:iter100 - \tLoss1: -6.280047- \tLoss2: -12.255468- \tLoss_total: -18.535516\n",
            "Train Epoch 12:iter110 - \tLoss1: -6.305418- \tLoss2: -12.255239- \tLoss_total: -18.560657\n",
            "Train Epoch 13:iter0 - \tLoss1: -6.307186- \tLoss2: -12.177588- \tLoss_total: -18.484773\n",
            "Train Epoch 13:iter10 - \tLoss1: -6.334632- \tLoss2: -12.240593- \tLoss_total: -18.575225\n",
            "Train Epoch 13:iter20 - \tLoss1: -6.336927- \tLoss2: -12.218473- \tLoss_total: -18.555401\n",
            "Train Epoch 13:iter30 - \tLoss1: -6.306289- \tLoss2: -12.201038- \tLoss_total: -18.507327\n",
            "Train Epoch 13:iter40 - \tLoss1: -6.284335- \tLoss2: -12.239736- \tLoss_total: -18.524070\n",
            "Train Epoch 13:iter50 - \tLoss1: -6.286605- \tLoss2: -12.215553- \tLoss_total: -18.502159\n",
            "Train Epoch 13:iter60 - \tLoss1: -6.335791- \tLoss2: -12.224860- \tLoss_total: -18.560651\n",
            "Train Epoch 13:iter70 - \tLoss1: -6.236014- \tLoss2: -12.180894- \tLoss_total: -18.416908\n",
            "Train Epoch 13:iter80 - \tLoss1: -6.247663- \tLoss2: -12.234299- \tLoss_total: -18.481962\n",
            "Train Epoch 13:iter90 - \tLoss1: -6.376589- \tLoss2: -12.222892- \tLoss_total: -18.599481\n",
            "Train Epoch 13:iter100 - \tLoss1: -6.318368- \tLoss2: -12.169004- \tLoss_total: -18.487372\n",
            "Train Epoch 13:iter110 - \tLoss1: -6.323361- \tLoss2: -12.200377- \tLoss_total: -18.523737\n",
            "Train Epoch 14:iter0 - \tLoss1: -6.311910- \tLoss2: -12.245097- \tLoss_total: -18.557007\n",
            "Train Epoch 14:iter10 - \tLoss1: -6.379493- \tLoss2: -12.233379- \tLoss_total: -18.612872\n",
            "Train Epoch 14:iter20 - \tLoss1: -6.335558- \tLoss2: -12.167107- \tLoss_total: -18.502665\n",
            "Train Epoch 14:iter30 - \tLoss1: -6.325654- \tLoss2: -12.179461- \tLoss_total: -18.505116\n",
            "Train Epoch 14:iter40 - \tLoss1: -6.338814- \tLoss2: -12.282618- \tLoss_total: -18.621432\n",
            "Train Epoch 14:iter50 - \tLoss1: -6.397748- \tLoss2: -12.282754- \tLoss_total: -18.680502\n",
            "Train Epoch 14:iter60 - \tLoss1: -6.433876- \tLoss2: -12.337371- \tLoss_total: -18.771246\n",
            "Train Epoch 14:iter70 - \tLoss1: -6.445478- \tLoss2: -12.342808- \tLoss_total: -18.788286\n",
            "Train Epoch 14:iter80 - \tLoss1: -6.323716- \tLoss2: -12.238568- \tLoss_total: -18.562284\n",
            "Train Epoch 14:iter90 - \tLoss1: -6.408041- \tLoss2: -12.259398- \tLoss_total: -18.667439\n",
            "Train Epoch 14:iter100 - \tLoss1: -6.284018- \tLoss2: -12.278667- \tLoss_total: -18.562685\n",
            "Train Epoch 14:iter110 - \tLoss1: -6.402387- \tLoss2: -12.237026- \tLoss_total: -18.639413\n",
            "Train Epoch 15:iter0 - \tLoss1: -6.401947- \tLoss2: -12.265686- \tLoss_total: -18.667633\n",
            "Train Epoch 15:iter10 - \tLoss1: -6.344089- \tLoss2: -12.230185- \tLoss_total: -18.574274\n",
            "Train Epoch 15:iter20 - \tLoss1: -6.336463- \tLoss2: -12.241658- \tLoss_total: -18.578121\n",
            "Train Epoch 15:iter30 - \tLoss1: -6.444695- \tLoss2: -12.299257- \tLoss_total: -18.743953\n",
            "Train Epoch 15:iter40 - \tLoss1: -6.374648- \tLoss2: -12.244750- \tLoss_total: -18.619398\n",
            "Train Epoch 15:iter50 - \tLoss1: -6.424206- \tLoss2: -12.306890- \tLoss_total: -18.731096\n",
            "Train Epoch 15:iter60 - \tLoss1: -6.347690- \tLoss2: -12.239361- \tLoss_total: -18.587051\n",
            "Train Epoch 15:iter70 - \tLoss1: -6.337483- \tLoss2: -12.269457- \tLoss_total: -18.606940\n",
            "Train Epoch 15:iter80 - \tLoss1: -6.353546- \tLoss2: -12.331322- \tLoss_total: -18.684868\n",
            "Train Epoch 15:iter90 - \tLoss1: -6.291472- \tLoss2: -12.264845- \tLoss_total: -18.556317\n",
            "Train Epoch 15:iter100 - \tLoss1: -6.355967- \tLoss2: -12.242069- \tLoss_total: -18.598036\n",
            "Train Epoch 15:iter110 - \tLoss1: -6.429867- \tLoss2: -12.271858- \tLoss_total: -18.701725\n",
            "Train Epoch 16:iter0 - \tLoss1: -6.381427- \tLoss2: -12.345852- \tLoss_total: -18.727279\n",
            "Train Epoch 16:iter10 - \tLoss1: -6.347804- \tLoss2: -12.260841- \tLoss_total: -18.608645\n",
            "Train Epoch 16:iter20 - \tLoss1: -6.420484- \tLoss2: -12.346334- \tLoss_total: -18.766817\n",
            "Train Epoch 16:iter30 - \tLoss1: -6.441514- \tLoss2: -12.308148- \tLoss_total: -18.749662\n",
            "Train Epoch 16:iter40 - \tLoss1: -6.288265- \tLoss2: -12.211490- \tLoss_total: -18.499754\n",
            "Train Epoch 16:iter50 - \tLoss1: -6.456388- \tLoss2: -12.322607- \tLoss_total: -18.778995\n",
            "Train Epoch 16:iter60 - \tLoss1: -6.396934- \tLoss2: -12.292373- \tLoss_total: -18.689306\n",
            "Train Epoch 16:iter70 - \tLoss1: -6.376678- \tLoss2: -12.283527- \tLoss_total: -18.660206\n",
            "Train Epoch 16:iter80 - \tLoss1: -6.436847- \tLoss2: -12.288054- \tLoss_total: -18.724900\n",
            "Train Epoch 16:iter90 - \tLoss1: -6.404570- \tLoss2: -12.297606- \tLoss_total: -18.702176\n",
            "Train Epoch 16:iter100 - \tLoss1: -6.374405- \tLoss2: -12.280869- \tLoss_total: -18.655273\n",
            "Train Epoch 16:iter110 - \tLoss1: -6.281430- \tLoss2: -12.326017- \tLoss_total: -18.607448\n",
            "Train Epoch 17:iter0 - \tLoss1: -6.409860- \tLoss2: -12.279844- \tLoss_total: -18.689704\n",
            "Train Epoch 17:iter10 - \tLoss1: -6.404228- \tLoss2: -12.241462- \tLoss_total: -18.645690\n",
            "Train Epoch 17:iter20 - \tLoss1: -6.417430- \tLoss2: -12.285665- \tLoss_total: -18.703094\n",
            "Train Epoch 17:iter30 - \tLoss1: -6.484439- \tLoss2: -12.360638- \tLoss_total: -18.845077\n",
            "Train Epoch 17:iter40 - \tLoss1: -6.395401- \tLoss2: -12.231907- \tLoss_total: -18.627308\n",
            "Train Epoch 17:iter50 - \tLoss1: -6.449385- \tLoss2: -12.305136- \tLoss_total: -18.754520\n",
            "Train Epoch 17:iter60 - \tLoss1: -6.429311- \tLoss2: -12.293533- \tLoss_total: -18.722845\n",
            "Train Epoch 17:iter70 - \tLoss1: -6.294185- \tLoss2: -12.176169- \tLoss_total: -18.470355\n",
            "Train Epoch 17:iter80 - \tLoss1: -6.268992- \tLoss2: -12.160513- \tLoss_total: -18.429505\n",
            "Train Epoch 17:iter90 - \tLoss1: -6.330709- \tLoss2: -12.157784- \tLoss_total: -18.488492\n",
            "Train Epoch 17:iter100 - \tLoss1: -6.185594- \tLoss2: -12.134094- \tLoss_total: -18.319688\n",
            "Train Epoch 17:iter110 - \tLoss1: -6.295441- \tLoss2: -12.146397- \tLoss_total: -18.441838\n",
            "Train Epoch 18:iter0 - \tLoss1: -6.273829- \tLoss2: -12.100094- \tLoss_total: -18.373923\n",
            "Train Epoch 18:iter10 - \tLoss1: -6.302039- \tLoss2: -12.141634- \tLoss_total: -18.443673\n",
            "Train Epoch 18:iter20 - \tLoss1: -6.276949- \tLoss2: -12.117596- \tLoss_total: -18.394545\n",
            "Train Epoch 18:iter30 - \tLoss1: -6.338286- \tLoss2: -12.183794- \tLoss_total: -18.522080\n",
            "Train Epoch 18:iter40 - \tLoss1: -6.391010- \tLoss2: -12.229964- \tLoss_total: -18.620975\n",
            "Train Epoch 18:iter50 - \tLoss1: -6.230700- \tLoss2: -12.220671- \tLoss_total: -18.451370\n",
            "Train Epoch 18:iter60 - \tLoss1: -6.233897- \tLoss2: -12.178273- \tLoss_total: -18.412170\n",
            "Train Epoch 18:iter70 - \tLoss1: -6.408972- \tLoss2: -12.276079- \tLoss_total: -18.685051\n",
            "Train Epoch 18:iter80 - \tLoss1: -6.322338- \tLoss2: -12.182214- \tLoss_total: -18.504551\n",
            "Train Epoch 18:iter90 - \tLoss1: -6.309671- \tLoss2: -12.218950- \tLoss_total: -18.528621\n",
            "Train Epoch 18:iter100 - \tLoss1: -6.278472- \tLoss2: -12.190573- \tLoss_total: -18.469045\n",
            "Train Epoch 18:iter110 - \tLoss1: -6.288229- \tLoss2: -12.228364- \tLoss_total: -18.516593\n",
            "Train Epoch 19:iter0 - \tLoss1: -6.330773- \tLoss2: -12.247393- \tLoss_total: -18.578166\n",
            "Train Epoch 19:iter10 - \tLoss1: -6.386809- \tLoss2: -12.232107- \tLoss_total: -18.618917\n",
            "Train Epoch 19:iter20 - \tLoss1: -6.220006- \tLoss2: -12.230392- \tLoss_total: -18.450398\n",
            "Train Epoch 19:iter30 - \tLoss1: -6.309148- \tLoss2: -12.263939- \tLoss_total: -18.573087\n",
            "Train Epoch 19:iter40 - \tLoss1: -6.261934- \tLoss2: -12.279504- \tLoss_total: -18.541438\n",
            "Train Epoch 19:iter50 - \tLoss1: -6.224524- \tLoss2: -12.200844- \tLoss_total: -18.425367\n",
            "Train Epoch 19:iter60 - \tLoss1: -6.407979- \tLoss2: -12.285778- \tLoss_total: -18.693757\n",
            "Train Epoch 19:iter70 - \tLoss1: -6.264879- \tLoss2: -12.244051- \tLoss_total: -18.508930\n",
            "Train Epoch 19:iter80 - \tLoss1: -6.344476- \tLoss2: -12.235312- \tLoss_total: -18.579788\n",
            "Train Epoch 19:iter90 - \tLoss1: -6.337649- \tLoss2: -12.182441- \tLoss_total: -18.520090\n",
            "Train Epoch 19:iter100 - \tLoss1: -6.320726- \tLoss2: -12.258244- \tLoss_total: -18.578969\n",
            "Train Epoch 19:iter110 - \tLoss1: -6.293016- \tLoss2: -12.272829- \tLoss_total: -18.565845\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm8F6cSXHs1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# モデル分類のクラスターの結果を確認する\n",
        "\n",
        "def test(model, device, train_loader):\n",
        "    model.eval()\n",
        "\n",
        "    # 結果を格納するリスト\n",
        "    out_targs = []\n",
        "    ref_targs = []\n",
        "    cnt = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            cnt += 1\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            outputs, outputs_overclustering = model(data)\n",
        "\n",
        "            # 分類結果をリストに追加\n",
        "            out_targs.append(outputs.argmax(dim=1).cpu())\n",
        "            ref_targs.append(target.cpu())\n",
        "\n",
        "    # リストをひとまとめに\n",
        "    out_targs = torch.cat(out_targs)\n",
        "    ref_targs = torch.cat(ref_targs)\n",
        "\n",
        "    return out_targs.numpy(), ref_targs.numpy()\n",
        "\n",
        "\n",
        "out_targs, ref_targs = test(model_trained, device, train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8kgHKwHIXd2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "7cb8496a-85e5-4c07-d11e-33d02d5f5c7b"
      },
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "# 混同行列（的な）を作る\n",
        "matrix = np.zeros((10, 10))\n",
        "\n",
        "# 縦に数字の0から9を、横に判定されたクラスの頻度表を作成\n",
        "for i in range(len(out_targs)):\n",
        "    row = ref_targs[i]\n",
        "    col = out_targs[i]\n",
        "    matrix[row][col] += 1\n",
        "\n",
        "np.set_printoptions(suppress=True)\n",
        "print(matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   0.  975.    2.    0.    0.    0.    2.    0.    0.    1.]\n",
            " [   1.    0.    3. 1112.    1.    0.    4.    1.   10.    3.]\n",
            " [   0.    2.    6.    0.    0.    0.    5.    2. 1014.    3.]\n",
            " [   0.    0.    1.    0.    0.    0.  999.    2.    1.    7.]\n",
            " [   1.    1.    0.    0.  940.    0.    0.   38.    0.    2.]\n",
            " [   3.    1.    0.    0.    0.  850.   24.    1.    1.   12.]\n",
            " [ 936.    5.    0.    2.    1.    1.    2.    0.    3.    8.]\n",
            " [   0.    0.  886.    0.    0.    0.    3.  132.    7.    0.]\n",
            " [   0.    3.    0.    0.    0.    1.    0.    4.    2.  964.]\n",
            " [   0.    4.    4.    0.    2.    1.   30.  959.    0.    9.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX6kQSzUJdmc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "bc311ace-8ec5-4167-8f9e-5b092800ae11"
      },
      "source": [
        "# 全データ\n",
        "total_num = matrix.sum().sum()\n",
        "print(total_num)\n",
        "\n",
        "# 各数字がきれいに各クラスに分かれている。\n",
        "# 例えば数字の0はクラスの1番目に978個集まった。数字の9であれば、7番目に949個集まった。\n",
        "# よって、最大のものを足していくと、正解の個数なので\n",
        "correct_num_list = matrix.max(axis=0)\n",
        "print(correct_num_list)\n",
        "print(correct_num_list.sum())\n",
        "\n",
        "print(\"正解率：\", correct_num_list.sum()/total_num*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000.0\n",
            "[ 936.  975.  886. 1112.  940.  850.  999.  959. 1014.  964.]\n",
            "9635.0\n",
            "正解率： 96.35000000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pnwfi2KZJebU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}