{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BART_finetuning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOyWL/PfsjehautoDZrWgEJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takky0330/NLP/blob/master/BART_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xc29xS8soXHM"
      },
      "outputs": [],
      "source": [
        "# fairseqインストール(gitから)\n",
        "!git clone https://github.com/utanaka2000/fairseq.git\n",
        "%cd fairseq\n",
        "!git fetch origin\n",
        "!git checkout japanese_bart_pretrained_model\n",
        "!git branch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## このインストールが終わったら、ランタイムを再起動する\n",
        "#!pip install --editable ./    ## インストール先が変わり後半の「要約用スクリプト」でエラーが出る\n",
        "!pip install ./\n",
        "## このインストールが終わったら、ランタイムを再起動する"
      ],
      "metadata": {
        "id": "uoYLrN2RDkBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fairseq有効化\n",
        "!echo $PYTHONPATH\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] = \"/env/python\"\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "\n",
        "!echo $PYTHONPATH"
      ],
      "metadata": {
        "id": "69cU4gbxo6P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# インストールの確認\n",
        "!pip show fairseq"
      ],
      "metadata": {
        "id": "Kl-w0NLxpGb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!wget http://lotus.kuee.kyoto-u.ac.jp/nl-resource/JapaneseBARTPretrainedModel/japanese_bart_base_1.1.tar.gz\n",
        "\n",
        "!tar -zxvf japanese_bart_base_1.1.tar.gz"
      ],
      "metadata": {
        "id": "PHXygWRTpG5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Juman++"
      ],
      "metadata": {
        "id": "k7KnLaYjFNRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# jumanpp-2.0.0-rc3 download\n",
        "%cd /content\n",
        "!wget https://github.com/ku-nlp/jumanpp/releases/download/v2.0.0-rc3/jumanpp-2.0.0-rc3.tar.xz\n",
        "\n",
        "# unzip a file\n",
        "!tar xvf jumanpp-2.0.0-rc3.tar.xz"
      ],
      "metadata": {
        "id": "rLHFIStJqlFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## jumanpp-2.0.0-rc3/src/core/analysis/analyzer.h\n",
        "## 16,17行目\n",
        "#  size_t pageSize = 48 * 1024 * 1024;\n",
        "#  size_t maxInputBytes = 48 * 1024;\n",
        "\n",
        "%%bash\n",
        "sed -i -e \"16c size_t pageSize = 48 * 1024 * 1024;\" jumanpp-2.0.0-rc3/src/core/analysis/analyzer.h\n",
        "sed -i -e \"17c size_t maxInputBytes = 48 * 1024;\" jumanpp-2.0.0-rc3/src/core/analysis/analyzer.h\n",
        "\n",
        "! head -17 jumanpp-2.0.0-rc3/src/core/analysis/analyzer.h | tail -2"
      ],
      "metadata": {
        "id": "SfKXpzv4FeT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## jumanpp-2.0.0-rc3/src/core/input/stream_reader.h\n",
        "## 27,28行目\n",
        "#  u64 maxInputLength_ = 48 * 1024;\n",
        "#  u64 maxCommentLength_ = 48 * 1024;\n",
        "\n",
        "%%bash\n",
        "sed -i -e \"27c u64 maxInputLength_ = 48 * 1024;\" jumanpp-2.0.0-rc3/src/core/input/stream_reader.h\n",
        "sed -i -e \"28c u64 maxCommentLength_ = 48 * 1024;\" jumanpp-2.0.0-rc3/src/core/input/stream_reader.h\n",
        "\n",
        "! head -28 jumanpp-2.0.0-rc3/src/core/input/stream_reader.h | tail -2"
      ],
      "metadata": {
        "id": "UPmTS1meFnBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build jumanpp\n",
        "%cd /content/jumanpp-2.0.0-rc3/\n",
        "!mkdir build\n",
        "%cd build\n",
        "\n",
        "!cmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/usr/local\n",
        "!make\n",
        "\n",
        "# install jumanpp\n",
        "!sudo make install"
      ],
      "metadata": {
        "id": "jb-9oXLNIVhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"本日は晴天なり\" | jumanpp"
      ],
      "metadata": {
        "id": "ixJvkWeJJKZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## その他のインストール"
      ],
      "metadata": {
        "id": "hdaO71wT77GJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## sentencepieceのインストール\n",
        "!pip install sentencepiece\n",
        "\n",
        "## zenhanとpyknpのインストール\n",
        "!pip install zenhan\n",
        "!pip install pyknp"
      ],
      "metadata": {
        "id": "C6uAbq32YPTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##前処理用の環境変数の設定"
      ],
      "metadata": {
        "id": "mhoLedH9AF8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データセットの前処理の設定\n",
        "%env TRAIN_SRC=train_src.txt\n",
        "%env TRAIN_TGT=train_tgt.txt\n",
        "%env VALID_SRC=val_src.txt\n",
        "%env VALID_TGT=val_tgt.txt\n",
        "%env TEST_SRC=test_src.txt\n",
        "%env TEST_TGT=test_tgt.txt\n",
        "# ダウンロードしたセンテンスピースモデル\n",
        "%env SENTENCEPIECE_MODEL=japanese_bart_base_1.1/sp.model\n",
        "# 前処理後のファイルを入れるフォルダ\n",
        "#%env DATASET_DIR=datasets/\n",
        "%env DATASET_DIR=./\n",
        "# ダウンロードした辞書ファイル\n",
        "%env DICT=japanese_bart_base_1.1/dict.txt"
      ],
      "metadata": {
        "id": "6dkjtZFD7K72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##データの準備\n",
        "\n",
        "bart_datasets.csv をアップロードする"
      ],
      "metadata": {
        "id": "BHvbjA_32hSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "\n",
        "# アップロードが遅いので…\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ToAzMsq5Et6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 既に、Google　Drive　に準備済みのデータがあれば、それをコピー\n",
        "\n",
        "!cp /content/drive/MyDrive/bart/dict.src.txt ./\n",
        "!cp /content/drive/MyDrive/bart/dict.tgt.txt ./\n",
        "!cp /content/drive/MyDrive/bart/valid.src-tgt.src ./\n",
        "!cp /content/drive/MyDrive/bart/valid.src-tgt.tgt ./\n",
        "!cp /content/drive/MyDrive/bart/test.src-tgt.src ./\n",
        "!cp /content/drive/MyDrive/bart/test.src-tgt.tgt ./\n",
        "!cp /content/drive/MyDrive/bart/train.src-tgt.src ./\n",
        "!cp /content/drive/MyDrive/bart/train.src-tgt.tgt ./\n",
        "\n",
        "!ls\n",
        "## 以下のデータ準備は実行しない\n",
        "## ファインチューニングの実行から再開する"
      ],
      "metadata": {
        "id": "eSL1vO044WNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/bart/bart_datasets.csv ./"
      ],
      "metadata": {
        "id": "wPDOoY5xE4BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "livedoor_df = pd.read_csv(\"bart_datasets.csv\", sep=',', encoding='UTF-8')\n",
        "input_df = livedoor_df.copy()\n",
        "\n",
        "input_df[\"tgt_length\"] = input_df[\"tgt\"].apply(lambda x: len(str(x)))\n",
        "input_df[\"src_length\"] = input_df[\"src\"].apply(lambda x: len(str(x)))\n",
        "input_df[\"rate\"] = input_df[\"tgt_length\"] / input_df[\"src_length\"]\n",
        "\n",
        "mask = input_df['src_length'] <= 1500\n",
        "input_df = input_df[mask]\n",
        "\n",
        "mask = input_df['rate'] <= 0.5\n",
        "input_df = input_df[mask]\n",
        "\n",
        "mask = input_df['rate'] >= 0.05\n",
        "input_df = input_df[mask]"
      ],
      "metadata": {
        "id": "0KnpkSMk3Ge7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## データをシャッフル\n",
        "input_df = input_df.sample(frac=1)"
      ],
      "metadata": {
        "id": "vqvj5Q9Z58hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_df = input_df[:42000]\n",
        "#val_df = input_df[42000:44000]\n",
        "#test_df = input_df[44000:]\n",
        "\n",
        "##　短縮版\n",
        "train_df = input_df[:8000]\n",
        "val_df = input_df[8000:9000]\n",
        "test_df = input_df[9000:10000]\n",
        "\n",
        "train_src = train_df[\"src\"].to_list()\n",
        "train_tgt = train_df[\"tgt\"].to_list()\n",
        "val_src = val_df[\"src\"].to_list()\n",
        "val_tgt = val_df[\"tgt\"].to_list()\n",
        "test_src = test_df[\"src\"].to_list()\n",
        "test_tgt = test_df[\"tgt\"].to_list()\n",
        "\n",
        "# 記号や特殊文字を削除する関数\n",
        "def delete_special_character(str_list):\n",
        "    new_list = []\n",
        "    characters = \"■◆☆●\\n\"\n",
        "    for strength in str_list:\n",
        "        for x in range(len(characters)):\n",
        "            strength = str(strength).replace(characters[x],\"\")\n",
        "            new_list.append(strength)\n",
        "    return new_list\n",
        "\n",
        "train_src = delete_special_character(train_src)\n",
        "train_tgt = delete_special_character(train_tgt)\n",
        "val_src = delete_special_character(val_src)\n",
        "val_tgt = delete_special_character(val_tgt)\n",
        "test_src = delete_special_character(test_src)\n",
        "test_tgt = delete_special_character(test_tgt)\n",
        "\n",
        "with open(\"train_src.txt\", 'w') as f:\n",
        "  for d in train_src:\n",
        "    f.write(\"%s\\n\" % d)  \n",
        "\n",
        "with open(\"train_tgt.txt\", 'w') as f:\n",
        "  for d in train_tgt:\n",
        "    f.write(\"%s\\n\" % d)  \n",
        "\n",
        "with open(\"val_src.txt\", 'w') as f:\n",
        "  for d in val_src:\n",
        "    f.write(\"%s\\n\" % d)  \n",
        "\n",
        "with open(\"val_tgt.txt\", 'w') as f:\n",
        "  for d in val_tgt:\n",
        "    f.write(\"%s\\n\" % d)\n",
        "\n",
        "with open(\"test_src.txt\", 'w') as f:\n",
        "  for d in test_src:\n",
        "    f.write(\"%s\\n\" % d)  \n",
        "\n",
        "with open(\"test_tgt.txt\", 'w') as f:\n",
        "  for d in test_tgt:\n",
        "    f.write(\"%s\\n\" % d) "
      ],
      "metadata": {
        "id": "7YkKF5bq6Jwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##前処理実行"
      ],
      "metadata": {
        "id": "DEsS6epsAQos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat $VALID_SRC | python3 fairseq/jaBART_preprocess.py --bpe_model $SENTENCEPIECE_MODEL --bpe_dict $DICT > $DATASET_DIR/valid.src-tgt.src\n",
        "!cp ./valid.src-tgt.src /content/drive/MyDrive/bart/\n",
        "!cat $VALID_TGT | python3 fairseq/jaBART_preprocess.py --bpe_model $SENTENCEPIECE_MODEL --bpe_dict $DICT > $DATASET_DIR/valid.src-tgt.tgt\n",
        "!cp ./valid.src-tgt.tgt /content/drive/MyDrive/bart/\n",
        "!cat $TEST_SRC | python3 fairseq/jaBART_preprocess.py --bpe_model $SENTENCEPIECE_MODEL --bpe_dict $DICT > $DATASET_DIR/test.src-tgt.src\n",
        "!cp ./test.src-tgt.src /content/drive/MyDrive/bart/\n",
        "!cat $TEST_TGT | python3 fairseq/jaBART_preprocess.py --bpe_model $SENTENCEPIECE_MODEL --bpe_dict $DICT > $DATASET_DIR/test.src-tgt.tgt\n",
        "!cp ./test.src-tgt.tgt /content/drive/MyDrive/bart/\n",
        "!cat $TRAIN_SRC | python3 fairseq/jaBART_preprocess.py --bpe_model $SENTENCEPIECE_MODEL --bpe_dict $DICT > $DATASET_DIR/train.src-tgt.src\n",
        "!cp ./train.src-tgt.src /content/drive/MyDrive/bart/\n",
        "!cat $TRAIN_TGT | python3 fairseq/jaBART_preprocess.py  --bpe_model $SENTENCEPIECE_MODEL --bpe_dict $DICT > $DATASET_DIR/train.src-tgt.tgt\n",
        "!cp ./train.src-tgt.tgt /content/drive/MyDrive/bart/\n",
        "!cp $DICT $DATASET_DIR/dict.src.txt\n",
        "!cp ./dict.src.txt /content/drive/MyDrive/bart/\n",
        "!cp $DICT $DATASET_DIR/dict.tgt.txt\n",
        "!cp ./dict.tgt.txt /content/drive/MyDrive/bart/"
      ],
      "metadata": {
        "id": "eZFiZI_QASms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#ファインチューニング"
      ],
      "metadata": {
        "id": "CRV-5IBdAWvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###ファインチューニング用の環境変数の設定\n"
      ],
      "metadata": {
        "id": "FAlhMbPbAbf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ダウンロードした事前学習済みBARTモデル\n",
        "%env PRETRAINED_MODEL=japanese_bart_base_1.1/bart_model.pt\n",
        "# bart_baseまたはbart_largeを設定\n",
        "%env BART=bart_base\n",
        "# その他保存用フォルダの設定\n",
        "%env TENSORBOARD_DIR=log/\n",
        "%env SAVE_MODEL_DIR=save/\n",
        "%env RESULT=result.txt"
      ],
      "metadata": {
        "id": "2xTvAwX9AgS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###ファインチューニング実行"
      ],
      "metadata": {
        "id": "KdGfuYpjAm4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 fairseq-train $DATASET_DIR --arch $BART --restore-file $PRETRAINED_MODEL \\\n",
        "--save-dir $SAVE_MODEL_DIR --tensorboard-logdir $TENSORBOARD_DIR \\\n",
        "--task translation_from_pretrained_bart --source-lang src --target-lang tgt \\\n",
        "--criterion label_smoothed_cross_entropy --label-smoothing 0.2 --dataset-impl raw \\\n",
        "--optimizer adam --adam-eps 1e-06 --adam-betas '{0.9, 0.98}' --lr-scheduler polynomial_decay --lr 3e-05 --min-lr -1 \\\n",
        "--warmup-updates 2500 --total-num-update 1000 --dropout 0.3 --attention-dropout 0.1  --weight-decay 0.0 \\\n",
        "--max-tokens 1024 --update-freq 5 --save-interval -1 --no-epoch-checkpoints --seed 222 --log-format simple --log-interval 2 \\\n",
        "--reset-optimizer --reset-meters --reset-dataloader --reset-lr-scheduler  --save-interval-updates 10000 \\\n",
        "--ddp-backend no_c10d --max-epoch 5 \\\n",
        "--encoder-normalize-before --decoder-normalize-before --prepend-bos \\\n",
        "--skip-invalid-size-inputs-valid-test"
      ],
      "metadata": {
        "id": "-WA2LpZnAofd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###ファインチューニング後モデルの保存"
      ],
      "metadata": {
        "id": "wcEM2gkFAtEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env MODEL_NAME=livedoor_5_eopch\n",
        "\n",
        "!mkdir -p models/$MODEL_NAME\n",
        "\n",
        "!cp -rf $SAVE_MODEL_DIR models/$MODEL_NAME\n",
        "!cp -rf $DICT models/$MODEL_NAME\n",
        "!cp -rf $SENTENCEPIECE_MODEL models/$MODEL_NAME\n",
        "#!cp -rf $TENSORBOARD_DIR models/$MODEL_NAME\n",
        "#!cp -rf $DATASET_DIR models/$MODEL_NAME\n",
        "\n",
        "!mkdir -p /content/drive/MyDrive/bart/models/$MODEL_NAME\n",
        "!cp -rf models/$MODEL_NAME/* /content/drive/MyDrive/bart/models/$MODEL_NAME"
      ],
      "metadata": {
        "id": "jWeKD2zX4YkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###要約実行"
      ],
      "metadata": {
        "id": "dpivD8MNA5Mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 既に、Google　Drive　に学習済みのモデルがあれば、それをコピー\n",
        "%cd /content\n",
        "\n",
        "# アップロードが遅いので…\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%env MODEL_NAME=livedoor_5_eopch\n",
        "MODEL_NAME = 'livedoor_5_eopch'\n",
        "\n",
        "!echo $MODEL_NAME\n",
        "!mkdir ./models\n",
        "!cp -r /content/drive/MyDrive/bart/models/$MODEL_NAME ./models\n",
        "## 既に、Google　Drive　に学習済みのモデルがあれば、それをコピー\n",
        "\n",
        "## Google　Drive　に保存したテストデータをコピー\n",
        "!mkdir models/$MODEL_NAME/datasets\n",
        "!cp /content/drive/MyDrive/bart/*.* models/$MODEL_NAME/datasets\n",
        "## Google　Drive　に保存したテストデータをコピー"
      ],
      "metadata": {
        "id": "CpmT6Z7Sh3T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###要約用スクリプト"
      ],
      "metadata": {
        "id": "lZpuxpVFA60p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "import fileinput\n",
        "import logging\n",
        "import math\n",
        "import sys\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils\n",
        "from fairseq.data import encoders\n",
        "from fairseq.token_generation_constraints import pack_constraints, unpack_constraints\n",
        "import zenhan\n",
        "from pyknp import Juman\n",
        "import sentencepiece\n",
        "\n",
        "logging.basicConfig(\n",
        "    format='%(name)s | %(message)s',\n",
        "    level=os.environ.get('LOGLEVEL', 'INFO').upper(),\n",
        "    stream=sys.stdout,\n",
        ")\n",
        "logger = logging.getLogger('infer')\n",
        "\n",
        "Batch = namedtuple('Batch', 'ids src_tokens src_lengths constraints')\n",
        "Translation = namedtuple('Translation', 'src_str hypos pos_scores alignments')\n",
        "\n",
        "\n",
        "def get_symbols_to_strip_from_output(generator):\n",
        "    if hasattr(generator, 'symbols_to_strip_from_output'):\n",
        "        return generator.symbols_to_strip_from_output\n",
        "    else:\n",
        "        return {generator.eos}\n",
        "\n",
        "\n",
        "def buffered_read(input, buffer_size):\n",
        "    buffer = []\n",
        "    with fileinput.input(files=[input], openhook=fileinput.hook_encoded(\"utf-8\")) as h:\n",
        "        for src_str in h:\n",
        "            buffer.append(src_str.strip())\n",
        "            if len(buffer) >= buffer_size:\n",
        "                yield buffer\n",
        "                buffer = []\n",
        "\n",
        "    if len(buffer) > 0:\n",
        "        yield buffer\n",
        "\n",
        "\n",
        "def make_batches(lines, args, task, max_positions, encode_fn):\n",
        "    tokens = [\n",
        "        task.source_dictionary.encode_line(\n",
        "            encode_fn(src_str), add_if_not_exist=False\n",
        "        ).long()\n",
        "        for src_str in lines\n",
        "    ]\n",
        "\n",
        "    if args.constraints:\n",
        "        constraints_tensor = pack_constraints(batch_constraints)\n",
        "    else:\n",
        "        constraints_tensor = None\n",
        "    lengths = [t.numel() for t in tokens]\n",
        "    itr = task.get_batch_iterator(\n",
        "        dataset=task.build_dataset_for_inference(tokens, lengths, constraints=constraints_tensor),\n",
        "        max_tokens=args.max_tokens,\n",
        "        max_sentences=args.max_sentences,\n",
        "        max_positions=max_positions,\n",
        "        ignore_invalid_inputs=args.skip_invalid_size_inputs_valid_test\n",
        "    ).next_epoch_itr(shuffle=False)\n",
        "    for batch in itr:\n",
        "        ids = batch['id']\n",
        "        src_tokens = batch['net_input']['src_tokens']\n",
        "        src_lengths = batch['net_input']['src_lengths']\n",
        "        constraints = batch.get(\"constraints\", None)\n",
        "\n",
        "        yield Batch(\n",
        "            ids=ids,\n",
        "            src_tokens=src_tokens,\n",
        "            src_lengths=src_lengths,\n",
        "            constraints=constraints,\n",
        "        )\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    start_time = time.time()\n",
        "    total_translate_time = 0\n",
        "\n",
        "    utils.import_user_module(args)\n",
        "\n",
        "    if args.buffer_size < 1:\n",
        "        args.buffer_size = 1\n",
        "    if args.max_tokens is None and args.max_sentences is None:\n",
        "        args.max_sentences = 1\n",
        "\n",
        "    assert not args.sampling or args.nbest == args.beam, \\\n",
        "        '--sampling requires --nbest to be equal to --beam'\n",
        "    assert not args.max_sentences or args.max_sentences <= args.buffer_size, \\\n",
        "        '--max-sentences/--batch-size cannot be larger than --buffer-size'\n",
        "\n",
        "    # Fix seed for stochastic decoding\n",
        "    if args.seed is not None and not args.no_seed_provided:\n",
        "        np.random.seed(args.seed)\n",
        "        utils.set_torch_seed(args.seed)\n",
        "\n",
        "    use_cuda = False\n",
        "\n",
        "    # Setup task, e.g., translation\n",
        "    task = tasks.setup_task(args)\n",
        "\n",
        "    # Load ensemble\n",
        "    models, _model_args = checkpoint_utils.load_model_ensemble(\n",
        "    ######  args.pa。rrides=eval(args.model_overrides),\n",
        "        [args.path],\n",
        "        arg_overrides=eval(args.model_overrides),\n",
        "        task=task,\n",
        "        suffix=getattr(args, \"checkpoint_suffix\", \"\"),\n",
        "    )\n",
        "\n",
        "    # Set dictionaries\n",
        "    src_dict = task.source_dictionary\n",
        "    tgt_dict = task.target_dictionary\n",
        "\n",
        "    # Optimize ensemble for generation\n",
        "    for model in models:\n",
        "        model.prepare_for_inference_(args)\n",
        "        if args.fp16:\n",
        "            model.half()\n",
        "        if use_cuda:\n",
        "            model.cuda()\n",
        "\n",
        "    # Initialize generator\n",
        "    generator = task.build_generator(models, args)\n",
        "\n",
        "    # Handle tokenization and BPE\n",
        "    tokenizer = encoders.build_tokenizer(args)\n",
        "    bpe = encoders.build_bpe(args)\n",
        "\n",
        "    jumanpp = Juman()\n",
        "    spm = sentencepiece.SentencePieceProcessor()\n",
        "    spm.Load(args.bpe_model)\n",
        "    #    return ' '.join([mrph.midasi for mrph in result.mrph_list()])\n",
        "\n",
        "    def juman_split(line, jumanpp):\n",
        "        result = jumanpp.analysis(line)\n",
        "        return ' '.join([mrph.midasi for mrph in result.mrph_list()])\n",
        "        \n",
        "    def bpe_encode(line, spm):\n",
        "        return ' '.join(spm.EncodeAsPieces(line.strip()))\n",
        "\n",
        "    def encode_fn(x):\n",
        "        x = x.strip()\n",
        "        x = zenhan.h2z(x)\n",
        "        x = juman_split(x, jumanpp)\n",
        "        x = bpe_encode(x, spm)\n",
        "        return x\n",
        "\n",
        "    def decode_fn(x):\n",
        "        x = x.translate({ord(i): None for i in ['▁', ' ']})\n",
        "        return x\n",
        "\n",
        "    align_dict = utils.load_align_dict(args.replace_unk)\n",
        "\n",
        "    max_positions = utils.resolve_max_positions(\n",
        "        task.max_positions(),\n",
        "        *[model.max_positions() for model in models]\n",
        "    )\n",
        "\n",
        "    if args.constraints:\n",
        "        logger.warning(\"NOTE: Constrained decoding currently assumes a shared subword vocabulary.\")\n",
        "\n",
        "    if args.buffer_size > 1:\n",
        "        logger.info('Sentence buffer size: %s', args.buffer_size)\n",
        "    logger.info('NOTE: hypothesis and token scores are output in base 2')\n",
        "    logger.info('Type the input sentence and press return:')\n",
        "    start_id = 0\n",
        "\n",
        "    # 入力用ファイルを指定する\n",
        "    input_text = 'test_src.txt'\n",
        "\n",
        "    # 出力用の配列\n",
        "    output_texts = []\n",
        "    output_texts_2 = []\n",
        "    output_texts_3 = []\n",
        "\n",
        "    for inputs in buffered_read(input_text, args.buffer_size):\n",
        "        results = []\n",
        "        for batch in make_batches(inputs, args, task, max_positions, encode_fn):\n",
        "            bsz = batch.src_tokens.size(0)\n",
        "            src_tokens = batch.src_tokens\n",
        "            src_lengths = batch.src_lengths\n",
        "            constraints = batch.constraints\n",
        "            if use_cuda:\n",
        "                src_tokens = src_tokens.cuda()\n",
        "                src_lengths = src_lengths.cuda()\n",
        "                if constraints is not None:\n",
        "                    constraints = constraints.cuda()\n",
        "\n",
        "            sample = {\n",
        "                'net_input': {\n",
        "                    'src_tokens': src_tokens,\n",
        "                    'src_lengths': src_lengths,\n",
        "                },\n",
        "            }\n",
        "            translate_start_time = time.time()\n",
        "            translations = task.inference_step(generator, models, sample, constraints=constraints)\n",
        "            translate_time = time.time() - translate_start_time\n",
        "            total_translate_time += translate_time\n",
        "            list_constraints = [[] for _ in range(bsz)]\n",
        "            if args.constraints:\n",
        "                list_constraints = [unpack_constraints(c) for c in constraints]\n",
        "            for i, (id, hypos) in enumerate(zip(batch.ids.tolist(), translations)):\n",
        "                src_tokens_i = utils.strip_pad(src_tokens[i], tgt_dict.pad())\n",
        "                constraints = list_constraints[i]\n",
        "                results.append((start_id + id, src_tokens_i, hypos,\n",
        "                    {\n",
        "                        \"constraints\": constraints,\n",
        "                        \"time\": translate_time / len(translations)\n",
        "                    }))\n",
        "\n",
        "        # sort output to match input order\n",
        "        for id_, src_tokens, hypos, info in sorted(results, key=lambda x: x[0]):\n",
        "            if src_dict is not None:\n",
        "                src_str = src_dict.string(src_tokens, args.remove_bpe)\n",
        "                print(f'Inference time: {info[\"time\"]:.3f} seconds')\n",
        "\n",
        "            # Process top predictions\n",
        "            for hypo_i, hypo in enumerate(hypos[:min(len(hypos), args.nbest)]):\n",
        "                hypo_tokens, hypo_str, alignment = utils.post_process_prediction(\n",
        "                    hypo_tokens=hypo['tokens'].int().cpu(),\n",
        "                    src_str=src_str,\n",
        "                    alignment=hypo['alignment'],\n",
        "                    align_dict=align_dict,\n",
        "                    tgt_dict=tgt_dict,\n",
        "                    remove_bpe=args.remove_bpe,\n",
        "                    extra_symbols_to_ignore=get_symbols_to_strip_from_output(generator),\n",
        "                )\n",
        "                detok_hypo_str = decode_fn(hypo_str)\n",
        "                score = hypo['score'] / math.log(2)  # convert to base 2\n",
        "\n",
        "                if hypo_i == 0:\n",
        "                    output_texts.append(detok_hypo_str)\n",
        "                if hypo_i == 1:\n",
        "                    output_texts_2.append(detok_hypo_str)\n",
        "                if hypo_i == 2:\n",
        "                    output_texts_3.append(detok_hypo_str)\n",
        "\n",
        "                print(f'Top {hypo_i+1} prediction score: {score}')\n",
        "\n",
        "        # update running id_ counter\n",
        "        start_id += len(inputs)\n",
        "    \n",
        "    # 要約結果を１〜３番目までそれぞれ書き出し\n",
        "    with open(f\"{SAVE_MODEL_NAME}_test_tgt.txt\", 'w') as f:\n",
        "        count = 0\n",
        "        for d in output_texts:\n",
        "            count+=1\n",
        "            f.write(\"%s\\n\" % d)\n",
        "    with open(f\"{SAVE_MODEL_NAME}_test_tgt_2.txt\", 'w') as f:\n",
        "        count = 0\n",
        "        for d in output_texts_2:\n",
        "            count+=1\n",
        "            f.write(\"%s\\n\" % d)\n",
        "    with open(f\"{SAVE_MODEL_NAME}_test_tgt_3.txt\", 'w') as f:\n",
        "        count = 0\n",
        "        for d in output_texts_3:\n",
        "            count+=1\n",
        "            f.write(\"%s\\n\" % d)"
      ],
      "metadata": {
        "id": "toKlJh6AA_B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###要約用スクリプトの実行"
      ],
      "metadata": {
        "id": "65GB5eUOBGyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/bart/test_src.txt ./"
      ],
      "metadata": {
        "id": "bbKXmKFJcwsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_MODEL_NAME = \"livedoor_5_eopch\"\n",
        "MODEL_NAME = 'models/' + SAVE_MODEL_NAME\n",
        "\n",
        "def cli_main():\n",
        "    parser = options.get_interactive_generation_parser()\n",
        "    parser.add_argument('--bpe_model', default='', required=True)\n",
        "    parser.add_argument('--bpe_dict', default='', required=True)\n",
        "\n",
        "    bpe_model = MODEL_NAME + \"/sp.model\"\n",
        "    bpe_dict = MODEL_NAME + \"/dict.txt\"\n",
        "    datasets_dir = MODEL_NAME + \"/datasets\"\n",
        "    tuning_model = MODEL_NAME + \"/save/checkpoint_best.pt\"\n",
        "    logger.info(datasets_dir)\n",
        "    tuning_model = 'models/livedoor_5_eopch/save/checkpoint_best.pt'\n",
        "    logger.info(tuning_model)\n",
        "\n",
        "    input_args = [\n",
        "        datasets_dir, \n",
        "        \"--path\", tuning_model,\n",
        "        \"--task\", \"translation_from_pretrained_bart\",\n",
        "        \"--max-sentences\", \"1\",\n",
        "        \"--bpe_model\", bpe_model,\n",
        "        \"--bpe_dict\", bpe_dict,\n",
        "        \"--nbest\", \"3\",\n",
        "        \"--skip-invalid-size-inputs-valid-test\"\n",
        "    ]\n",
        "    args = options.parse_args_and_arch(parser,input_args)\n",
        "\n",
        "    distributed_utils.call_main(args, main)\n",
        "\n",
        "\n",
        "cli_main()"
      ],
      "metadata": {
        "id": "hXhb0AwhBLm2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}